[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": "由于可分辨搜索索引的普及，新兴的生成检索范式将经典的信息检索问题重新定义为一个序列到序列的建模任务，放弃外部索引，并在一个 former 中编码整个文档语料库。为了提高生成检索的有效性，人们提出了许多不同的方法，但这些方法在文献语料库中的检索效果只有10万次左右。我们进行了第一次实证研究的生成检索技术在不同的语料库尺度，最终扩大到整个 MS MARCO 段落排序任务与8.8 M 段落的语料库和评估模型大小高达11B 参数。我们发现了关于将生成性检索扩展到数百万段的一些发现; 值得注意的是，在索引过程中使用合成查询作为文档表示的核心重要性，在计算计算成本时现有提议的架构修改的无效性，以及天真地扩展模型参数对检索性能的限制。虽然我们发现生成检索与小型语料库上最先进的双编码器相比具有竞争力，但是扩展到数百万段仍然是一个重要的未解决的挑战。我们相信这些研究结果将有助于社区阐明生成性检索的现状，突出独特的挑战，并启发新的研究方向。"
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": "对建议提供系统生成的解释是朝向透明和可靠的推荐系统迈出的重要一步。可解释的推荐系统为其输出提供了一个人类可理解的理由。在过去的二十年中，可解释的推荐引起了推荐系统研究界的广泛关注。本文旨在对推荐系统中视觉解释的研究工作进行综述。更具体地，我们从解释目的、解释范围、解释风格和解释格式四个维度系统地回顾了关于推荐系统中解释的文献。认识到可视化的重要性，我们从解释性可视化的角度来看待推荐系统文献，即使用可视化作为一种解释的显示方式。因此，我们得出了一套指导方针，可能是建设性的设计解释性可视化在推荐系统，并确定未来的工作在这一领域的前景。本综述的目的是帮助推荐研究人员和从业人员更好地理解可视化解释推荐研究的潜力，并支持他们在目前和未来的推荐系统中系统地设计可视化解释。"
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": "神经信息检索通常采用一个检索-重新排序框架: 一个双编码器网络首先检索 K (例如，100)候选人，然后使用一个更强大的交叉编码器模型重新排序，以排序更好的候选人更高。重新排名通常比检索器产生更好的候选人分数，但是仅限于看到被检索的最高 K 的候选人，因此没有提供根据 Recall@K 测量的检索性能的改善。在这项工作中，我们利用重新排名也提高检索提供推理时间关联反馈的检索。具体来说，我们使用重新排序器对测试实例的预测的轻量级推理时间精馏来更新检索器对该实例的查询表示。蒸馏损失的目的是使猎犬的候选分数更接近那些重新排名。然后使用更新的查询向量执行第二个检索步骤。我们的实验表明，我们的方法，可以服务任意的检索和重新排序管道，显着提高检索召回在多个领域，语言和模式。"
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": "基于文本的协同过滤(TCF)已经成为文本和新闻推荐的主流方法，利用文本编码器(也称为语言模型(LMs))来表示项目。然而，现有的 TCF 模型主要侧重于使用中小型 LM。用最大最强的 LM (比如1750亿参数的 GPT-3模型)替换项目编码器会对推荐性能产生什么影响还不确定。我们能期待前所未有的结果吗？为此，我们进行了一系列广泛的实验，旨在探索 TCF 范式的性能极限。具体来说，我们将条目编码器的大小从1亿增加到1亿，以揭示 TCF 范例的伸缩限制。然后，我们检查这些极大的 LM 是否能够为推荐任务启用通用项表示。此外，我们比较了使用最强大的 LM 的 TCF 范式和目前占主导地位的基于 ID 嵌入的 TCF 范式的性能，并研究了这种 TCF 范式的可转移性。最后，我们使用 ChatGPT 比较 TCF 和最近推广的基于提示的推荐。我们的研究结果不仅产生了积极的结果，而且还揭示了一些令人惊讶的和以前未知的负面结果，这可以激发关于基于文本的推荐系统的更深层次的反思和创新思维。代码和数据集将被发布用于进一步的研究。"
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": "视力受损者在日常工作中经常面临挑战，购买衣服是最具挑战性的工作之一。许多人从事网上购物，这消除了实体购物的一些挑战。然而，网上购物的服装受到许多其他限制和障碍。需要更多的研究来解决这些挑战，现存的工作往往基于他们的调查结果仅仅访谈，只提供主观的，回忆偏见的信息。我们使用观察和访谈的方法进行了两个互补的研究，以填补在线选择和购买服装时对 VI 人群行为的理解差距。我们的研究结果表明，购物网站容易受到不准确、误导和自相矛盾的服装描述的困扰; VI 用户主要依靠(不可靠的)搜索工具，通过查看顾客评论来检查产品描述。我们的研究结果还表明，VI 人士不愿意接受自动化系统的帮助，但如果研究人员能够开发出更好地适应用户需求和偏好的系统，那么对这种系统的信任就可以得到改善。"
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": "我们引入了一个新的基于指令的信息抽取(IE)任务，其目的是要求系统遵循特定的指令或指南来提取信息。为了促进这一领域的研究，我们构建了一个名为 DirectIE 的数据集，由来自中文维基百科的270,000个弱监督数据和1000个高质量的众包注释实例组成。我们进一步评估了各种基线模型在 DirectIE 数据集上的性能。结果表明，虽然目前的模型表现出良好的性能，仍然有改进的空间。此外，我们进行了一个全面的案例研究分析，强调了基于教学的 IE 任务固有的挑战。代码和数据集可在 https://github.com/zjunlp/deepke/tree/main/example/llm 下载。"
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": "事件提取(EE) ，旨在从事件提及中识别和分类事件触发器和参数，已经受益于预训练语言模型(PLM)。然而，现有的基于 PLM 的方法忽略了触发器/参数字段的信息，这对于理解事件模式是至关重要的。为此，我们提出了一个概率重耦合模型增强的事件抽取框架(ProCE)。具体来说，我们首先将与句法相关的事件字段建模为概率偏差，以澄清来自模糊纠缠的事件字段。此外，考虑到同一触发器/参数在 EE 中的多次出现，我们探索了同一触发器/参数的多个域之间的概率交互策略，以重新耦合相应的澄清分布并捕获更多的潜在信息域。在 EE 数据集上的实验证明了该方法的有效性和推广性。"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": "虽然 LLM 在理解和生成传统会话环境中的文本方面取得了巨大的成功，但它们执行定义不清的复杂任务的潜力在很大程度上还没有得到充分的研究。事实上，我们还没有进行全面的基准研究与多个 LLM，专门集中在一个复杂的任务。然而，进行这样的基准测试研究是具有挑战性的，因为当使用不同的提示类型/风格和提示中提供不同程度的细节时，LLM 的性能有很大的差异。为了解决这个问题，本文提出了一个通用分类法，可用于设计具有特定属性的提示符，以便执行范围广泛的复杂任务。这种分类法将使未来的基准研究能够报告作为研究的一部分使用的特定类别的提示，使不同研究之间的有意义的比较成为可能。此外，通过建立一个共同的标准通过这个分类，研究人员将能够得出更准确的结论 LLM 的表现在一个特定的复杂的任务。"
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": "创造者经济彻底改变了个人通过在线平台获利的方式。本文通过将创造者经济建模为用户、平台和内容创造者之间的三方博弈，平台与内容创造者在委托-代理模式下通过契约互动来鼓励更好的内容，开展了创造者经济中在线学习的研究。此外，该平台与用户互动，推荐新内容，接受评估，并最终从内容中获利，这些内容可以被建模为推荐系统。我们的研究旨在探讨这个平台如何能够共同优化合同和推荐系统，以便在网上学习的模式中最大限度地发挥效用。我们主要分析和比较了两类契约: 基于回报的契约和基于特征的契约。基于回报的合同支付给内容创作者的报酬只是平台收益的一小部分。相比之下，基于特性的合同根据内容的质量或特性支付给内容创作者，而不管平台获得什么报酬。结果表明，在平滑假设下，基于回报的合同和推荐策略的联合优化得到了遗憾的 $Θ (T ^ {2/3}) $。对于基于特征的契约，我们引入了本征维度 $d $的定义来描述学习契约的难度，并给出了遗憾 $数学{ O }(T ^ {(d + 1)/(d + 2)}) $的上界。线性族的上界是紧的。"
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": "电子健康记录(EHR)编码涉及将 EHR 自动分类为诊断代码。虽然大多数以前的研究将其视为一个多标签分类任务，为每个代码产生概率，并选择那些高于某个阈值的代码作为标签，但这些方法往往忽视了识别复杂疾病的挑战。在这项研究中，我们的重点是检测并发症的 EHR 疾病。我们提出了一个新的由粗到精的 ICD 路径生成框架，称为拷贝递归神经网络结构网络(crnNet) ，它使用路径生成器(PG)和路径鉴别器(PD)进行电子健康记录(eHR)编码。通过使用 RNN 产生序列输出和合并拷贝模块，我们有效地识别并发症疾病。我们的方法在预测复杂疾病方面达到了57.30% 的比例，超过了最先进的方法和以前的方法。此外，通过消融研究，我们证明复制机制在检测复杂疾病中起着至关重要的作用。"
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": "近年来，各种研究都致力于探索使用预训练语言模型的密集通道检索技术，其中掩蔽自动编码器(MAE)预训练结构是最有前途的一种。传统的 MAE 框架依赖于利用解码器的通道重构来增强编码器的文本表示能力，从而提高密集检索系统的性能。在通过解码器的通道重构来建立编码器的表示能力的背景下，假设“更高要求”的解码器需要相应提高编码器的表示能力是合理的。为此，我们提出了一种新颖的基于点间互信息的标记重要性感知掩蔽策略，以增强解码器的挑战性。重要的是，我们的方法可以在一个无监督的方式实施，而不增加额外的费用，预培训阶段。实验结果表明，该方法对于大规模有监督通道检索数据集和域外零镜头检索基准具有良好的鲁棒性和有效性。"
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": "本演示文件介绍了第一个注释文本、图像和数学公式在文档对中的重用的工具—— TEIMMA。注释内容重用对于开发剽窃检测算法特别有用。真实世界的内容重用通常是模糊的，这使得识别这种情况变得很困难。TEIMMA 允许输入模糊类型，以便对确认的剽窃案件进行新的分类。它支持在 HTML 中记录文本、图像和数学公式的不同重用类型，并通过使用文本和数学的相似性检测方法可视化文档对中的内容重用来支持用户。"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": "深度学习的最新进展催生了大型语言模型(LLM)的出现，它们在理解和产生类似于人类语言的文本方面表现出令人印象深刻的天赋。尽管有能力培训高能力的 LLM，但保持其相关性和纠正错误的方法仍然是难以捉摸的。为此，过去几年见证了 LLM 编辑技术的激增，其目标是改变特定领域内 LLM 的行为，而不会对其他输入的性能产生负面影响。本文对 LLM 模型编辑的问题、方法和机会进行了深入的探讨。特别是，我们提供了任务定义和与模型编辑有关的挑战的详尽概述，以及对我们目前掌握的最进步的方法的深入实证分析。我们还建立了一个新的基准数据集，以促进更强大的评估，并确定持久的问题内在的现有技术。我们的目标是提供有价值的见解，每个模型编辑技术的有效性和可行性，从而协助研究界作出知情的决定时，选择最适当的方法，具体的任务或背景。代码和数据集将在 https://github.com/zjunlp/easyedit 提供。"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": "本文对知识图(KG)构造和推理的大语言模型(LLM)进行了详尽的定量和定性评价。我们使用八个不同的数据集，包括实体、关系和事件提取、链接预测和问题回答。经验上，我们的研究结果表明，GPT-4在大多数任务中的表现优于 ChatGPT，甚至在某些推理和问答数据集中优于微调模型。此外，我们的研究扩展到 LLM 对信息抽取的潜在推广能力，最终导致虚拟知识提取任务的介绍和 VINE 数据集的开发。基于这些实证研究结果，我们进一步提出了 AutoKG，这是一种基于多主体的方法，利用 LLM 进行 KG 的构建和推理，旨在描绘这一领域的未来，并提供令人兴奋的发展机会。我们期望我们的研究能为幼稚园日后的工作提供宝贵的意见。{ https://github.com/zjunlp/autokg 及数据集将可供参考。"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": "大型语言模型(LLM)最近的成功显示了开发更强大的会话推荐系统(CRS)的巨大潜力，CRS 依赖于自然语言会话来满足用户的需求。本文对 ChatGPT 在会话推荐中的应用进行了研究，揭示了现有评价协议的不足之处。它可能过分强调与地面真相项目或人类注释者产生的话语的匹配，而忽视了作为一个有能力的 CRS 的互动性质。为了克服这一局限性，我们进一步提出了一种基于 LLM 的交互式评估方法 iEvaLM，该方法利用了基于 LLM 的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的 CRS 数据集的实验，我们发现与现行的评估协议相比有显著的改进。此外，我们强调可解释性的评价，ChatGPT 展示了其建议的说服性解释生成。我们的研究有助于更深入地了解 LLM 在 CRS 中尚未开发的潜力，并为未来的研究工作提供了一个更加灵活和易于使用的评估框架。这些代码和数据可以在 https://github.com/rucaibox/ievalm-crs 上公开获得。"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": "工具作为关键的接口，使人类能够理解和重塑世界。随着基础模型的出现，人工智能系统可以利用工具来扩展它们的能力，并与世界互动。现有的工具学习方法，包括有监督的微调和及时的工程方法，经常诱导语言模型不加区分地使用工具，因为复杂的问题往往超出了它们自己的能力。然而，引入用于简单任务的工具(模型本身可以很容易地解决这些任务)可能会在无意中传播错误，而不是提高性能。这就引出了一个研究问题: 我们可以教语言模型何时以及如何使用工具吗？为了满足这一需求，我们提出工具学习与执行反馈(TRICE) ，一个两阶段的端到端框架，使模型能够通过反馈不断学习从工具执行，从而学习何时和如何有效地使用工具。实验结果表明，TRICE 能够使语言模型有选择地使用工具，降低模型对工具的依赖性，同时提高语言的性能。代码和数据集将以 https://github.com/zjunlp/trice 形式提供。"
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": "大型语言模型(LLM)作为解决大型语言相关任务的少量推理工具正变得越来越有吸引力。然而，关于 LLM 如何很好地理解结构化数据(如表) ，还有很多东西需要学习。虽然表确实可以用作具有序列化的 LLM 的输入，但是缺乏全面的研究来检查 LLM 是否能够真正理解这些数据。在本文中，我们试图通过设计一个基准来评估 LLM 的结构理解能力(SUC)来理解这一点。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如，单元格查找、行检索和大小检测。我们对 GPT-3家族模型(例如 text-davinci-003)进行了一系列的评估。我们发现，性能取决于许多输入选择，包括表输入格式、内容顺序、角色提示和分区标记。根据基准评估所获得的见解，我们提出自我增强的有效结构激励，例如，利用 LLM 的内部知识识识别临界值/范围。当结合精心选择的输入选择时，这些结构化的提示方法导致了各种表格任务的 LLM 性能的有希望的改善，例如 TabFact ($uparrow2.31% $) ，HybridQA ($uparrow2.13% $) ，SQA ($uparrow2.72% $) ，Feverous ($uparrow0.84% $)和 ToTTo ($uparrow5.68% $)。我们相信，我们的基准和提议的激励方法可以作为一个简单而通用的选择，为未来的研究。代码和数据以 https://anonymous.4open.science/r/structuredllm-76f3的形式发布。"
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": "文本数据是当今建立用户偏好模型的常用辅助信息。虽然许多以前的作品利用用户评论进行评分预测，但很少关注前 N 名的推荐，甚至很少尝试合并项目文本内容，如标题和描述。通过实证研究，我们发现许多基于评论的模型在排名前 N 的推荐中表现不佳。此外，在某些推荐场景中，用户评论是不可用的，而项目文本内容更为普遍。另一方面，最近基于图卷积网络(GCN)的模型展示了最高 N 推荐的最新性能。因此，在本研究中，我们的目标是通过有效地建立用户项目图中项目文本内容和高阶连通性的模型，进一步改善最高 N 推荐。提出了一种基于注意图的文本感知推荐模型(AGTM)。通过大量实验验证了模型设计的合理性和有效性。"
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": "线性自动编码器模型通过 L2正则化和零对角约束的凸优化学习一个项目对项目的权重矩阵。尽管它们很简单，但与复杂的非线性模型相比，它们表现出了显著的性能。本文旨在从理论上理解线性自动编码器中两项的性质。通过奇异值分解(SVD)和主成分分析(PCA)透镜，我们发现二语正规化增强了高等级个人电脑的影响。与此同时，零对角线约束减少了低排名个人电脑的影响，导致不受欢迎项目的性能下降。受此分析的启发，我们提出了使用对角不等式约束的简单而有效的线性自动编码器模型，称为松弛线性自动编码器(RLAE)和松弛去噪线性自动编码器(RDLAE)。我们证明了它们通过调整对角线约束的程度来推广线性自动编码器。实验结果表明，在六个基准数据集上，我们的模型与最先进的线性和非线性模型具有可比性或优越性，它们显著提高了长尾项目的准确性。这些结果也支持我们对线性自动编码器的正则化和对角线约束的理论认识。"
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and\n  Objects",
        "url": "http://arxiv.org/abs/2305.14345v1",
        "pub_date": "2023-05-23",
        "summary": "Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.",
        "translated": "深层生成模型最近已经扩展到合成3D 数字人类。然而，以前的方法没有考虑到衣服和配件的组合性，而是把穿着衣服的人当作一个单一的几何块来对待。因此，个别项目不能自然地组成新的身份，导致有限的表现力和可控性的生成3D 化身。虽然有几种方法试图通过利用合成数据来解决这个问题，但是由于领域间的差距，人与对象之间的交互并不真实，而且手动资产创建难以适用于各种各样的对象。在这项工作中，我们提出了一个新的框架来学习人类和物体(背包，外套，围巾等)的组合生成模型从现实世界的3 d 扫描。我们的构图模型是交互感知的，这意味着人与物体之间的空间关系，以及通过物理接触的相互形状变化被完全纳入。关键的挑战是，因为人类和物体是接触的，他们的3D 扫描合并成一个单一的部分。为了不用手动注释就可以分解它们，我们建议利用一个人的两组3D 扫描，有对象的和没有对象的。我们的方法学会了分解对象，并自然地以无监督的方式将它们重新组合成一个可生成的人类模型。尽管我们的简单设置只需要捕获具有对象的单个主体，但是我们的实验证明了我们的模型的强大泛化，通过使得对象的自然组合以不同姿势的不同身份和多个对象的组合，这在训练数据中是看不到的。"
    },
    {
        "title": "Siamese Masked Autoencoders",
        "url": "http://arxiv.org/abs/2305.14344v1",
        "pub_date": "2023-05-23",
        "summary": "Establishing correspondence between images or scenes is a significant\nchallenge in computer vision, especially given occlusions, viewpoint changes,\nand varying object appearances. In this paper, we present Siamese Masked\nAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for\nlearning visual correspondence from videos. SiamMAE operates on pairs of\nrandomly sampled video frames and asymmetrically masks them. These frames are\nprocessed independently by an encoder network, and a decoder composed of a\nsequence of cross-attention layers is tasked with predicting the missing\npatches in the future frame. By masking a large fraction ($95\\%$) of patches in\nthe future frame while leaving the past frame unchanged, SiamMAE encourages the\nnetwork to focus on object motion and learn object-centric representations.\nDespite its conceptual simplicity, features learned via SiamMAE outperform\nstate-of-the-art self-supervised methods on video object segmentation, pose\nkeypoint propagation, and semantic part propagation tasks. SiamMAE achieves\ncompetitive results without relying on data augmentation, handcrafted\ntracking-based pretext tasks, or other techniques to prevent representational\ncollapse.",
        "translated": "建立图像或场景之间的对应关系是计算机视觉中的一个重大挑战，特别是考虑到遮挡、视点变化和不同的物体外观。本文介绍了 Siamese 蒙版自动编码器(SiamMAE) ，它是蒙版自动编码器(MAE)的一个简单扩展，用于从视频中学习视觉对应。SiamMAE 对一对随机采样的视频帧进行操作，并非对称地屏蔽它们。这些帧由编码器网络独立处理，由一系列交叉注意层组成的解码器负责预测未来帧中丢失的补丁。SiamMAE 通过在未来帧中屏蔽大部分补丁(95%) ，同时保持过去帧不变，鼓励网络关注物体运动并学习以物体为中心的表示。尽管概念简单，通过 SiamMAE 学习的特征在视频对象分割、姿态关键点传播和语义部分传播任务方面优于最先进的自监督方法。SiamMAE 无需依赖数据增强、手工制作的基于跟踪的托辞任务或其他技术来防止表象崩溃，就能获得具有竞争力的结果。"
    },
    {
        "title": "Video Prediction Models as Rewards for Reinforcement Learning",
        "url": "http://arxiv.org/abs/2305.14343v1",
        "pub_date": "2023-05-23",
        "summary": "Specifying reward signals that allow agents to learn complex behaviors is a\nlong-standing challenge in reinforcement learning. A promising approach is to\nextract preferences for behaviors from unlabeled videos, which are widely\navailable on the internet. We present Video Prediction Rewards (VIPER), an\nalgorithm that leverages pretrained video prediction models as action-free\nreward signals for reinforcement learning. Specifically, we first train an\nautoregressive transformer on expert videos and then use the video prediction\nlikelihoods as reward signals for a reinforcement learning agent. VIPER enables\nexpert-level control without programmatic task rewards across a wide range of\nDMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction\nmodel allows us to derive rewards for an out-of-distribution environment where\nno expert data is available, enabling cross-embodiment generalization for\ntabletop manipulation. We see our work as starting point for scalable reward\nspecification from unlabeled videos that will benefit from the rapid advances\nin generative modeling. Source code and datasets are available on the project\nwebsite: https://escontrela.me",
        "translated": "指定奖励信号，让代理人学习复杂的行为，是强化学习研究中一个长期存在的挑战。一个有前途的方法是从未标记的视频中提取行为偏好，这些视频在互联网上广泛可用。我们提出了视频预测奖励(VIPER)算法，该算法利用预先训练的视频预测模型作为强化学习的无动作奖励信号。具体来说，我们首先在专家视频上训练一个自回归变换器，然后使用视频预测可能性作为强化学习代理的奖励信号。VIPER 可以实现专家级别的控制，无需程序任务奖励，跨越广泛的 DMC、 Atari 和 RLBench 任务。此外，视频预测模型的推广使我们能够在没有专家数据可用的分布外环境中获得奖励，从而能够对桌面操作进行跨实施例的推广。我们将我们的工作视为从未标记的视频中获得可扩展奖励规范的起点，这些视频将受益于生成建模的快速发展。源代码和数据集可在项目网站下载:  https://escontrela.me"
    },
    {
        "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\n  Correspondence",
        "url": "http://arxiv.org/abs/2305.14334v1",
        "pub_date": "2023-05-23",
        "summary": "Diffusion models have been shown to be capable of generating high-quality\nimages, suggesting that they could contain meaningful internal representations.\nUnfortunately, the feature maps that encode a diffusion model's internal\ninformation are spread not only over layers of the network, but also over\ndiffusion timesteps, making it challenging to extract useful descriptors. We\npropose Diffusion Hyperfeatures, a framework for consolidating multi-scale and\nmulti-timestep feature maps into per-pixel feature descriptors that can be used\nfor downstream tasks. These descriptors can be extracted for both synthetic and\nreal images using the generation and inversion processes. We evaluate the\nutility of our Diffusion Hyperfeatures on the task of semantic keypoint\ncorrespondence: our method achieves superior performance on the SPair-71k real\nimage benchmark. We also demonstrate that our method is flexible and\ntransferable: our feature aggregation network trained on the inversion features\nof real image pairs can be used on the generation features of synthetic image\npairs with unseen objects and compositions. Our code is available at\n\\url{https://diffusion-hyperfeatures.github.io}.",
        "translated": "扩散模型已被证明能够产生高质量的图像，表明它们可以包含有意义的内部表示。遗憾的是，编码扩散模型内部信息的特征映射不仅分布在网络的各个层上，而且还分布在扩散时间步长上，这使得提取有用的描述符变得非常困难。我们提出了扩散超特征，一个框架，巩固多尺度和多时间步特征映射到每像素特征描述符，可用于下游任务。这些描述符可以提取合成图像和真实图像使用生成和反演过程。我们评估扩散超特征在语义关键点对应任务中的效用: 我们的方法在 SPair-71k 真实图像基准上获得了优越的性能。实验结果表明，该方法具有灵活性和可移植性: 基于实际图像对反演特征的特征聚合网络可以用于具有不可见物体和成分的合成图像对的特征生成。我们的代码可以在 url { https://diffusion-hyperfeatures.github.io }找到。"
    },
    {
        "title": "Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud\n  Semantic Segmentation",
        "url": "http://arxiv.org/abs/2305.14335v1",
        "pub_date": "2023-05-23",
        "summary": "In this work, we address the challenging task of few-shot and zero-shot 3D\npoint cloud semantic segmentation. The success of few-shot semantic\nsegmentation in 2D computer vision is mainly driven by the pre-training on\nlarge-scale datasets like imagenet. The feature extractor pre-trained on\nlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, the\ndevelopment of 3D deep learning is hindered by the limited volume and instance\nmodality of datasets due to the significant cost of 3D data collection and\nannotation. This results in less representative features and large intra-class\nfeature variation for few-shot 3D point cloud segmentation. As a consequence,\ndirectly extending existing popular prototypical methods of 2D few-shot\nclassification/segmentation into 3D point cloud segmentation won't work as well\nas in 2D domain. To address this issue, we propose a Query-Guided Prototype\nAdaption (QGPA) module to adapt the prototype from support point clouds feature\nspace to query point clouds feature space. With such prototype adaption, we\ngreatly alleviate the issue of large feature intra-class variation in point\ncloud and significantly improve the performance of few-shot 3D segmentation.\nBesides, to enhance the representation of prototypes, we introduce a\nSelf-Reconstruction (SR) module that enables prototype to reconstruct the\nsupport mask as well as possible. Moreover, we further consider zero-shot 3D\npoint cloud semantic segmentation where there is no support sample. To this\nend, we introduce category words as semantic information and propose a\nsemantic-visual projection model to bridge the semantic and visual spaces. Our\nproposed method surpasses state-of-the-art algorithms by a considerable 7.90%\nand 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,\nrespectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.",
        "translated": "在这项工作中，我们解决了具有挑战性的任务少镜头和零镜头三维点云语义分割。二维计算机视觉中少镜头语义分割的成功主要是由对大规模数据集(如图像网)的预训练所驱动的。在大规模二维数据集上预训练的特征提取器对二维少镜头学习有很大的帮助。然而，由于三维数据的采集和注释成本较高，数据集的体积和实例形式有限，阻碍了三维深度学习的发展。这导致了少镜头三维点云分割的代表性较差的特征和较大的类内特征变化。因此，将现有流行的二维少镜头分类/分割原型方法直接扩展到三维点云分割将不能像在二维领域那样有效。针对这一问题，提出了一种基于查询引导的原型适配(QGPA)模块，将原型从支持点云特征空间调整到查询点云特征空间。采用这种原型自适应方法，大大减轻了点云中特征类内变化大的问题，显著提高了少镜头三维分割的性能。此外，为了提高原型的表示能力，我们引入了自重构(SR)模块，使原型能够尽可能地重构支撑掩模。此外，在没有支持样本的情况下，我们进一步考虑了零拍3D 点云语义分割。为此，我们引入类别词作为语义信息，并提出一个语义-视觉投影模型，以连接语义和视觉空间。在 S3DIS 和 ScanNet 基准的双向单镜头设置下，我们提出的方法分别以7.90% 和14.82% 的优势超越了最先进的算法。密码可于 https://github.com/heshuting555/pap-fzs3d 索取。"
    },
    {
        "title": "Large Language Models are Frame-level Directors for Zero-shot\n  Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2305.14330v1",
        "pub_date": "2023-05-23",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.",
        "translated": "在人工智能生成内容(AIGC)的范式中，将预先训练好的文本到图像(T2I)模型扩展到文本到视频(T2V)生成已经受到越来越多的关注。尽管这些框架很有效，但它们在保持一致的叙述和处理来自单一用户提示的场景构成或物体放置的快速变化方面面临挑战。本文介绍了一个名为 DirecT2V 的新框架，它利用指令调优的大型语言模型(LLM)从单个抽象用户提示符生成一帧一帧的描述。DirecT2V 利用 LLM 控制器将用户输入划分为每个帧的单独提示，从而能够包含时变内容并促进一致的视频生成。为了保持时间一致性和防止对象崩溃，提出了一种新的值映射方法和双软极大值滤波。广泛的实验结果验证了 DirecT2V 框架在从抽象用户提示生成视觉一致性和一致性视频方面的有效性，解决了零拍摄视频生成的挑战。"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate",
        "url": "http://arxiv.org/abs/2305.14325v1",
        "pub_date": "2023-05-23",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "translated": "近年来，大语言模型(LLM)在语言生成、理解和短镜头学习等方面表现出了显著的能力。大量的工作已经探索了如何通过提示工具进一步提高他们的性能，从验证，自我一致性，或中间的暂存器。在本文中，我们提出了一个互补的方法来改善语言反应，其中多个语言模型实例提出和辩论他们的个别反应和推理过程，多轮得出一个共同的最终答案。我们的研究结果表明，这种方法显着提高了数学和战略推理的一些任务。我们还证明，我们的方法提高了生成内容的实际有效性，减少了当代模型容易产生的谬误答案和幻觉。我们的方法可以直接应用于现有的黑盒模型，并对我们调查的所有任务使用相同的过程和提示。总的来说，我们的研究结果表明，这种“心灵社会”的方法有潜力显着提高语言学习者的能力，并为语言生成和理解的进一步突破铺平道路。"
    },
    {
        "title": "Text-guided 3D Human Generation from 2D Collections",
        "url": "http://arxiv.org/abs/2305.14312v1",
        "pub_date": "2023-05-23",
        "summary": "3D human modeling has been widely used for engaging interaction in gaming,\nfilm, and animation. The customization of these characters is crucial for\ncreativity and scalability, which highlights the importance of controllability.\nIn this work, we introduce Text-guided 3D Human Generation (\\texttt{T3H}),\nwhere a model is to generate a 3D human, guided by the fashion description.\nThere are two goals: 1) the 3D human should render articulately, and 2) its\noutfit is controlled by the given text. To address this \\texttt{T3H} task, we\npropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention\nto fuse compositional human rendering with the extracted fashion semantics.\nEach human body part perceives relevant textual guidance as its visual\npatterns. We incorporate the human prior and semantic discrimination to enhance\n3D geometry transformation and fine-grained consistency, enabling it to learn\nfrom 2D collections for data efficiency. We conduct evaluations on DeepFashion\nand SHHQ with diverse fashion attributes covering the shape, fabric, and color\nof upper and lower clothing. Extensive experiments demonstrate that CCH\nachieves superior results for \\texttt{T3H} with high efficiency.",
        "translated": "3D 人体建模已经广泛应用于游戏、电影和动画中的交互。这些字符的定制对于创造性和可扩展性至关重要，这突出了可控性的重要性。在这项工作中，我们介绍了文本引导的三维人类生成(texttt { T3H }) ，其中一个模型是生成一个三维人，在时尚描述的指导下。有两个目标: 1)3D 人应该清晰地渲染，2)它的装备是由给定的文本控制。为了解决这个文本{ T3H }任务，我们提出了组合跨模态人(CCH)。CCH 采用交叉模态注意将提取的时尚语义融合到组合人体绘制中。人体的每个部分都将相关的文本指导视为其视觉模式。我们结合了人类先验和语义识别，以增强三维几何变换和细粒度的一致性，使其能够学习从2D 集合的数据效率。我们对 DeepFashion 和 SHHQ 进行评估，它们具有多种时尚属性，包括上衣和下衣的形状、面料和颜色。大量的实验表明，CCH 对 texttt { T3H }具有较高的效率，取得了较好的效果。"
    },
    {
        "title": "Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications\n  in Large-scale Point Clouds",
        "url": "http://arxiv.org/abs/2305.14306v1",
        "pub_date": "2023-05-23",
        "summary": "While point-based neural architectures have demonstrated their efficacy, the\ntime-consuming sampler currently prevents them from performing real-time\nreasoning on scene-level point clouds. Existing methods attempt to overcome\nthis issue by using random sampling strategy instead of the commonly-adopted\nfarthest point sampling~(FPS), but at the expense of lower performance. So the\neffectiveness/efficiency trade-off remains under-explored. In this paper, we\nreveal the key to high-quality sampling is ensuring an even spacing between\npoints in the subset, which can be naturally obtained through a grid. Based on\nthis insight, we propose a hierarchical adaptive voxel-guided point sampler\nwith linear complexity and high parallelization for real-time applications.\nExtensive experiments on large-scale point cloud detection and segmentation\ntasks demonstrate that our method achieves competitive performance with the\nmost powerful FPS, at an amazing speed that is more than 100 times faster. This\nbreakthrough in efficiency addresses the bottleneck of the sampling step when\nhandling scene-level point clouds. Furthermore, our sampler can be easily\nintegrated into existing models and achieves a 20$\\sim$80\\% reduction in\nruntime with minimal effort. The code will be available at\nhttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt",
        "translated": "虽然基于点的神经结构已经证明了它们的功效，但耗时的采样器目前阻止它们对场景级别的点云进行实时推理。现有的方法试图用随机抽样策略代替常用的最远点抽样，但是以牺牲较低的性能为代价来克服这一问题。因此，有效性/效率之间的权衡仍未得到充分探讨。在本文中，我们揭示了高质量采样的关键是确保子集中点之间的均匀间距，这可以通过网格自然获得。在此基础上，我们提出了一种分层自适应体素引导的点采样器，它具有线性复杂度和高并行性，适用于实时应用。大规模点云检测和分割任务的大量实验表明，我们的方法实现了与最强大的 FPS 具有竞争力的性能，以惊人的速度，超过100倍的速度。这一效率上的突破解决了处理场景级点云时采样步骤的瓶颈。此外，我们的采样器可以很容易地集成到现有的模型，并实现20美元西姆 $80% 的运行时减少最小的努力。代码将在 https://github.com/ouyangjunyuan/pointcloud-3d-detector-tensorrt 公布"
    },
    {
        "title": "A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network",
        "url": "http://arxiv.org/abs/2305.14301v1",
        "pub_date": "2023-05-23",
        "summary": "Hematoxylin and Eosin (H&amp;E) staining is a widely used sample preparation\nprocedure for enhancing the saturation of tissue sections and the contrast\nbetween nuclei and cytoplasm in histology images for medical diagnostics.\nHowever, various factors, such as the differences in the reagents used, result\nin high variability in the colors of the stains actually recorded. This\nvariability poses a challenge in achieving generalization for machine-learning\nbased computer-aided diagnostic tools. To desensitize the learned models to\nstain variations, we propose the Generative Stain Augmentation Network (G-SAN)\n-- a GAN-based framework that augments a collection of cell images with\nsimulated yet realistic stain variations. At its core, G-SAN uses a novel and\nhighly computationally efficient Laplacian Pyramid (LP) based generator\narchitecture, that is capable of disentangling stain from cell morphology.\nThrough the task of patch classification and nucleus segmentation, we show that\nusing G-SAN-augmented training data provides on average 15.7% improvement in F1\nscore and 7.3% improvement in panoptic quality, respectively. Our code is\navailable at https://github.com/lifangda01/GSAN-Demo.",
        "translated": "苏木精-伊红(H & E)染色是一种广泛应用的提高组织切片饱和度和增强组织学图像中细胞核和细胞质对比度的样品制备方法。然而，各种因素，例如使用的试剂的差异，导致实际记录的污渍颜色的高度可变性。这种可变性对基于机器学习的计算机辅助诊断工具的普及提出了挑战。为了使学习的模型对染色变化不敏感，我们提出生成染色增强网络(G-SAN)——一种基于 GAN 的框架，用模拟但真实的染色变化增强细胞图像集合。在其核心，G-SAN 使用了一种新的和高度计算效率的拉普拉斯金字塔(LP)为基础的生成器架构，这是能够从细胞形态学分离染色。通过斑块分类和细胞核分割实验，我们发现使用 G-SAN 增强训练数据，F1评分平均提高15.7% ，视觉质量平均提高7.3% 。我们的代码可以在 https://github.com/lifangda01/gsan-demo 找到。"
    },
    {
        "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic\n  Contrast Sets",
        "url": "http://arxiv.org/abs/2305.15407v1",
        "pub_date": "2023-05-24",
        "summary": "Vision-language models are growing in popularity and public visibility to\ngenerate, edit, and caption images at scale; but their outputs can perpetuate\nand amplify societal biases learned during pre-training on uncurated image-text\npairs from the internet. Although debiasing methods have been proposed, we\nargue that these measurements of model bias lack validity due to dataset bias.\nWe demonstrate there are spurious correlations in COCO Captions, the most\ncommonly used dataset for evaluating bias, between background context and the\ngender of people in-situ. This is problematic because commonly-used bias\nmetrics (such as Bias@K) rely on per-gender base rates. To address this issue,\nwe propose a novel dataset debiasing pipeline to augment the COCO dataset with\nsynthetic, gender-balanced contrast sets, where only the gender of the subject\nis edited and the background is fixed. However, existing image editing methods\nhave limitations and sometimes produce low-quality images; so, we introduce a\nmethod to automatically filter the generated images based on their similarity\nto real images. Using our balanced synthetic contrast sets, we benchmark bias\nin multiple CLIP-based models, demonstrating how metrics are skewed by\nimbalance in the original COCO images. Our results indicate that the proposed\napproach improves the validity of the evaluation, ultimately contributing to\nmore realistic understanding of bias in vision-language models.",
        "translated": "视觉语言模型在大规模生成、编辑和标题图像方面越来越受欢迎和公众可见度越来越高; 但是它们的输出可以延续和放大在互联网上未经策划的图像-文本对的预培训中学到的社会偏见。虽然已经提出了消除偏差的方法，但是我们认为由于数据集的偏差，这些模型偏差的测量缺乏有效性。我们证明在 COCO 标题中存在虚假的相关性，COCO 标题是最常用于评估偏倚的数据集，背景环境和现场人员的性别之间存在虚假的相关性。这是有问题的，因为常用的偏见指标(如偏见@K)依赖于每个性别的基础比率。为了解决这一问题，我们提出了一种新的数据集去偏流水线，以增加合成的，性别平衡的对比集 COCO 数据集，其中只编辑主题的性别和背景是固定的。然而，现有的图像编辑方法存在一定的局限性，有时会产生低质量的图像，因此，我们提出了一种基于图像与真实图像相似度的自动过滤方法。使用我们的平衡合成对比度集，我们在多个基于 CLIP 的模型基准偏差，演示了如何在原始 COCO 图像的不平衡度量偏斜。我们的研究结果表明，提出的方法提高了评价的有效性，最终有助于更现实的理解偏见的视觉语言模型。"
    },
    {
        "title": "RoMa: Revisiting Robust Losses for Dense Feature Matching",
        "url": "http://arxiv.org/abs/2305.15404v1",
        "pub_date": "2023-05-24",
        "summary": "Dense feature matching is an important computer vision task that involves\nestimating all correspondences between two images of a 3D scene. In this paper,\nwe revisit robust losses for matching from a Markov chain perspective, yielding\ntheoretical insights and large gains in performance. We begin by constructing a\nunifying formulation of matching as a Markov chain, based on which we identify\ntwo key stages which we argue should be decoupled for matching. The first is\nthe coarse stage, where the estimated result needs to be globally consistent.\nThe second is the refinement stage, where the model needs precise localization\ncapabilities. Inspired by the insight that these stages concern distinct\nissues, we propose a coarse matcher following the regression-by-classification\nparadigm that provides excellent globally consistent, albeit not exactly\nlocalized, matches. This is followed by a local feature refinement stage using\nwell-motivated robust regression losses, yielding extremely precise matches.\nOur proposed approach, which we call RoMa, achieves significant improvements\ncompared to the state-of-the-art. Code is available at\nhttps://github.com/Parskatt/RoMa",
        "translated": "密集特征匹配是一项重要的计算机视觉任务，它涉及到估计三维场景中两幅图像之间的所有对应关系。在本文中，我们从马尔可夫链的角度重新审视匹配的鲁棒性损失，产生理论见解和性能的大幅提高。我们首先构造一个统一的匹配公式作为一个马尔可夫链，在此基础上，我们确定了两个关键的阶段，我们认为应该解耦匹配。第一个阶段是粗略阶段，其中估计的结果需要具有全局一致性。第二个阶段是精化阶段，在这个阶段模型需要精确的定位能力。受到这些阶段关注不同问题的洞察力的启发，我们提出了遵循分类回归范式的粗匹配器，它提供了优秀的全局一致性匹配，尽管不是完全本地化的匹配。然后是局部特征细化阶段，使用动机良好的鲁棒回归损失，产生非常精确的匹配。我们提出的方法，我们称之为 RoMa，与最先进的技术相比，取得了显著的改进。密码可于 https://github.com/parskatt/roma 索取"
    },
    {
        "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
        "url": "http://arxiv.org/abs/2305.15399v1",
        "pub_date": "2023-05-24",
        "summary": "Synthesizing novel 3D models that resemble the input example has long been\npursued by researchers and artists in computer graphics. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmodel can generate 3D shapes of various types with better quality than prior\nmethods.",
        "translated": "长期以来，计算机图形学的研究人员和艺术家一直致力于合成类似于输入样本的新颖3D 模型。本文提出了一种扩散模型 Sin3DM，该模型从单个三维纹理形状中学习内部斑块分布，并生成具有精细几何和纹理细节的高质量变化。直接在三维空间中训练扩散模型会产生大量的内存和计算开销。因此，我们首先将输入压缩到一个低维的潜在空间，然后在其上训练一个扩散模型。具体来说，我们将输入的三维纹理形状编码成三平面特征映射，表示输入的有符号距离和纹理字段。为了避免过拟合，扩散模型的去噪网络具有有限的接收域，并且使用了三平面感知的二维卷积块来提高结果的质量。除了随机产生新的样本，我们的模型还促进应用程序，如重定向，外绘和本地编辑。通过广泛的定性和定量评价，我们表明我们的模型可以生成各种类型的三维形状，比以往的方法更好的质量。"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2305.15393v1",
        "pub_date": "2023-05-24",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "translated": "在可视化生成中获得高度的用户可控性通常需要复杂的、细粒度的输入，如布局。然而，与简单的文本输入相比，这种输入对用户造成了相当大的负担。为了解决这个问题，我们研究了大语言模型(LLM)如何通过根据文本条件生成布局来充当可视化规划器，从而与可视化生成模型进行协作。我们提出了 LayoutGPT，一种用样式表语言组合上下文视觉演示的方法，以提高 LLM 的视觉规划技巧。LayoutGPT 可以在多个领域生成合理的布局，从2D 图像到3D 室内场景。LayoutGPT 在将具有挑战性的语言概念(如数值和空间关系)转换为可靠的文本到图像生成的布局安排方面也表现出优越的性能。结合下游图像生成模型，LayoutGPT 的性能比文本到图像的模型/系统高出20-40% ，并且在数值和空间正确性的可视化布局设计方面达到与人类用户相当的性能。最后，LayoutGPT 在三维室内场景合成中实现了与监督方法相当的性能，证明了其在多视觉领域中的有效性和潜力。"
    },
    {
        "title": "A Neural Space-Time Representation for Text-to-Image Personalization",
        "url": "http://arxiv.org/abs/2305.15391v1",
        "pub_date": "2023-05-24",
        "summary": "A key aspect of text-to-image personalization methods is the manner in which\nthe target concept is represented within the generative process. This choice\ngreatly affects the visual fidelity, downstream editability, and disk space\nneeded to store the learned concept. In this paper, we explore a new\ntext-conditioning space that is dependent on both the denoising process\ntimestep (time) and the denoising U-Net layers (space) and showcase its\ncompelling properties. A single concept in the space-time representation is\ncomposed of hundreds of vectors, one for each combination of time and space,\nmaking this space challenging to optimize directly. Instead, we propose to\nimplicitly represent a concept in this space by optimizing a small neural\nmapper that receives the current time and space parameters and outputs the\nmatching token embedding. In doing so, the entire personalized concept is\nrepresented by the parameters of the learned mapper, resulting in a compact,\nyet expressive, representation. Similarly to other personalization methods, the\noutput of our neural mapper resides in the input space of the text encoder. We\nobserve that one can significantly improve the convergence and visual fidelity\nof the concept by introducing a textual bypass, where our neural mapper\nadditionally outputs a residual that is added to the output of the text\nencoder. Finally, we show how one can impose an importance-based ordering over\nour implicit representation, providing users control over the reconstruction\nand editability of the learned concept using a single trained model. We\ndemonstrate the effectiveness of our approach over a range of concepts and\nprompts, showing our method's ability to generate high-quality and controllable\ncompositions without fine-tuning any parameters of the generative model itself.",
        "translated": "文本到图像个性化方法的一个关键方面是目标概念在生成过程中的表示方式。这种选择极大地影响了视觉保真度、下游可编辑性和存储所学概念所需的磁盘空间。在本文中，我们探索了一个新的文本条件空间，它同时依赖于去噪过程的时间步长(时间)和去噪的 U-Net 层(空间) ，并展示了其引人注目的性质。时空表示中的一个概念由数百个向量组成，每个向量对应于时间和空间的组合，这使得直接优化这个空间具有挑战性。相反，我们建议通过优化一个接收当前时间和空间参数并输出匹配令牌嵌入的小型神经映射器来隐式表示这个空间中的一个概念。在这样做时，整个个性化的概念是由学习映射器的参数表示，导致一个紧凑，但表达，表示。与其他个性化方法类似，我们的神经映射器的输出驻留在文本编码器的输入空间中。我们观察到，通过引入文本旁路，可以显著提高概念的收敛性和视觉保真度，其中我们的神经映射器额外输出一个残差，添加到文本编码器的输出。最后，我们展示了如何在我们的隐式表示上强加一个基于重要性的排序，使用一个单一的训练模型为用户提供对所学概念的重构和可编辑性的控制。我们通过一系列的概念和提示展示了我们的方法的有效性，展示了我们的方法能够生成高质量和可控的合成物，而不需要对生成模型本身的任何参数进行微调。"
    },
    {
        "title": "What can generic neural networks learn from a child's visual experience?",
        "url": "http://arxiv.org/abs/2305.15372v1",
        "pub_date": "2023-05-24",
        "summary": "Young children develop sophisticated internal models of the world based on\ntheir egocentric visual experience. How much of this is driven by innate\nconstraints and how much is driven by their experience? To investigate these\nquestions, we train state-of-the-art neural networks on a realistic proxy of a\nchild's visual experience without any explicit supervision or domain-specific\ninductive biases. Specifically, we train both embedding models and generative\nmodels on 200 hours of headcam video from a single child collected over two\nyears. We train a total of 72 different models, exploring a range of model\narchitectures and self-supervised learning algorithms, and comprehensively\nevaluate their performance in downstream tasks. The best embedding models\nperform at 70% of a highly performant ImageNet-trained model on average. They\nalso learn broad semantic categories without any labeled examples and learn to\nlocalize semantic categories in an image without any location supervision.\nHowever, these models are less object-centric and more background-sensitive\nthan comparable ImageNet-trained models. Generative models trained with the\nsame data successfully extrapolate simple properties of partially masked\nobjects, such as their texture, color, orientation, and rough outline, but\nstruggle with finer object details. We replicate our experiments with two other\nchildren and find very similar results. Broadly useful high-level visual\nrepresentations are thus robustly learnable from a representative sample of a\nchild's visual experience without strong inductive biases.",
        "translated": "幼儿根据自我中心的视觉经验，发展出复杂的内在世界模型。这其中有多少是由先天约束驱动的，又有多少是由他们的经验驱动的？为了研究这些问题，我们训练最先进的神经网络，在没有任何明确的监督或领域特定的归纳偏见的情况下，对儿童的视觉经验进行现实的替代。具体来说，我们训练嵌入模型和生成模型的200个小时的头部摄像头视频从一个孩子收集了两年多。我们总共训练了72个不同的模型，探索了一系列模型结构和自我监督学习算法，并全面评估了它们在下游任务中的表现。最好的嵌入模型在高性能 ImageNet 训练模型中的平均执行率为70% 。他们还学习广泛的语义类别没有任何标记的例子和学习本地化的语义类别在一个图像没有任何位置监督。然而，与可比较的 ImageNet 训练模型相比，这些模型更少的以对象为中心，更多的是对背景敏感。使用相同数据训练的生成模型成功地推断出部分遮蔽对象的简单属性，例如它们的纹理、颜色、方向和粗略轮廓，但是难以获得更精细的对象细节。我们在另外两个孩子身上做了同样的实验，得到了非常相似的结果。因此，广泛有用的高层次视觉表征可以从一个具有代表性的儿童视觉经验样本中强有力地学习，而没有强烈的归纳偏见。"
    },
    {
        "title": "SAMScore: A Semantic Structural Similarity Metric for Image Translation\n  Evaluation",
        "url": "http://arxiv.org/abs/2305.15367v1",
        "pub_date": "2023-05-24",
        "summary": "Image translation has wide applications, such as style transfer and modality\nconversion, usually aiming to generate images having both high degrees of\nrealism and faithfulness. These problems remain difficult, especially when it\nis important to preserve semantic structures. Traditional image-level\nsimilarity metrics are of limited use, since the semantics of an image are\nhigh-level, and not strongly governed by pixel-wise faithfulness to an original\nimage. Towards filling this gap, we introduce SAMScore, a generic semantic\nstructural similarity metric for evaluating the faithfulness of image\ntranslation models. SAMScore is based on the recent high-performance Segment\nAnything Model (SAM), which can perform semantic similarity comparisons with\nstandout accuracy. We applied SAMScore on 19 image translation tasks, and found\nthat it is able to outperform all other competitive metrics on all of the\ntasks. We envision that SAMScore will prove to be a valuable tool that will\nhelp to drive the vibrant field of image translation, by allowing for more\nprecise evaluations of new and evolving translation models. The code is\navailable at https://github.com/Kent0n-Li/SAMScore.",
        "translated": "意象翻译在文体转换、情态转换等方面有着广泛的应用，通常意象翻译的目的是生成逼真度高、忠实度高的意象。这些问题仍然很难解决，特别是在保护语义结构很重要的情况下。传统的图像级相似度指标的用途有限，因为图像的语义是高级的，并且不受像素级对原始图像的忠实度的强烈支配。为了填补这个空白，我们引入了 SAMScore，这是一个通用的语义结构相似性指标，用于评估图像翻译模型的可靠性。SAMScore 是基于最新的高性能分段任意模型(Segment AnyModel，SAM) ，它可以执行语义相似度比较，具有突出的准确性。我们将 SAMScore 应用于19个图像翻译任务，发现它在所有任务中的表现都优于其他竞争指标。我们设想 SAMScore 将被证明是一个有价值的工具，通过允许对新的和不断发展的翻译模式进行更精确的评估，将有助于推动图像翻译这一充满活力的领域。密码可在 https://github.com/kent0n-li/samscore 查阅。"
    },
    {
        "title": "Boundary Attention Mapping (BAM): Fine-grained saliency maps for\n  segmentation of Burn Injuries",
        "url": "http://arxiv.org/abs/2305.15365v1",
        "pub_date": "2023-05-24",
        "summary": "Burn injuries can result from mechanisms such as thermal, chemical, and\nelectrical insults. A prompt and accurate assessment of burns is essential for\ndeciding definitive clinical treatments. Currently, the primary approach for\nburn assessments, via visual and tactile observations, is approximately 60%-80%\naccurate. The gold standard is biopsy and a close second would be non-invasive\nmethods like Laser Doppler Imaging (LDI) assessments, which have up to 97%\naccuracy in predicting burn severity and the required healing time. In this\npaper, we introduce a machine learning pipeline for assessing burn severities\nand segmenting the regions of skin that are affected by burn. Segmenting 2D\ncolour images of burns allows for the injured versus non-injured skin to be\ndelineated, clearly marking the extent and boundaries of the localized\nburn/region-of-interest, even during remote monitoring of a burn patient. We\ntrained a convolutional neural network (CNN) to classify four severities of\nburns. We built a saliency mapping method, Boundary Attention Mapping (BAM),\nthat utilises this trained CNN for the purpose of accurately localizing and\nsegmenting the burn regions from skin burn images. We demonstrated the\neffectiveness of our proposed pipeline through extensive experiments and\nevaluations using two datasets; 1) A larger skin burn image dataset consisting\nof 1684 skin burn images of four burn severities, 2) An LDI dataset that\nconsists of a total of 184 skin burn images with their associated LDI scans.\nThe CNN trained using the first dataset achieved an average F1-Score of 78% and\nmicro/macro- average ROC of 85% in classifying the four burn severities.\nMoreover, a comparison between the BAM results and LDI results for measuring\ninjury boundary showed that the segmentations generated by our method achieved\n91.60% accuracy, 78.17% sensitivity, and 93.37% specificity.",
        "translated": "烧伤可能是由热、化学和电气等机制造成的。及时和准确的评估烧伤是决定明确的临床治疗是必不可少的。目前，烧伤评估的主要方法，通过视觉和触觉观察，大约60% -80% 的准确率。黄金标准是活组织检查，紧随其后的是非侵入性方法，如激光多普勒成像(LDI)评估，它在预测烧伤严重程度和所需的愈合时间方面有高达97% 的准确性。在本文中，我们介绍了一个机器学习管道，用于评估烧伤的严重程度和分割皮肤受烧伤影响的区域。分割烧伤的二维彩色图像允许描绘受伤与未受伤的皮肤，即使在远程监测烧伤患者期间，也清楚地标记局部烧伤/感兴趣区域的范围和边界。我们训练了一个卷积神经网络(CNN)来分类烧伤的四种严重程度。我们建立了一个突出映射方法，边界注意映射(BAM) ，利用这个训练的 CNN 的目的是准确定位和分割烧伤区域从皮肤烧伤图像。我们通过使用两个数据集进行广泛的实验和评估，证明了我们提出的管道的有效性; 1)由1684个四种烧伤严重程度的皮肤烧伤图像组成的更大的皮肤烧伤图像数据集，2)总共由184个皮肤烧伤图像及其相关的 LDI 扫描组成的 LDI 数据集。使用第一个数据集训练的美国有线电视新闻网在对四种烧伤严重程度进行分类时，平均达到了78% 的 f1-得分和85% 的微观/宏观平均 ROC。此外，测量损伤边界的 BAM 结果与 LDI 结果之间的比较显示，由我们的方法产生的分割达到91.60% 的准确性，78.17% 的灵敏度和93.37% 的特异性。"
    },
    {
        "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image\n  Super-Resolution",
        "url": "http://arxiv.org/abs/2305.15357v1",
        "pub_date": "2023-05-24",
        "summary": "Diffusion models, as a kind of powerful generative model, have given\nimpressive results on image super-resolution (SR) tasks. However, due to the\nrandomness introduced in the reverse process of diffusion models, the\nperformances of diffusion-based SR models are fluctuating at every time of\nsampling, especially for samplers with few resampled steps. This inherent\nrandomness of diffusion models results in ineffectiveness and instability,\nmaking it challenging for users to guarantee the quality of SR results.\nHowever, our work takes this randomness as an opportunity: fully analyzing and\nleveraging it leads to the construction of an effective plug-and-play sampling\nmethod that owns the potential to benefit a series of diffusion-based SR\nmethods. More in detail, we propose to steadily sample high-quality SR images\nfrom pretrained diffusion-based SR models by solving diffusion ordinary\ndifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)\nand analyze the characteristics between the choices of BCs and their\ncorresponding SR results. Our analysis shows the route to obtain an\napproximately optimal BC via an efficient exploration in the whole space. The\nquality of SR results sampled by the proposed method with fewer steps\noutperforms the quality of results sampled by current methods with randomness\nfrom the same pretrained diffusion-based SR model, which means that our\nsampling method ``boosts'' current diffusion-based SR models without any\nadditional training.",
        "translated": "扩散模型作为一种强大的生成模型，在图像超分辨率(SR)任务中给出了令人印象深刻的结果。然而，由于扩散模型反向过程的随机性，基于扩散的 SR 模型在每次采样时的性能都会发生波动，特别是对于重采样步数较少的采样者。这种扩散模型固有的随机性导致无效性和不稳定性，使得用户难以保证 SR 结果的质量。然而，我们的工作把这种随机性作为一个机会: 充分分析和利用它导致建立一个有效的即插即用的抽样方法，拥有受益于一系列基于扩散的 SR 方法的潜力。具体来说，我们提出了通过求解具有最优边界条件的扩散常微分方程，从基于扩散的预训练 SR 模型中稳定地采集高质量的 SR 图像，并分析了扩散常微分方程的选择与其相应 SR 结果之间的特点。我们的分析表明，通过在整个空间进行有效的探索，可以获得一个近似最优 BC 的路径。本文提出的方法采样的 SR 结果的质量较少的步骤优于目前的方法采样的结果的质量与随机性从相同的预先训练的扩散为基础的 SR 模型，这意味着我们的采样方法“推动”目前的扩散为基础的 SR 模型没有任何额外的训练。"
    },
    {
        "title": "Mitigating Biased Activation in Weakly-supervised Object Localization\n  via Counterfactual Learning",
        "url": "http://arxiv.org/abs/2305.15354v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we focus on an under-explored issue of biased activation in\nprior weakly-supervised object localization methods based on Class Activation\nMapping (CAM). We analyze the cause of this problem from a causal view and\nattribute it to the co-occurring background confounders. Following this\ninsight, we propose a novel Counterfactual Co-occurring Learning (CCL) paradigm\nto synthesize the counterfactual representations via coupling constant\nforeground and unrealized backgrounds in order to cut off their co-occurring\nrelationship. Specifically, we design a new network structure called\nCounterfactual-CAM, which embeds the counterfactual representation perturbation\nmechanism into the vanilla CAM-based model. This mechanism is responsible for\ndecoupling foreground as well as background and synthesizing the counterfactual\nrepresentations. By training the detection model with these synthesized\nrepresentations, we compel the model to focus on the constant foreground\ncontent while minimizing the influence of distracting co-occurring background.\nTo our best knowledge, it is the first attempt in this direction. Extensive\nexperiments on several benchmarks demonstrate that Counterfactual-CAM\nsuccessfully mitigates the biased activation problem, achieving improved object\nlocalization accuracy.",
        "translated": "本文研究了基于类激活映射(CAM)的弱监督目标定位方法中存在的偏向激活问题。我们从因果关系的角度分析了这一问题的原因，并将其归因于共同发生的背景混杂因素。根据这一观点，我们提出了一种新的反事实共现学习范式，通过耦合常数前景和未实现背景来综合反事实表征，以切断它们之间的共现关系。具体来说，我们设计了一种新的网络结构，称为反事实-CAM，它将反事实表示扰动机制嵌入到普通的基于 CAM 的模型中。该机制负责解耦前景和背景，并综合反事实表示。通过训练这些综合表征的检测模型，我们迫使模型集中在恒定的前景内容，同时尽量减少干扰共现背景的影响。据我们所知，这是朝这个方向的第一次尝试。在几个基准上的大量实验表明，反事实 CAM 成功地缓解了有偏激活问题，提高了目标定位精度。"
    },
    {
        "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.16322v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-Image diffusion models have made tremendous progress over the past\ntwo years, enabling the generation of highly realistic images based on\nopen-domain text descriptions. However, despite their success, text\ndescriptions often struggle to adequately convey detailed controls, even when\ncomposed of long and complex texts. Moreover, recent studies have also shown\nthat these models face challenges in understanding such complex texts and\ngenerating the corresponding images. Therefore, there is a growing need to\nenable more control modes beyond text description. In this paper, we introduce\nUni-ControlNet, a novel approach that allows for the simultaneous utilization\nof different local controls (e.g., edge maps, depth map, segmentation masks)\nand global controls (e.g., CLIP image embeddings) in a flexible and composable\nmanner within one model. Unlike existing methods, Uni-ControlNet only requires\nthe fine-tuning of two additional adapters upon frozen pre-trained\ntext-to-image diffusion models, eliminating the huge cost of training from\nscratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet\nonly necessitates a constant number (i.e., 2) of adapters, regardless of the\nnumber of local or global controls used. This not only reduces the fine-tuning\ncosts and model size, making it more suitable for real-world deployment, but\nalso facilitate composability of different conditions. Through both\nquantitative and qualitative comparisons, Uni-ControlNet demonstrates its\nsuperiority over existing methods in terms of controllability, generation\nquality and composability. Code is available at\n\\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.",
        "translated": "在过去的两年中，文本到图像的扩散模型取得了巨大的进展，使得基于开放域文本描述的高度真实感图像的生成成为可能。然而，尽管成功，文本描述往往难以充分传达详细的控制，即使是组成了长期和复杂的文本。此外，最近的研究也表明，这些模型在理解这些复杂的文本和生成相应的图像面临挑战。因此，越来越需要在文本描述之外启用更多的控制模式。在本文中，我们介绍了 Uni-ControlNet，一种新的方法，允许同时利用不同的局部控件(例如，边缘映射，深度映射，分割掩码)和全局控件(例如，CLIP 图像嵌入)在一个灵活和可组合的方式在一个模型。与现有的方法不同，Uni-ControlNet 只需要在冻结的预先训练的文本到图像扩散模型上对另外两个适配器进行微调，从而消除了从头开始训练的巨大成本。此外，由于一些专用的适配器设计，Uni-ControlNet 只需要一个常数(即2)的适配器，而不管所使用的本地或全局控件的数量。这不仅降低了微调成本和模型大小，使其更适合于实际部署，而且还促进了不同条件的可组合性。通过定量和定性比较，Uni-ControlNet 在可控性、生成质量和可组合性等方面均优于现有方法。代码可在网址{ https://github.com/shihaozhaozsh/uni-controlnet }下载。"
    },
    {
        "title": "Eclipse: Disambiguating Illumination and Materials using Unintended\n  Shadows",
        "url": "http://arxiv.org/abs/2305.16321v1",
        "pub_date": "2023-05-25",
        "summary": "Decomposing an object's appearance into representations of its materials and\nthe surrounding illumination is difficult, even when the object's 3D shape is\nknown beforehand. This problem is ill-conditioned because diffuse materials\nseverely blur incoming light, and is ill-posed because diffuse materials under\nhigh-frequency lighting can be indistinguishable from shiny materials under\nlow-frequency lighting. We show that it is possible to recover precise\nmaterials and illumination -- even from diffuse objects -- by exploiting\nunintended shadows, like the ones cast onto an object by the photographer who\nmoves around it. These shadows are a nuisance in most previous inverse\nrendering pipelines, but here we exploit them as signals that improve\nconditioning and help resolve material-lighting ambiguities. We present a\nmethod based on differentiable Monte Carlo ray tracing that uses images of an\nobject to jointly recover its spatially-varying materials, the surrounding\nillumination environment, and the shapes of the unseen light occluders who\ninadvertently cast shadows upon it.",
        "translated": "分解一个物体的外观到其材料和周围照明的表示是困难的，即使当物体的三维形状是已知的。这个问题是病态的，因为漫反射材料严重模糊入射光，而且是病态的，因为在高频照明下漫反射材料可以与低频照明下发光材料难以区分。我们展示了通过利用意想不到的阴影(如摄影师在物体周围移动时投射到物体上的阴影)来恢复精确的材料和照明——即使是从漫反射的物体上也是可能的。这些阴影在大多数以前的反向渲染管道中是一个麻烦，但在这里我们利用它们作为改善条件反射和帮助解决材质-照明模糊的信号。我们提出了一种基于可微蒙特卡罗射线追踪的方法，该方法利用一个物体的图像来共同恢复其空间变化的材料，周围的照明环境，以及无意中在其上投射阴影的看不见的光遮挡物的形状。"
    },
    {
        "title": "Image is First-order Norm+Linear Autoregressive",
        "url": "http://arxiv.org/abs/2305.16319v1",
        "pub_date": "2023-05-25",
        "summary": "This paper reveals that every image can be understood as a first-order\nnorm+linear autoregressive process, referred to as FINOLA, where norm+linear\ndenotes the use of normalization before the linear model. We demonstrate that\nimages of size 256$\\times$256 can be reconstructed from a compressed vector\nusing autoregression up to a 16$\\times$16 feature map, followed by upsampling\nand convolution. This discovery sheds light on the underlying partial\ndifferential equations (PDEs) governing the latent feature space. Additionally,\nwe investigate the application of FINOLA for self-supervised learning through a\nsimple masked prediction technique. By encoding a single unmasked quadrant\nblock, we can autoregressively predict the surrounding masked region.\nRemarkably, this pre-trained representation proves effective for image\nclassification and object detection tasks, even in lightweight networks,\nwithout requiring fine-tuning. The code will be made publicly available.",
        "translated": "本文揭示了每幅图像都可以理解为一阶范数 + 线性自回归过程，称为 FINOLA，其中范数 + 线性表示在线性模型之前使用归一化。我们证明了大小为256美元乘以256美元的图像可以从一个压缩向量重建使用自回归高达16美元乘以16美元的特征映射，然后上采样和卷积。这一发现揭示了控制潜在特征空间的基本偏微分方程(PDE)。此外，我们还通过一个简单的掩蔽预测技术研究了 FINOLA 在自监督学习中的应用。通过编码一个未遮蔽的象限块，我们可以自回归地预测周围的遮蔽区域。值得注意的是，这种预先训练的表示被证明对图像分类和目标检测任务非常有效，即使在轻量级网络中，也不需要进行微调。代码将公开发布。"
    },
    {
        "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation",
        "url": "http://arxiv.org/abs/2305.16318v1",
        "pub_date": "2023-05-25",
        "summary": "Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.",
        "translated": "近年来，基于语言、音频等多模态信号的视频对象分割技术引起了业界和学术界的广泛关注。这对于探索模式内的语义对齐和跨框架的视觉对应是一个挑战。然而，现有的方法针对不同的模式采用不同的网络结构，而忽略了帧间与参考文献的时间交互。本文提出了一种多模态统一时态转换器 MUTR，用于参考视频对象分割。MUTR 首次采用了统一的框架，采用了 DETR 风格的变换器，能够对文本或音频参考指定的视频对象进行分割。具体来说，我们引入了两种策略来充分探索视频和多模态信号之间的时间关系。首先，对于转换前的低级时间聚合，我们使多模态参考能够从连续的视频帧中捕获多尺度的视觉线索。这有效地赋予了文本或音频信号时间知识，并提高了形态之间的语义对齐。其次，对于变压器后的高层次时间交互，针对不同的目标嵌入进行帧间特征通信，有助于提高视频跟踪的目标对应性。在 Ref-YouTube-VOS 和 AVSBench 数据集上，各自的文本和音频参考，MUTR 对最先进的方法实现了 + 4.2% 和 + 4.2% 的 J & F 改进，表明了我们对统一的多模态 VOS 的重要性。代码在 https://github.com/opengvlab/mutr 发布。"
    },
    {
        "title": "Making Vision Transformers Truly Shift-Equivariant",
        "url": "http://arxiv.org/abs/2305.16316v1",
        "pub_date": "2023-05-25",
        "summary": "For computer vision tasks, Vision Transformers (ViTs) have become one of the\ngo-to deep net architectures. Despite being inspired by Convolutional Neural\nNetworks (CNNs), ViTs remain sensitive to small shifts in the input image. To\naddress this, we introduce novel designs for each of the modules in ViTs, such\nas tokenization, self-attention, patch merging, and positional encoding. With\nour proposed modules, we achieve truly shift-equivariant ViTs on four\nwell-established models, namely, Swin, SwinV2, MViTv2, and CvT, both in theory\nand practice. Empirically, we tested these models on image classification and\nsemantic segmentation, achieving competitive performance across three different\ndatasets while maintaining 100% shift consistency.",
        "translated": "在计算机视觉任务中，视觉变换器(ViTs)已经成为一种常用的深层网络体系结构。尽管受到卷积神经网络(CNN)的启发，ViTs 仍然对输入图像的微小变化敏感。为了解决这个问题，我们为 ViT 中的每个模块引入了新的设计，例如标记化、自注意、补丁合并和位置编码。通过我们提出的模块，我们在理论和实践上实现了四个已经建立的模型，即 Swin，SwinV2，MViTv2和 CvT 上的真正的移位等变 VIT。经验上，我们在图像分类和语义分割上测试了这些模型，在保持100% 移位一致性的同时，在三个不同的数据集上实现了竞争性能。"
    },
    {
        "title": "NAP: Neural 3D Articulation Prior",
        "url": "http://arxiv.org/abs/2305.16315v1",
        "pub_date": "2023-05-25",
        "summary": "We propose Neural 3D Articulation Prior (NAP), the first 3D deep generative\nmodel to synthesize 3D articulated object models. Despite the extensive\nresearch on generating 3D objects, compositions, or scenes, there remains a\nlack of focus on capturing the distribution of articulated objects, a common\nobject category for human and robot interaction. To generate articulated\nobjects, we first design a novel articulation tree/graph parameterization and\nthen apply a diffusion-denoising probabilistic model over this representation\nwhere articulated objects can be generated via denoising from random complete\ngraphs. In order to capture both the geometry and the motion structure whose\ndistribution will affect each other, we design a graph-attention denoising\nnetwork for learning the reverse diffusion process. We propose a novel distance\nthat adapts widely used 3D generation metrics to our novel task to evaluate\ngeneration quality, and experiments demonstrate our high performance in\narticulated object generation. We also demonstrate several conditioned\ngeneration applications, including Part2Motion, PartNet-Imagination,\nMotion2Part, and GAPart2Object.",
        "translated": "我们提出了神经三维关节优先级(nAP) ，这是第一个合成三维关节物体模型的三维深度生成模型。尽管在生成三维物体、构图或场景方面有着广泛的研究，但是对于捕捉关节物体的分布这一人机交互的常见对象类别仍然缺乏关注。为了生成关节对象，我们首先设计了一个新的关节树/图形参量化，然后应用扩散去噪概率模型，通过从随机完整图中去噪来生成关节对象。为了捕捉反向扩散过程中几何和运动结构的相互影响，我们设计了一个图注意去噪网络来学习反向扩散过程。我们提出了一个新的距离，适应广泛使用的三维生成度量的新任务，以评估生成质量，实验表明我们的高性能的铰接对象生成。我们还演示了几个条件生成应用程序，包括 Part2Motion、 PartNet-Imagination、 Motion2Part 和 GAPart2Object。"
    },
    {
        "title": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with\n  Inter-Part Equivariance",
        "url": "http://arxiv.org/abs/2305.16314v1",
        "pub_date": "2023-05-25",
        "summary": "Equivariance has gained strong interest as a desirable network property that\ninherently ensures robust generalization. However, when dealing with complex\nsystems such as articulated objects or multi-object scenes, effectively\ncapturing inter-part transformations poses a challenge, as it becomes entangled\nwith the overall structure and local transformations. The interdependence of\npart assignment and per-part group action necessitates a novel equivariance\nformulation that allows for their co-evolution. In this paper, we present\nBanana, a Banach fixed-point network for equivariant segmentation with\ninter-part equivariance by construction. Our key insight is to iteratively\nsolve a fixed-point problem, where point-part assignment labels and per-part\nSE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations\nof both per-step equivariance and global convergence, which induces an\nequivariant final convergent state. Our formulation naturally provides a strict\ndefinition of inter-part equivariance that generalizes to unseen inter-part\nconfigurations. Through experiments conducted on both articulated objects and\nmulti-object scans, we demonstrate the efficacy of our approach in achieving\nstrong generalization under inter-part transformations, even when confronted\nwith substantial changes in pointcloud geometry and topology.",
        "translated": "等方差作为一种理想的网络属性，在本质上确保了鲁棒的推广，已经引起了人们的极大兴趣。然而，当处理复杂的系统，如铰接对象或多对象场景，有效地捕获部分间的转换提出了一个挑战，因为它成为纠缠在整体结构和局部转换。部分分配和每部分群体行为的相互依赖性需要一个新的等方差公式，允许它们的协同演化。本文提出了一种基于构造的 Banana 不动点网络，用于部分间等方差的等变分割。我们的主要见解是迭代求解一个不动点问题，其中点部分分配标签和每部分 SE (3)-等方差同时共同演化。我们给出了每步等方差和全局收敛的理论推导，得到了一个等变的最终收敛状态。我们的公式自然提供了一个严格的部分间等方差的定义，推广到看不见的部分间配置。通过对关联对象和多目标扫描的实验，我们证明了该方法在部分间转换下实现强泛化的有效性，即使在面临点云几何和拓扑结构的实质性变化时也是如此。"
    },
    {
        "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
        "url": "http://arxiv.org/abs/2305.16311v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-image model personalization aims to introduce a user-provided concept\nto the model, allowing its synthesis in diverse contexts. However, current\nmethods primarily focus on the case of learning a single concept from multiple\nimages with variations in backgrounds and poses, and struggle when adapted to a\ndifferent scenario. In this work, we introduce the task of textual scene\ndecomposition: given a single image of a scene that may contain several\nconcepts, we aim to extract a distinct text token for each concept, enabling\nfine-grained control over the generated scenes. To this end, we propose\naugmenting the input image with masks that indicate the presence of target\nconcepts. These masks can be provided by the user or generated automatically by\na pre-trained segmentation model. We then present a novel two-phase\ncustomization process that optimizes a set of dedicated textual embeddings\n(handles), as well as the model weights, striking a delicate balance between\naccurately capturing the concepts and avoiding overfitting. We employ a masked\ndiffusion loss to enable handles to generate their assigned concepts,\ncomplemented by a novel loss on cross-attention maps to prevent entanglement.\nWe also introduce union-sampling, a training strategy aimed to improve the\nability of combining multiple concepts in generated images. We use several\nautomatic metrics to quantitatively compare our method against several\nbaselines, and further affirm the results using a user study. Finally, we\nshowcase several applications of our method. Project page is available at:\nhttps://omriavrahami.com/break-a-scene/",
        "translated": "文本到图像模型个性化旨在向模型引入用户提供的概念，允许在不同的上下文中进行合成。然而，目前的方法主要集中在从背景和姿势不同的多幅图像中学习单一概念的情况下，并在适应不同情景时进行斗争。在这项工作中，我们介绍了文本场景分解的任务: 给定一个场景的单个图像，可能包含几个概念，我们的目标是提取一个不同的文本标记为每个概念，使细粒度控制生成的场景。为此，我们提出用掩码来增强输入图像，以表明目标概念的存在。这些掩码可以由用户提供，也可以由预先训练好的分割模型自动生成。然后，我们提出了一个新的两阶段定制过程，优化了一组专用的文本嵌入(处理) ，以及模型权重，在准确捕捉概念和避免过度拟合之间找到人海万花筒(电影)。我们使用一个掩蔽的扩散损失，使处理能够生成其指定的概念，补充交叉注意地图上的一个新的损失，以防止纠缠。我们还引入了联合采样，这是一种旨在提高生成图像中多个概念组合能力的训练策略。我们使用几个自动指标来定量比较我们的方法与几个基线，并进一步确认结果使用用户研究。最后，我们展示了我们的方法的几个应用。项目网页可于以下 https://omriavrahami.com/break-a-scene/下载:"
    },
    {
        "title": "UMat: Uncertainty-Aware Single Image High Resolution Material Capture",
        "url": "http://arxiv.org/abs/2305.16312v1",
        "pub_date": "2023-05-25",
        "summary": "We propose a learning-based method to recover normals, specularity, and\nroughness from a single diffuse image of a material, using microgeometry\nappearance as our primary cue. Previous methods that work on single images tend\nto produce over-smooth outputs with artifacts, operate at limited resolution,\nor train one model per class with little room for generalization. Previous\nmethods that work on single images tend to produce over-smooth outputs with\nartifacts, operate at limited resolution, or train one model per class with\nlittle room for generalization. In contrast, in this work, we propose a novel\ncapture approach that leverages a generative network with attention and a U-Net\ndiscriminator, which shows outstanding performance integrating global\ninformation at reduced computational complexity. We showcase the performance of\nour method with a real dataset of digitized textile materials and show that a\ncommodity flatbed scanner can produce the type of diffuse illumination required\nas input to our method. Additionally, because the problem might be illposed\n-more than a single diffuse image might be needed to disambiguate the specular\nreflection- or because the training dataset is not representative enough of the\nreal distribution, we propose a novel framework to quantify the model's\nconfidence about its prediction at test time. Our method is the first one to\ndeal with the problem of modeling uncertainty in material digitization,\nincreasing the trustworthiness of the process and enabling more intelligent\nstrategies for dataset creation, as we demonstrate with an active learning\nexperiment.",
        "translated": "我们提出了一个基于学习的方法来恢复法线，反射率和粗糙度从一个单一的漫反射图像的材料，使用显微几何外观作为我们的主要线索。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。相比之下，在这项工作中，我们提出了一种新的捕获方法，利用生成网络的注意力和 U-Net 鉴别器，它显示了在降低计算复杂度的情况下集成全局信息的出色性能。我们展示了我们的方法的性能与数字化纺织材料的真实数据集，并表明，一个商品平板扫描仪可以产生所需的漫反射照明类型作为输入我们的方法。此外，由于问题可能是病态的——消除镜面反射(物理)可能需要不止一张漫反射图像——或者由于训练数据集不足以代表真实分布，我们提出了一个新的框架来量化模型在测试时对其预测的信心。我们的方法是第一个处理材料数字化建模不确定性的问题，提高过程的可信度，使数据集创建更智能的策略，正如我们用一个积极的学习实验所证明的那样。"
    },
    {
        "title": "Securing Deep Generative Models with Universal Adversarial Signature",
        "url": "http://arxiv.org/abs/2305.16310v1",
        "pub_date": "2023-05-25",
        "summary": "Recent advances in deep generative models have led to the development of\nmethods capable of synthesizing high-quality, realistic images. These models\npose threats to society due to their potential misuse. Prior research attempted\nto mitigate these threats by detecting generated images, but the varying traces\nleft by different generative models make it challenging to create a universal\ndetector capable of generalizing to new, unseen generative models. In this\npaper, we propose to inject a universal adversarial signature into an arbitrary\npre-trained generative model, in order to make its generated contents more\ndetectable and traceable. First, the imperceptible optimal signature for each\nimage can be found by a signature injector through adversarial training.\nSubsequently, the signature can be incorporated into an arbitrary generator by\nfine-tuning it with the images processed by the signature injector. In this\nway, the detector corresponding to the signature can be reused for any\nfine-tuned generator for tracking the generator identity. The proposed method\nis validated on the FFHQ and ImageNet datasets with various state-of-the-art\ngenerative models, consistently showing a promising detection rate. Code will\nbe made publicly available at \\url{https://github.com/zengxianyu/genwm}.",
        "translated": "深度生成模型的最新进展导致了能够合成高质量、真实图像的方法的发展。这些模式由于可能被滥用而对社会构成威胁。先前的研究试图通过检测生成的图像来减轻这些威胁，但不同的生成模型留下的不同痕迹使得创建一个能够推广到新的、看不见的生成模型的通用检测器具有挑战性。在这篇文章中，我们建议将一个通用的对抗性签名注入到一个任意的预先训练的生成模型中，以使其生成的内容更加可检测和可追踪。首先，通过对抗训练，利用签名注入器可以找到每幅图像的不可察觉的最优签名。随后，可以通过使用签名注入器处理的图像对签名进行微调，从而将签名合并到任意生成器中。这样，对应于签名的检测器可以重用于任何微调发生器，用于跟踪发生器标识。该方法在 FFHQ 和 ImageNet 数据集上通过各种最先进的生成模型进行了验证，一致地显示出有希望的检测率。代码将在 url { https://github.com/zengxianyu/genwm }公开发布。"
    },
    {
        "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient\n  and High-Quality Rendering Support",
        "url": "http://arxiv.org/abs/2305.17134v1",
        "pub_date": "2023-05-26",
        "summary": "We present a method for generating high-quality watertight manifold meshes\nfrom multi-view input images. Existing volumetric rendering methods are robust\nin optimization but tend to generate noisy meshes with poor topology.\nDifferentiable rasterization-based methods can generate high-quality meshes but\nare sensitive to initialization. Our method combines the benefits of both\nworlds; we take the geometry initialization obtained from neural volumetric\nfields, and further optimize the geometry as well as a compact neural texture\nrepresentation with differentiable rasterizers. Through extensive experiments,\nwe demonstrate that our method can generate accurate mesh reconstructions with\nfaithful appearance that are comparable to previous volume rendering methods\nwhile being an order of magnitude faster in rendering. We also show that our\ngenerated mesh and neural texture reconstruction is compatible with existing\ngraphics pipelines and enables downstream 3D applications such as simulation.\nProject page: https://sarahweiii.github.io/neumanifold/",
        "translated": "提出了一种从多视图输入图像中生成高质量防水流形网格的方法。现有的体绘制方法在优化时具有鲁棒性，但容易产生拓扑结构较差的噪声网格。基于可微栅格化的方法可以生成高质量的网格，但对初始化非常敏感。我们的方法结合了两个世界的好处，我们采取的几何初始化获得的神经体积领域，并进一步优化的几何以及紧凑的神经纹理表示与可微光栅。通过大量的实验，我们证明了我们的方法可以产生准确的网格重建与忠实的外观相比，以前的立体渲染方法，同时是一个数量级更快的渲染。我们还表明，我们生成的网格和神经纹理重建与现有的图形管道兼容，并支持下游3D 应用程序，如模拟。项目主页:  https://sarahweiii.github.io/neumanifold/"
    },
    {
        "title": "Manifold Regularization for Memory-Efficient Training of Deep Neural\n  Networks",
        "url": "http://arxiv.org/abs/2305.17119v1",
        "pub_date": "2023-05-26",
        "summary": "One of the prevailing trends in the machine- and deep-learning community is\nto gravitate towards the use of increasingly larger models in order to keep\npushing the state-of-the-art performance envelope. This tendency makes access\nto the associated technologies more difficult for the average practitioner and\nruns contrary to the desire to democratize knowledge production in the field.\nIn this paper, we propose a framework for achieving improved memory efficiency\nin the process of learning traditional neural networks by leveraging\ninductive-bias-driven network design principles and layer-wise\nmanifold-oriented regularization objectives. Use of the framework results in\nimproved absolute performance and empirical generalization error relative to\ntraditional learning techniques. We provide empirical validation of the\nframework, including qualitative and quantitative evidence of its effectiveness\non two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed\nframework can be seamlessly combined with existing network compression methods\nfor further memory savings.",
        "translated": "机器和深度学习领域的一个流行趋势是，为了不断推进最先进的性能外壳，越来越多地使用更大的模型。这一趋势使得普通从业者更难获得相关技术，并与实地知识生产民主化的愿望背道而驰。本文提出了一种在学习传统神经网络过程中，利用感应偏置驱动的网络设计原理和面向分层流形的正则化目标来提高存储效率的框架。相对于传统的学习方法，使用这个框架可以提高绝对表现和经验泛化误差。我们提供了框架的经验验证，包括定性和定量的证据，其有效性的两个标准图像数据集，即 CIFAR-10和 CIFAR-100。该框架可以与现有的网络压缩方法无缝结合，以进一步节省内存。"
    },
    {
        "title": "Random-Access Neural Compression of Material Textures",
        "url": "http://arxiv.org/abs/2305.17105v1",
        "pub_date": "2023-05-26",
        "summary": "The continuous advancement of photorealism in rendering is accompanied by a\ngrowth in texture data and, consequently, increasing storage and memory\ndemands. To address this issue, we propose a novel neural compression technique\nspecifically designed for material textures. We unlock two more levels of\ndetail, i.e., 16x more texels, using low bitrate compression, with image\nquality that is better than advanced image compression techniques, such as AVIF\nand JPEG XL. At the same time, our method allows on-demand, real-time\ndecompression with random access similar to block texture compression on GPUs,\nenabling compression on disk and memory. The key idea behind our approach is\ncompressing multiple material textures and their mipmap chains together, and\nusing a small neural network, that is optimized for each material, to\ndecompress them. Finally, we use a custom training implementation to achieve\npractical compression speeds, whose performance surpasses that of general\nframeworks, like PyTorch, by an order of magnitude.",
        "translated": "随着渲染技术的不断进步，纹理数据也不断增加，存储和内存需求也随之增加。为了解决这个问题，我们提出了一种新的神经压缩技术，专门为材质纹理设计。我们使用低比特率压缩技术解锁更多的细节，即16倍以上的文本，其图像质量优于高级图像压缩技术，如 AVIF 和 JPEG XL。与此同时，我们的方法允许按需实时解压缩，随机访问类似于 gpUs 上的块纹理压缩，允许在磁盘和内存上进行压缩。我们方法背后的关键思想是将多种材质的纹理和它们的 mipmap 链压缩在一起，并使用一个针对每种材质优化的小型神经网络来解压缩它们。最后，我们使用一个定制的训练实现来实现实际的压缩速度，其性能超过了一般框架(如 PyTorch)的一个数量级。"
    },
    {
        "title": "GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot\n  Attention for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.17102v1",
        "pub_date": "2023-05-26",
        "summary": "Most existing works solving Room-to-Room VLN problem only utilize RGB images\nand do not consider local context around candidate views, which lack sufficient\nvisual cues about surrounding environment. Moreover, natural language contains\ncomplex semantic information thus its correlations with visual inputs are hard\nto model merely with cross attention. In this paper, we propose GeoVLN, which\nlearns Geometry-enhanced visual representation based on slot attention for\nrobust Visual-and-Language Navigation. The RGB images are compensated with the\ncorresponding depth maps and normal maps predicted by Omnidata as visual\ninputs. Technically, we introduce a two-stage module that combine local slot\nattention and CLIP model to produce geometry-enhanced representation from such\ninput. We employ V&amp;L BERT to learn a cross-modal representation that\nincorporate both language and vision informations. Additionally, a novel\nmultiway attention module is designed, encouraging different phrases of input\ninstruction to exploit the most related features from visual input. Extensive\nexperiments demonstrate the effectiveness of our newly designed modules and\nshow the compelling performance of the proposed method.",
        "translated": "现有的解决房间到房间 VLN 问题的作品大多只利用 RGB 图像，没有考虑候选视图周围的局部环境，缺乏对周围环境的足够的视觉线索。此外，自然语言包含复杂的语义信息，因此它与视觉输入的相关性很难仅仅通过交叉注意来建模。本文提出了一种基于时隙注意学习几何增强视觉表示的 GeoVLN，用于鲁棒的视觉语言导航。RGB 图像由相应的深度图和由 Omnidata 预测的法线图作为视觉输入进行补偿。在技术上，我们引入了一个两阶段的模块，结合本地时隙注意和 CLIP 模型，从这样的输入产生几何增强的表示。我们使用 V & L BERT 来学习包含语言和视觉信息的跨模态表示。此外，设计了一个新颖的多路注意模块，鼓励输入指令的不同阶段从视觉输入中发掘最相关的特征。大量的实验证明了我们新设计的模块的有效性，并显示了该方法的引人注目的性能。"
    },
    {
        "title": "ControlVideo: Adding Conditional Control for One Shot Text-to-Video\n  Editing",
        "url": "http://arxiv.org/abs/2305.17098v1",
        "pub_date": "2023-05-26",
        "summary": "In this paper, we present ControlVideo, a novel method for text-driven video\nediting. Leveraging the capabilities of text-to-image diffusion models and\nControlNet, ControlVideo aims to enhance the fidelity and temporal consistency\nof videos that align with a given text while preserving the structure of the\nsource video. This is achieved by incorporating additional conditions such as\nedge maps, fine-tuning the key-frame and temporal attention on the source\nvideo-text pair with carefully designed strategies. An in-depth exploration of\nControlVideo's design is conducted to inform future research on one-shot tuning\nvideo diffusion models. Quantitatively, ControlVideo outperforms a range of\ncompetitive baselines in terms of faithfulness and consistency while still\naligning with the textual prompt. Additionally, it delivers videos with high\nvisual realism and fidelity w.r.t. the source content, demonstrating\nflexibility in utilizing controls containing varying degrees of source video\ninformation, and the potential for multiple control combinations. The project\npage is available at\n\\href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}.",
        "translated": "本文提出了一种新的文本驱动视频编辑方法 ControlVideo。ControlVideo 利用文本到图像扩散模型和 ControlNet 的能力，旨在提高与给定文本对齐的视频的保真度和时间一致性，同时保留源视频的结构。这是通过加入额外的条件，如边缘地图，微调关键帧和时间注意力的源视频文本对与精心设计的策略。本文对 ControlVideo 的设计进行了深入的探索，为今后一次调谐视频扩散模型的研究提供了参考。在数量上，ControlVideo 在忠实性和一致性方面优于一系列竞争性基线，同时仍然与文本提示保持一致。此外，它提供的视频具有高度的视觉真实感和源内容的保真度，展示了在利用包含不同程度的源视频信息的控件方面的灵活性，以及多种控件组合的潜力。项目页面可在 href { https://ml.cs.tsinghua.edu.cn/controlvideo/}{ https://ml.cs.tsinghua.edu.cn/controlvideo/}获得。"
    },
    {
        "title": "GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance\n  Segmentation",
        "url": "http://arxiv.org/abs/2305.17096v1",
        "pub_date": "2023-05-26",
        "summary": "Recent trends in Video Instance Segmentation (VIS) have seen a growing\nreliance on online methods to model complex and lengthy video sequences.\nHowever, the degradation of representation and noise accumulation of the online\nmethods, especially during occlusion and abrupt changes, pose substantial\nchallenges. Transformer-based query propagation provides promising directions\nat the cost of quadratic memory attention. However, they are susceptible to the\ndegradation of instance features due to the above-mentioned challenges and\nsuffer from cascading effects. The detection and rectification of such errors\nremain largely underexplored. To this end, we introduce \\textbf{GRAtt-VIS},\n\\textbf{G}ated \\textbf{R}esidual \\textbf{Att}ention for \\textbf{V}ideo\n\\textbf{I}nstance \\textbf{S}egmentation. Firstly, we leverage a\nGumbel-Softmax-based gate to detect possible errors in the current frame. Next,\nbased on the gate activation, we rectify degraded features from its past\nrepresentation. Such a residual configuration alleviates the need for dedicated\nmemory and provides a continuous stream of relevant instance features.\nSecondly, we propose a novel inter-instance interaction using gate activation\nas a mask for self-attention. This masking strategy dynamically restricts the\nunrepresentative instance queries in the self-attention and preserves vital\ninformation for long-term tracking. We refer to this novel combination of Gated\nResidual Connection and Masked Self-Attention as \\textbf{GRAtt} block, which\ncan easily be integrated into the existing propagation-based framework.\nFurther, GRAtt blocks significantly reduce the attention overhead and simplify\ndynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on\nYouTube-VIS and the highly challenging OVIS dataset, significantly improving\nover previous methods. Code is available at\n\\url{https://github.com/Tanveer81/GRAttVIS}.",
        "translated": "视频实例分割(VIS)最近的发展趋势是越来越依赖于在线方法来建模复杂和冗长的视频序列。然而，在线方法的表示退化和噪声积累，特别是在遮挡和突变过程中，带来了实质性的挑战。基于变压器的查询传播以二次内存注意力为代价提供了有希望的方向。然而，由于上述挑战，它们很容易受到实例特性退化的影响，并受到级联效应的影响。这些错误的发现和纠正仍然在很大程度上没有得到充分的探索。为此，我们引入 textbf { GRAtt-VIS } ，textbf { G }化 textbf { R }剩余 textbf { Att }为 textbf { V } ideo textbf { I } nstance textbf { S }分割。首先，我们利用 Gumbel-Softmax 门检测当前帧中可能存在的错误。接下来，基于门激活，我们纠正退化的特征从其过去的表示。这种剩余配置减轻了对专用内存的需求，并提供了相关实例特性的连续流。其次，我们提出了一种新的使用门激活作为自我注意掩蔽的实例间交互。这种掩蔽策略动态地限制了自注意中不具代表性的实例查询，并且为长期跟踪保留了重要信息。我们把这种门控残余连接和掩蔽自我注意的新颖组合称为 textbf { GRAtt }块，它可以很容易地集成到现有的基于传播的框架中。此外，GRAtt 块显著降低了注意开销，简化了动态时间建模。GRATt-VIS 在 YouTube-VIS 和极具挑战性的 OVIS 数据集上实现了最先进的性能，明显改进了以前的方法。代码可在网址{ https://github.com/tanveer81/grattvis }下载。"
    },
    {
        "title": "SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox\n  Based on PyTorch",
        "url": "http://arxiv.org/abs/2305.17091v1",
        "pub_date": "2023-05-26",
        "summary": "This paper presents SSSegmenation, which is an open source supervised\nsemantic image segmentation toolbox based on PyTorch. The design of this\ntoolbox is motivated by MMSegmentation while it is easier to use because of\nfewer dependencies and achieves superior segmentation performance under a\ncomparable training and testing setup. Moreover, the toolbox also provides\nplenty of trained weights for popular and contemporary semantic segmentation\nmethods, including Deeplab, PSPNet, OCRNet, MaskFormer, \\emph{etc}. We expect\nthat this toolbox can contribute to the future development of semantic\nsegmentation. Codes and model zoos are available at\n\\href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.",
        "translated": "这篇文章介绍了一个基于 PyTorch 的开源监督语义图像分割工具箱。这个工具箱的设计是由 MMS 分割驱动的，它更容易使用，因为较少的依赖性和实现卓越的分割性能在一个可比较的培训和测试设置。此外，该工具箱还为流行的和当代的语义分割方法提供了大量的训练权重，包括 Deeplab、 PSPNet、 OCRNet、 MaskForm、 emph {等}。我们期望这个工具箱能够为语义分割的未来发展做出贡献。代码和模型动物园可在 href { https://github.com/segmentationblwx/sssegmentation/}{ SSSegmenation }获得。"
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "url": "http://arxiv.org/abs/2305.17066v1",
        "pub_date": "2023-05-26",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "translated": "明斯基的“心智社会”和施密德胡贝尔的“学会思考”都激发了大型多模式神经网络(NN)的多样化社会，这些神经网络通过在“思维风暴”中相互访谈来解决问题最近基于神经网络的思维社会的实现包括大型语言模型(LLM)和其他基于神经网络的专家通过自然语言接口进行交流。这样，他们克服了单个 LLM 的局限性，改进了多模态零点推理。在这些以自然语言为基础的思维社会(NLSOM)中，新的主体——所有通过相同的通用符号语言进行交流的主体——很容易以模块化的方式添加进来。为了展示 NLSOM 的威力，我们组装并实验了其中的几个(拥有多达129个成员) ，利用其中的思维风暴来解决一些实际的人工智能任务: 视觉问题回答，图像字幕，文本到图像合成，3D 生成，自我中心检索，体现人工智能，以及一般的基于语言的任务解决。我们认为这是通往拥有数十亿代理人(其中一些可能是人类)的更大的非线性有机体的一个起点。随着这个由异质思维组成的伟大社会的出现，许多新的研究问题突然变得对人工智能的未来至关重要。什么是一个 NLSOM 的社会结构？拥有君主制而不是民主制的结构有什么好处？如何运用神经网络经济学的原理来最大限度地提高强化学习非劳动就业市场的总回报？在这项工作中，我们确定，讨论，并试图回答其中的一些问题。"
    },
    {
        "title": "Extremely weakly-supervised blood vessel segmentation with\n  physiologically based synthesis and domain adaptation",
        "url": "http://arxiv.org/abs/2305.17054v1",
        "pub_date": "2023-05-26",
        "summary": "Accurate analysis and modeling of renal functions require a precise\nsegmentation of the renal blood vessels. Micro-CT scans provide image data at\nhigher resolutions, making more small vessels near the renal cortex visible.\nAlthough deep-learning-based methods have shown state-of-the-art performance in\nautomatic blood vessel segmentations, they require a large amount of labeled\ntraining data. However, voxel-wise labeling in micro-CT scans is extremely\ntime-consuming given the huge volume sizes. To mitigate the problem, we\nsimulate synthetic renal vascular trees physiologically while generating\ncorresponding scans of the simulated trees by training a generative model on\nunlabeled scans. This enables the generative model to learn the mapping\nimplicitly without the need for explicit functions to emulate the image\nacquisition process. We further propose an additional segmentation branch over\nthe generative model trained on the generated scans. We demonstrate that the\nmodel can directly segment blood vessels on real scans and validate our method\non both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on\n2D retinal images. Code and 3D results are available at\nhttps://github.com/miccai2023anony/RenalVesselSeg",
        "translated": "精确的肾功能分析和建模需要精确的肾血管分割。微型 CT 扫描提供更高分辨率的图像数据，使肾皮质附近更多的小血管可见。尽管基于深度学习的方法在自动血管分割中表现出了最先进的性能，但是它们需要大量的标记训练数据。然而，在显微 CT 扫描的体素标记是非常耗时的，因为巨大的体积大小。为了缓解这个问题，我们在生理学上模拟合成肾血管树，同时通过训练一个生成模型对未标记的扫描产生相应的模拟树扫描。这使得生成模型能够隐式地学习映射，而不需要显式的函数来模拟图像采集过程。我们进一步提出了一个额外的分割分支超过生成模型训练生成的扫描。我们证明该模型可以直接分割血管的实际扫描和验证我们的方法都三维显微 CT 扫描大鼠肾脏和二维视网膜图像的概念验证实验。编码及立体效果可于 https://github.com/miccai2023anony/renalvesselseg 下载"
    },
    {
        "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
        "url": "http://arxiv.org/abs/2305.17048v1",
        "pub_date": "2023-05-26",
        "summary": "Most commonly used benchmark datasets for computer vision contain irrelevant\nimages, near duplicates, and label errors. Consequently, model performance on\nthese benchmarks may not be an accurate estimate of generalization ability.\nThis is a particularly acute concern in computer vision for medicine where\ndatasets are typically small, stakes are high, and annotation processes are\nexpensive and error-prone. In this paper, we propose SelfClean, a general\nprocedure to clean up image datasets exploiting a latent space learned with\nself-supervision. By relying on self-supervised learning, our approach focuses\non intrinsic properties of the data and avoids annotation biases. We formulate\ndataset cleaning as either a set of ranking problems, where human experts can\nmake decisions with significantly reduced effort, or a set of scoring problems,\nwhere decisions can be fully automated based on score distributions. We compare\nSelfClean against other algorithms on common computer vision benchmarks\nenhanced with synthetic noise and demonstrate state-of-the-art performance on\ndetecting irrelevant images, near duplicates, and label errors. In addition, we\napply our method to multiple image datasets and confirm an improvement in\nevaluation reliability.",
        "translated": "最常用的计算机视觉基准数据集包含不相关的图像、接近重复的图像和标签错误。因此，这些基准上的模型性能可能不是泛化能力的准确估计。在医学计算机视觉领域，这是一个特别严重的问题，因为数据集通常很小，风险很高，注释过程昂贵且容易出错。在本文中，我们提出了自我清理，一个通用的程序来清理图像数据集利用潜在的空间学习与自我监督。通过依赖于自监督学习，我们的方法侧重于数据的内在属性，避免了注释偏差。我们将数据集清理描述为一组排序问题，在这些问题中，人类专家可以大大减少工作量来做出决策; 或者将数据集清理描述为一组评分问题，在这些问题中，决策可以根据分数分布完全自动化。我们比较了 SelfClean 和其他基于合成噪声增强的常用计算机视觉基准的算法，并展示了在检测不相关图像、近重复图像和标签错误方面的最新性能。此外，将该方法应用于多个图像数据集，证实了该方法在评价信度方面的改进。"
    },
    {
        "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
        "url": "http://arxiv.org/abs/2305.18295v1",
        "pub_date": "2023-05-29",
        "summary": "Text-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of\nmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling\nbillions of diffusion paths (routes) from the network input to the output. Each\npath intuitively functions as a \"painter\" for depicting a particular textual\nconcept onto a specified image region at a diffusion timestep. Comprehensive\nexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both\nimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics,\nrealism, cyberpunk, and ink illustration. Secondly, a single model with three\nbillion parameters, trained on 1,000 A100 GPUs for two months, achieves a\nstate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,\nRAPHAEL significantly surpasses its counterparts in human evaluation on the\nViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the\nfrontiers of image generation research in both academia and industry, paving\nthe way for future breakthroughs in this rapidly evolving field. More details\ncan be found on a project webpage: https://raphael-painter.github.io/.",
        "translated": "文本到图像的生成近年来取得了显著的成就。我们引入了一个文本条件的图像扩散模型，称为 RAPHAEL，生成高度艺术化的图像，准确地描述文本提示，包括多个名词，形容词和动词。这是通过叠加数十个混合专家(MoEs)层来实现的，即空间-MoE 层和时间-MoE 层，使得从网络输入到输出的数十亿条扩散路径(路径)成为可能。每条路径直观地起到“画家”的作用，在扩散时间步骤中将特定的文本概念描绘到指定的图像区域。全面的实验表明，RAPHAEL 在图像质量和美学吸引力方面都优于最近的尖端模型，如稳定扩散，ERNIE-ViLG 2.0，DeepFloyd 和 DALL-E2。首先，RAPHAEL 在转换不同风格的图像方面表现出了卓越的表现，比如日本漫画、现实主义、赛博朋克和水墨插图。其次，一个拥有30亿个参数的单一模型，在1000个 A100图形处理器上训练了两个月，在 COCO 数据集上实现了最先进的零拍 FID 得分6.61。此外，RAPHAEL 在 ViLG-300基准的人体评估方面显著超越了同行。我们相信，RAPHAEL 具有推动学术界和工业界图像生成研究前沿的潜力，为这一快速发展领域的未来突破铺平了道路。更多详情可浏览计划网页:  https://raphael-painter.github.io/。"
    },
    {
        "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept\n  Customization of Diffusion Models",
        "url": "http://arxiv.org/abs/2305.18292v1",
        "pub_date": "2023-05-29",
        "summary": "Public large-scale text-to-image diffusion models, such as Stable Diffusion,\nhave gained significant attention from the community. These models can be\neasily customized for new concepts using low-rank adaptations (LoRAs). However,\nthe utilization of multiple concept LoRAs to jointly support multiple\ncustomized concepts presents a challenge. We refer to this scenario as\ndecentralized multi-concept customization, which involves single-client concept\ntuning and center-node concept fusion. In this paper, we propose a new\nframework called Mix-of-Show that addresses the challenges of decentralized\nmulti-concept customization, including concept conflicts resulting from\nexisting single-client LoRA tuning and identity loss during model fusion.\nMix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client\ntuning and gradient fusion for the center node to preserve the in-domain\nessence of single concepts and support theoretically limitless concept fusion.\nAdditionally, we introduce regionally controllable sampling, which extends\nspatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address\nattribute binding and missing object problems in multi-concept sampling.\nExtensive experiments demonstrate that Mix-of-Show is capable of composing\nmultiple customized concepts with high fidelity, including characters, objects,\nand scenes.",
        "translated": "公共的大规模文本到图像的扩散模型，如稳定扩散，已经引起了社会的重视。这些模型可以很容易地使用低等级适应性(LoRA)为新概念进行定制。然而，利用多个概念 LoRA 共同支持多个定制概念提出了一个挑战。我们将这种情况称为分散式多概念定制，其中包括单客户端概念调优和中心节点概念融合。在本文中，我们提出了一个名为 Mix-of-Show 的新框架，它解决了分散式多概念定制的挑战，包括现有的单客户 LoRA 调优导致的概念冲突和模型融合过程中的身份丢失。Mix-of-Show 采用嵌入分解 LoRA (ED-LoRA)进行单客户端调优和中心节点的梯度融合，以保留单个概念的域内实质，并支持理论上的无限概念融合。此外，本文还引入了区域可控抽样，扩展了空间可控抽样(如 ControlNet 和 T2I-Adaptor) ，解决了多概念抽样中的属性绑定和缺失对象问题。大量的实验表明，Mix-of-Show 能够以高保真度组合多个定制概念，包括人物、对象和场景。"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and\n  Unlabeled Image Collections",
        "url": "http://arxiv.org/abs/2305.18287v1",
        "pub_date": "2023-05-29",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "translated": "最近，大规模的预先训练的视觉和语言(VL)模型在零拍视觉分类中设置了一个新的最先进的(SOTA) ，使得开放词汇表能够识别定义为简单语言提示的潜在无限的类别集。然而，尽管有这些巨大的进步，这些零拍分类器的性能仍然不能满足专用(封闭类集)分类器的结果与监督微调训练。在本文中，我们首次展示了如何在不使用任何标签和任何配对 VL 数据的情况下，通过使用一个未标签的图像集和一组使用大语言模型(LLM)自动生成的文本来缩小这种差距，并有效地替换这些类别的标签视觉实例。使用我们的无标签方法，我们能够在广泛的数据集上获得比基础 VL 模型和其他当代方法和基线的零射击性能更显著的性能改进，显示在无标签设置下绝对改善高达11.7% (平均3.8%)。此外，尽管我们的方法是没有标签的，我们观察到1.3% 的平均收益超过领先的几杆提示基线，确实使用5杆监督。"
    },
    {
        "title": "Photoswap: Personalized Subject Swapping in Images",
        "url": "http://arxiv.org/abs/2305.18286v1",
        "pub_date": "2023-05-29",
        "summary": "In an era where images and visual content dominate our digital landscape, the\nability to manipulate and personalize these images has become a necessity.\nEnvision seamlessly substituting a tabby cat lounging on a sunlit window sill\nin a photograph with your own playful puppy, all while preserving the original\ncharm and composition of the image. We present Photoswap, a novel approach that\nenables this immersive image editing experience through personalized subject\nswapping in existing images. Photoswap first learns the visual concept of the\nsubject from reference images and then swaps it into the target image using\npre-trained diffusion models in a training-free manner. We establish that a\nwell-conceptualized visual subject can be seamlessly transferred to any image\nwith appropriate self-attention and cross-attention manipulation, maintaining\nthe pose of the swapped subject and the overall coherence of the image.\nComprehensive experiments underscore the efficacy and controllability of\nPhotoswap in personalized subject swapping. Furthermore, Photoswap\nsignificantly outperforms baseline methods in human ratings across subject\nswapping, background preservation, and overall quality, revealing its vast\napplication potential, from entertainment to professional editing.",
        "translated": "在一个图像和视觉内容主导我们的数字景观的时代，操纵和个性化这些图像的能力已成为一种必要。想象一下，在一张照片中，一只虎斑猫懒洋洋地躺在阳光照射下的窗台上，与你自己顽皮的小狗一起，同时保留了照片原有的魅力和构图。我们提出的 Photoswap，一种新颖的方法，使这种沉浸式图像编辑的经验，通过个性化的主题交换在现有的图像。照片交换首先从参考图像中学习目标的视觉概念，然后使用预先训练的扩散模型以一种无需训练的方式将其交换到目标图像中。我们建立了一个概念化的视觉主体可以通过适当的自我注意和交叉注意操作无缝地转移到任何图像上，保持交换主体的姿态和图像的整体一致性。全面的实验强调了 Photoswap 在个性化学科交换方面的有效性和可控性。此外，Photoswap 在主题交换、背景保存和整体质量方面显著优于基线评分方法，显示出其从娱乐到专业编辑的巨大应用潜力。"
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2305.18279v1",
        "pub_date": "2023-05-29",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "translated": "最近的多模态大语言模型(mLLMs)在视觉语言任务(如图像字幕和问答)中表现突出，但缺乏基本的感知能力，即目标检测。在这项工作中，我们通过引入一个关于语境目标检测的新的研究问题——在不同的人工智能交互环境中理解可见物体来解决这一局限性。研究了三种典型的情景，包括语言完形填空测试、视觉字幕和问答。此外，本文还提出了一个统一的多模态模型 ContextDET，该模型能够对视觉语言环境进行端到端的可微分建模，从而定位、识别和关联视觉对象和语言输入，实现人机交互。我们的 ContextDET 涉及三个关键子模型: (i)用于提取视觉表示的可视化编码器，(ii)用于多模态上下文解码的预先训练的 LLM，以及(iii)用于预测给定上下文对象词的边界框的可视化解码器。新的生成然后检测框架使我们能够检测人类词汇表中的对象词。大量的实验显示了 ContextDET 在我们提出的 CODE 基准、开放词汇表检测和引用图像分割上的优势。Https://Github.com/yuhangzang/contextdet."
    },
    {
        "title": "3DTeethSeg'22: 3D Teeth Scan Segmentation and Labeling Challenge",
        "url": "http://arxiv.org/abs/2305.18277v1",
        "pub_date": "2023-05-29",
        "summary": "Teeth localization, segmentation, and labeling from intra-oral 3D scans are\nessential tasks in modern dentistry to enhance dental diagnostics, treatment\nplanning, and population-based studies on oral health. However, developing\nautomated algorithms for teeth analysis presents significant challenges due to\nvariations in dental anatomy, imaging protocols, and limited availability of\npublicly accessible data. To address these challenges, the 3DTeethSeg'22\nchallenge was organized in conjunction with the International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI) in 2022,\nwith a call for algorithms tackling teeth localization, segmentation, and\nlabeling from intraoral 3D scans. A dataset comprising a total of 1800 scans\nfrom 900 patients was prepared, and each tooth was individually annotated by a\nhuman-machine hybrid algorithm. A total of 6 algorithms were evaluated on this\ndataset. In this study, we present the evaluation results of the 3DTeethSeg'22\nchallenge. The 3DTeethSeg'22 challenge code can be accessed at:\nhttps://github.com/abenhamadou/3DTeethSeg22_challenge",
        "translated": "口腔内3D 扫描的牙齿定位、分割和标记是现代牙科学加强口腔诊断、治疗计划和基于人群的口腔健康研究的基本任务。然而，开发用于牙齿分析的自动化算法由于牙齿解剖学的变化，成像协议和有限的可公开获取的数据提出了重大挑战。为了应对这些挑战，3DTeethSeg’22挑战与2022年国际医学图像计算和计算机辅助干预会议(MICCAI)一起组织，呼吁从口内3D 扫描中解决牙齿定位，分割和标记的算法。准备了一个包含来自900名患者的总共1800次扫描的数据集，并用人机混合算法对每颗牙齿进行单独注释。在这个数据集上总共评估了6种算法。在这项研究中，我们提出了3DTeethSeg’22挑战的评估结果。3dtethseg’22挑战码可在以下 https://github.com/abenhamadou/3dteethseg22_challenge 查阅:"
    },
    {
        "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning\n  and Diffusion Priors",
        "url": "http://arxiv.org/abs/2305.18274v1",
        "pub_date": "2023-05-29",
        "summary": "We present MindEye, a novel fMRI-to-image approach to retrieve and\nreconstruct viewed images from brain activity. Our model comprises two parallel\nsubmodules that are specialized for retrieval (using contrastive learning) and\nreconstruction (using a diffusion prior). MindEye can map fMRI brain activity\nto any high dimensional multimodal latent space, like CLIP image space,\nenabling image reconstruction using generative models that accept embeddings\nfrom this latent space. We comprehensively compare our approach with other\nexisting methods, using both qualitative side-by-side comparisons and\nquantitative evaluations, and show that MindEye achieves state-of-the-art\nperformance in both reconstruction and retrieval tasks. In particular, MindEye\ncan retrieve the exact original image even among highly similar candidates\nindicating that its brain embeddings retain fine-grained image-specific\ninformation. This allows us to accurately retrieve images even from large-scale\ndatabases like LAION-5B. We demonstrate through ablations that MindEye's\nperformance improvements over previous methods result from specialized\nsubmodules for retrieval and reconstruction, improved training techniques, and\ntraining models with orders of magnitude more parameters. Furthermore, we show\nthat MindEye can better preserve low-level image features in the\nreconstructions by using img2img, with outputs from a separate autoencoder. All\ncode is available on GitHub.",
        "translated": "我们提出了 MindEye，一种新的功能磁共振成像的方法来检索和重建从大脑活动中看到的图像。我们的模型包括两个并行的子模块，专门用于检索(使用对比学习)和重构(使用扩散先验)。MindEye 可以将 fMRI 大脑活动映射到任何高维多模式潜伏空间，比如 CLIP 图像空间，使得能够使用接受来自这个潜伏空间的嵌入的生成模型进行图像重建。我们全面比较了我们的方法与其他现有的方法，使用定性并列比较和定量评估，并表明 MindEye 实现了最先进的表现，在重建和检索任务。尤其值得一提的是，MindEye 甚至可以在高度相似的候选图像中检索到精确的原始图像，这表明它的大脑嵌入保留了细粒度的图像特定信息。这使我们能够准确地检索图像，甚至从大型数据库，如 LAION-5B。我们通过消融实验证明，MindEye 的性能优于以前的方法是由专门用于检索和重建的子模块、改进的训练技术以及具有更多参数的训练模型数量级的结果。此外，我们表明，MindEye 可以更好地保存低水平的图像特征，在重建使用 img2img，从一个单独的自动编码器的输出。所有代码都可以在 GitHub 上找到。"
    },
    {
        "title": "Pix2Repair: Implicit Shape Restoration from Images",
        "url": "http://arxiv.org/abs/2305.18273v1",
        "pub_date": "2023-05-29",
        "summary": "We present Pix2Repair, an automated shape repair approach that generates\nrestoration shapes from images to repair fractured objects. Prior repair\napproaches require a high-resolution watertight 3D mesh of the fractured object\nas input. Input 3D meshes must be obtained using expensive 3D scanners, and\nscanned meshes require manual cleanup, limiting accessibility and scalability.\nPix2Repair takes an image of the fractured object as input and automatically\ngenerates a 3D printable restoration shape. We contribute a novel shape\nfunction that deconstructs a latent code representing the fractured object into\na complete shape and a break surface. We show restorations for synthetic\nfractures from the Geometric Breaks and Breaking Bad datasets, and cultural\nheritage objects from the QP dataset, and for real fractures from the Fantastic\nBreaks dataset. We overcome challenges in restoring axially symmetric objects\nby predicting view-centered restorations. Our approach outperforms shape\ncompletion approaches adapted for shape repair in terms of chamfer distance,\nearth mover's distance, normal consistency, and percent restorations generated.",
        "translated": "我们介绍了 Pix2修复，一种自动形状修复方法，从图像生成修复形状，以修复断裂的对象。先前的修复方法需要将断裂物体的高分辨率水密3D 网格作为输入。输入3D 网格必须使用昂贵的3D 扫描仪获得，并扫描网格需要手动清理，限制可访问性和可伸缩性。Pix2Amendment 将断裂物体的图像作为输入，并自动生成一个3D 可打印的修复形状。我们贡献了一个新的形状函数，解构一个潜在的代码代表破碎的物体成为一个完整的形状和破裂表面。我们展示了来自几何断裂和绝命毒师数据集的合成骨折的修复，来自 QP 数据集的文化遗产对象，以及来自荒诞断裂数据集的真实骨折的修复。我们通过预测以视点为中心的复原来克服轴对称物体复原的挑战。我们的方法在倒角距离、推土机的距离、法向一致性和恢复百分比方面优于适用于形状修复的形状完成方法。"
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal\n  Co-Denoising",
        "url": "http://arxiv.org/abs/2305.18264v1",
        "pub_date": "2023-05-29",
        "summary": "Leveraging large-scale image-text datasets and advancements in diffusion\nmodels, text-driven generative models have made remarkable strides in the field\nof image generation and editing. This study explores the potential of extending\nthe text-driven ability to the generation and editing of multi-text conditioned\nlong videos. Current methodologies for video generation and editing, while\ninnovative, are often confined to extremely short videos (typically less than\n24 frames) and are limited to a single text condition. These constraints\nsignificantly limit their applications given that real-world videos usually\nconsist of multiple segments, each bearing different semantic information. To\naddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,\ncapable of extending off-the-shelf short video diffusion models for generating\nand editing videos comprising hundreds of frames with diverse semantic segments\nwithout introducing additional training, all while preserving content\nconsistency. We have implemented three mainstream text-driven video generation\nand editing methodologies and extended them to accommodate longer videos imbued\nwith a variety of semantic segments with our proposed paradigm. Our\nexperimental outcomes reveal that our approach significantly broadens the\ngenerative and editing capabilities of video diffusion models, offering new\npossibilities for future research and applications. The code is available at\nhttps://github.com/G-U-N/Gen-L-Video.",
        "translated": "利用大规模图文数据集和扩散模型的进步，文本驱动的生成模型在图像生成和编辑领域取得了显著的进步。本研究探讨文本驱动能力扩展到多文本条件长视频的生成和编辑的潜力。目前的视频生成和编辑方法，虽然具有创新性，但往往局限于极短的视频(通常少于24帧) ，并且仅限于一个文本条件。考虑到现实世界中的视频通常由多个片段组成，每个片段都有不同的语义信息，这些限制极大地限制了它们的应用。为了应对这一挑战，我们引入了一种称为 Gen-L-Video 的新型范例，该范例能够扩展现成的短视频扩散模型，用于生成和编辑包含数百帧具有不同语义片段的视频，而无需引入额外的训练，同时保持内容的一致性。我们已经实现了三种主流的文本驱动的视频生成和编辑方法，并扩展了它们，以适应与我们提出的范例中的各种语义片段灌输的较长的视频。我们的实验结果表明，我们的方法显着扩大了视频扩散模型的生成和编辑能力，为未来的研究和应用提供了新的可能性。密码可在 https://github.com/g-u-n/gen-l-video 查阅。"
    },
    {
        "title": "Synfeal: A Data-Driven Simulator for End-to-End Camera Localization",
        "url": "http://arxiv.org/abs/2305.18260v1",
        "pub_date": "2023-05-29",
        "summary": "Collecting real-world data is often considered the bottleneck of Artificial\nIntelligence, stalling the research progress in several fields, one of which is\ncamera localization. End-to-end camera localization methods are still\noutperformed by traditional methods, and we argue that the inconsistencies\nassociated with the data collection techniques are restraining the potential of\nend-to-end methods. Inspired by the recent data-centric paradigm, we propose a\nframework that synthesizes large localization datasets based on realistic 3D\nreconstructions of the real world. Our framework, termed Synfeal: Synthetic\nfrom Real, is an open-source, data-driven simulator that synthesizes RGB images\nby moving a virtual camera through a realistic 3D textured mesh, while\ncollecting the corresponding ground-truth camera poses. The results validate\nthat the training of camera localization algorithms on datasets generated by\nSynfeal leads to better results when compared to datasets generated by\nstate-of-the-art methods. Using Synfeal, we conducted the first analysis of the\nrelationship between the size of the dataset and the performance of camera\nlocalization algorithms. Results show that the performance significantly\nincreases with the dataset size. Our results also suggest that when a large\nlocalization dataset with high quality is available, training from scratch\nleads to better performances. Synfeal is publicly available at\nhttps://github.com/DanielCoelho112/synfeal.",
        "translated": "真实世界数据的采集往往被认为是人工智能的瓶颈，阻碍了人工智能在多个领域的研究进展，其中摄像机定位就是其中之一。端到端摄像机定位方法仍然比传统的定位方法要好，我们认为与数据采集技术相关的不一致性限制了端到端定位方法的潜力。受最近以数据为中心的范式的启发，我们提出了一个基于真实世界的三维重建的大型定位数据集合成框架。我们的框架，称为 Synfeal: 从真实合成，是一个开源的，数据驱动的模拟器，通过移动虚拟相机通过一个真实的3D 纹理网格合成 RGB 图像，同时收集相应的地面真相相机的姿势。实验结果表明，在 Synfeal 生成的数据集上进行摄像机定位算法的训练，比采用最先进的方法生成的数据集有更好的定位效果。使用 Synfeal，我们首先分析了数据集大小与摄像机定位算法性能之间的关系。结果表明，随着数据集大小的增加，性能显著提高。我们的结果还表明，当一个高质量的大型定位数据集是可用的，从头训练导致更好的性能。Synfeal 可以在 https://github.com/danielcoelho112/Synfeal 上公开使用。"
    },
    {
        "title": "Learning without Forgetting for Vision-Language Models",
        "url": "http://arxiv.org/abs/2305.19270v1",
        "pub_date": "2023-05-30",
        "summary": "Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance.",
        "translated": "课堂增量学习(CIL)或连续学习是现实世界需要的一种能力，它需要一个学习系统来适应新的任务而不忘记以前的任务。虽然传统的 CIL 方法侧重于视觉信息来掌握核心特征，但视觉语言模型(VLM)的最新进展已经显示出在借助文本信息学习可推广表示方面的有前途的能力。然而，当不断培训新的类，VLM 往往遭受灾难性的遗忘以前的知识。在 CIL 中应用 VLM 模型提出了两个主要的挑战: 1)如何在不遗忘的情况下调整模型; 2)如何充分利用多模态信息。为此，我们提出了投影融合(PROOF) ，使 VLM 学习而不会忘记。为了应对第一个挑战，我们提出了基于冻结图像/文本编码器的训练任务特定投影。当面对新的任务时，新的预测会被扩展，旧的预测会被固定，从而减轻对旧概念的遗忘。对于第二个挑战，我们提出了融合模块，以更好地利用交叉模态信息。通过对视觉特征和文本特征的联合调整，该模型可以捕捉具有更强表现能力的语义信息。在九个基准数据集上的大量实验验证了 PROOF 实现了最先进的性能。"
    },
    {
        "title": "Ambient Diffusion: Learning Clean Distributions from Corrupted Data",
        "url": "http://arxiv.org/abs/2305.19256v1",
        "pub_date": "2023-05-30",
        "summary": "We present the first diffusion-based framework that can learn an unknown\ndistribution using only highly-corrupted samples. This problem arises in\nscientific applications where access to uncorrupted samples is impossible or\nexpensive to acquire. Another benefit of our approach is the ability to train\ngenerative models that are less likely to memorize individual training samples\nsince they never observe clean training data. Our main idea is to introduce\nadditional measurement distortion during the diffusion process and require the\nmodel to predict the original corrupted image from the further corrupted image.\nWe prove that our method leads to models that learn the conditional expectation\nof the full uncorrupted image given this additional measurement corruption.\nThis holds for any corruption process that satisfies some technical conditions\n(and in particular includes inpainting and compressed sensing). We train models\non standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn\nthe distribution even when all the training samples have $90\\%$ of their pixels\nmissing. We also show that we can finetune foundation models on small corrupted\ndatasets (e.g. MRI scans with block corruptions) and learn the clean\ndistribution without memorizing the training set.",
        "translated": "我们提出了第一个扩散为基础的框架，可以学习一个未知的分布，只使用高度腐败的样本。这个问题出现在科学应用领域，因为无法获得未受污染的样品或获得这些样品的费用很高。我们的方法的另一个好处是能够训练生成模型，这些模型不太可能记住单独的训练样本，因为它们从来没有观察到干净的训练数据。我们的主要思想是在扩散过程中引入额外的测量失真，并要求该模型从进一步的损伤图像中预测出原始的损伤图像。我们证明了我们的方法导致模型学习完整未损坏的图像的条件期望，考虑到这种额外的测量损坏。这适用于任何满足某些技术条件的腐败过程(特别是包括油漆和压缩感知)。我们在标准基准(CelebA，CIFAR-10和 AFHQ)上训练模型，并表明即使所有的训练样本都丢失了90% 的像素，我们仍然可以学习分布。我们还展示了我们可以在小的损坏数据集上微调基础模型(例如带有块损坏的 MRI 扫描) ，并且不需要记忆训练集就可以学习干净的分布。"
    },
    {
        "title": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation",
        "url": "http://arxiv.org/abs/2305.19245v1",
        "pub_date": "2023-05-30",
        "summary": "This paper presents a method that can quickly adapt dynamic 3D avatars to\narbitrary text descriptions of novel styles. Among existing approaches for\navatar stylization, direct optimization methods can produce excellent results\nfor arbitrary styles but they are unpleasantly slow. Furthermore, they require\nredoing the optimization process from scratch for every new input. Fast\napproximation methods using feed-forward networks trained on a large dataset of\nstyle images can generate results for new inputs quickly, but tend not to\ngeneralize well to novel styles and fall short in quality. We therefore\ninvestigate a new approach, AlteredAvatar, that combines those two approaches\nusing the meta-learning framework. In the inner loop, the model learns to\noptimize to match a single target style well; while in the outer loop, the\nmodel learns to stylize efficiently across many styles. After training,\nAlteredAvatar learns an initialization that can quickly adapt within a small\nnumber of update steps to a novel style, which can be given using texts, a\nreference image, or a combination of both. We show that AlteredAvatar can\nachieve a good balance between speed, flexibility and quality, while\nmaintaining consistency across a wide range of novel views and facial\nexpressions.",
        "translated": "本文提出了一种快速自适应动态3D 化身的方法，以适应任意文本描述的新风格。在现有的化身风格化方法中，直接优化方法可以对任意风格产生优秀的结果，但是它们的速度慢得令人不快。此外，它们需要从头开始为每个新输入重做优化过程。使用前馈网络训练样式图像的快速逼近方法可以快速生成新输入的结果，但往往不能很好地推广到新的样式和质量不足。因此，我们研究了一种新的方法，AlteredAvatar，它使用元学习框架将这两种方法结合起来。在内部循环中，模型学习如何优化以很好地匹配单个目标样式; 而在外部循环中，模型学习如何有效地跨多种样式进行样式化。经过训练，AlteredAvatar 学会了一种初始化，它可以在少量更新步骤内快速适应一种新的风格，这种风格可以使用文本、参考图像或两者的组合。我们展示了 AlteredAvatar 可以在速度、灵活性和质量之间取得很好的平衡，同时保持广泛的新视图和面部表情的一致性。"
    },
    {
        "title": "Translation-Enhanced Multilingual Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2305.19216v1",
        "pub_date": "2023-05-30",
        "summary": "Research on text-to-image generation (TTI) still predominantly focuses on the\nEnglish language due to the lack of annotated image-caption data in other\nlanguages; in the long run, this might widen inequitable access to TTI\ntechnology. In this work, we thus investigate multilingual TTI (termed mTTI)\nand the current potential of neural machine translation (NMT) to bootstrap mTTI\nsystems. We provide two key contributions. 1) Relying on a multilingual\nmulti-modal encoder, we provide a systematic empirical study of standard\nmethods used in cross-lingual NLP when applied to mTTI: Translate Train,\nTranslate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),\na novel parameter-efficient approach that learns to weigh and consolidate the\nmultilingual text knowledge within the mTTI framework, mitigating the language\ngap and thus improving mTTI performance. Our evaluations on standard mTTI\ndatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of\ntranslation-enhanced mTTI systems and also validate the benefits of the\nproposed EnsAd which derives consistent gains across all datasets. Further\ninvestigations on model variants, ablation studies, and qualitative analyses\nprovide additional insights on the inner workings of the proposed mTTI\napproaches.",
        "translated": "由于缺乏其他语言的注释图像标题数据，文本到图像生成(TTI)的研究仍然主要集中在英语上; 从长远来看，这可能会扩大 TTI 技术的不公平使用。在这项工作中，我们因此研究多语言 TTI (称为 mTTI)和神经机器翻译(NMT)目前的潜力引导 mTTI 系统。我们提供了两个关键的贡献。1)以多语言多模态编码器为基础，对跨语言自然语言处理中的标准方法进行了系统的实证研究。2)提出了一种新的参数有效方法 EnsAd，该方法可以在 mTTI 框架内学习权衡和整合多语言文本知识，减小语言差距，从而提高 mTTI 的性能。我们对标准 mTTI 数据集 COCO-CN，Multi30K Task2和 LAION-5B 的评估证明了翻译增强的 mTTI 系统的潜力，并且还验证了所提议的 EnsAd 的益处，其在所有数据集中获得一致的增益。对模型变异、消融研究和定性分析的进一步研究提供了对拟议的 mTTI 方法内部工作的额外见解。"
    },
    {
        "title": "Group Invariant Global Pooling",
        "url": "http://arxiv.org/abs/2305.19207v1",
        "pub_date": "2023-05-30",
        "summary": "Much work has been devoted to devising architectures that build\ngroup-equivariant representations, while invariance is often induced using\nsimple global pooling mechanisms. Little work has been done on creating\nexpressive layers that are invariant to given symmetries, despite the success\nof permutation invariant pooling in various molecular tasks. In this work, we\npresent Group Invariant Global Pooling (GIGP), an invariant pooling layer that\nis provably sufficiently expressive to represent a large class of invariant\nfunctions. We validate GIGP on rotated MNIST and QM9, showing improvements for\nthe latter while attaining identical results for the former. By making the\npooling process group orbit-aware, this invariant aggregation method leads to\nimproved performance, while performing well-principled group aggregation.",
        "translated": "许多工作致力于设计构建群等变表示的体系结构，而不变性通常是使用简单的全局池机制来诱导的。尽管在各种分子任务中排列不变量池的成功，但在创建对给定对称性不变的表达层方面几乎没有做什么工作。在这项工作中，我们提出了群不变全局池(GIGP) ，一个不变的池层，可证明充分表达，以表示一个大类的不变函数。我们在旋转的 MNIST 和 QM9上验证了 GIGP，显示了后者的改进，同时获得了前者相同的结果。这种不变聚集方法通过使池处理过程组轨道感知，提高了性能，同时执行了原则性良好的组聚集。"
    },
    {
        "title": "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer",
        "url": "http://arxiv.org/abs/2305.19205v1",
        "pub_date": "2023-05-30",
        "summary": "Learning based feature matching methods have been commonly studied in recent\nyears. The core issue for learning feature matching is to how to learn (1)\ndiscriminative representations for feature points (or regions) within each\nintra-image and (2) consensus representations for feature points across\ninter-images. Recently, self- and cross-attention models have been exploited to\naddress this issue. However, in many scenes, features are coming with\nlarge-scale, redundant and outliers contaminated. Previous\nself-/cross-attention models generally conduct message passing on all primal\nfeatures which thus lead to redundant learning and high computational cost. To\nmitigate limitations, inspired by recent seed matching methods, in this paper,\nwe propose a novel efficient Anchor Matching Transformer (AMatFormer) for the\nfeature matching problem. AMatFormer has two main aspects: First, it mainly\nconducts self-/cross-attention on some anchor features and leverages these\nanchor features as message bottleneck to learn the representations for all\nprimal features. Thus, it can be implemented efficiently and compactly. Second,\nAMatFormer adopts a shared FFN module to further embed the features of two\nimages into the common domain and thus learn the consensus feature\nrepresentations for the matching problem. Experiments on several benchmarks\ndemonstrate the effectiveness and efficiency of the proposed AMatFormer\nmatching approach.",
        "translated": "基于学习的特征匹配方法是近年来研究的热点。学习特征匹配的核心问题是如何学习(1)图像内特征点(或区域)的区分表示和(2)图像间特征点的一致表示。最近，自我和交叉注意模型已经被用来解决这个问题。然而，在许多场景中，特性都伴随着大规模的、冗余的和受到污染的异常值。以往的自我/交叉注意模型通常将信息传递给所有的原始特征，从而导致冗余学习和高计算成本。为了解决这一问题，本文提出了一种新的基于特征匹配的锚匹配变换器。AMatForm 主要有两个方面: 首先，它主要对一些锚特征进行自我/交叉注意，并利用这些锚特征作为消息瓶颈来学习所有原始特征的表示。因此，它可以有效和紧凑地实现。其次，采用共享 FFN 模块将两幅图像的特征进一步嵌入到公共域中，从而学习匹配问题的一致特征表示。在多个基准上的实验结果表明了该方法的有效性和高效性。"
    },
    {
        "title": "DäRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth\n  Adaptation",
        "url": "http://arxiv.org/abs/2305.19201v1",
        "pub_date": "2023-05-30",
        "summary": "Neural radiance fields (NeRF) shows powerful performance in novel view\nsynthesis and 3D geometry reconstruction, but it suffers from critical\nperformance degradation when the number of known viewpoints is drastically\nreduced. Existing works attempt to overcome this problem by employing external\npriors, but their success is limited to certain types of scenes or datasets.\nEmploying monocular depth estimation (MDE) networks, pretrained on large-scale\nRGB-D datasets, with powerful generalization capability would be a key to\nsolving this problem: however, using MDE in conjunction with NeRF comes with a\nnew set of challenges due to various ambiguity problems exhibited by monocular\ndepths. In this light, we propose a novel framework, dubbed D\\\"aRF, that\nachieves robust NeRF reconstruction with a handful of real-world images by\ncombining the strengths of NeRF and monocular depth estimation through online\ncomplementary training. Our framework imposes the MDE network's powerful\ngeometry prior to NeRF representation at both seen and unseen viewpoints to\nenhance its robustness and coherence. In addition, we overcome the ambiguity\nproblems of monocular depths through patch-wise scale-shift fitting and\ngeometry distillation, which adapts the MDE network to produce depths aligned\naccurately with NeRF geometry. Experiments show our framework achieves\nstate-of-the-art results both quantitatively and qualitatively, demonstrating\nconsistent and reliable performance in both indoor and outdoor real-world\ndatasets. Project page is available at https://ku-cvlab.github.io/DaRF/.",
        "translated": "神经辐射场(NeRF)在新视点合成和三维几何重建方面表现出强大的性能，但当已知视点数大幅减少时，其性能会出现临界退化。现有的作品试图通过使用外部先验来克服这个问题，但是他们的成功仅限于某些类型的场景或数据集。使用单目深度估计(MDE)网络，在大规模 RGB-D 数据集上预先训练，具有强大的泛化能力将是解决这一问题的关键: 然而，使用 MDE 与 NeRF 结合带来了一系列新的挑战，由于单目深度表现出的各种模糊问题。在此基础上，我们提出了一种新的框架，称为 D“ aRF，通过在线互补训练结合了 NERF 和单目深度估计的优点，实现了对少量真实世界图像的强大的 NERF 重建。我们的框架将 MDE 网络的强大的几何形状强加于 NERF 表示之前，在可见和不可见的观点，以增强其健壮性和一致性。此外，通过分片尺度变换拟合和几何精馏克服了单目深度的模糊性问题，使 MDE 网络能够产生与 NeRF 几何精确对齐的深度。实验表明，我们的框架在定量和定性上都达到了最先进的结果，在室内和室外的真实世界数据集中表现出一致和可靠的性能。项目网页可于 https://ku-cvlab.github.io/darf/下载。"
    },
    {
        "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for\n  Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.19195v1",
        "pub_date": "2023-05-30",
        "summary": "Vision-and-Language Navigation (VLN) requires the agent to follow language\ninstructions to navigate through 3D environments. One main challenge in VLN is\nthe limited availability of photorealistic training environments, which makes\nit hard to generalize to new and unseen environments. To address this problem,\nwe propose PanoGen, a generation method that can potentially create an infinite\nnumber of diverse panoramic environments conditioned on text. Specifically, we\ncollect room descriptions by captioning the room images in existing\nMatterport3D environments, and leverage a state-of-the-art text-to-image\ndiffusion model to generate the new panoramic environments. We use recursive\noutpainting over the generated images to create consistent 360-degree panorama\nviews. Our new panoramic environments share similar semantic information with\nthe original environments by conditioning on text descriptions, which ensures\nthe co-occurrence of objects in the panorama follows human intuition, and\ncreates enough diversity in room appearance and layout with image outpainting.\nLastly, we explore two ways of utilizing PanoGen in VLN pre-training and\nfine-tuning. We generate instructions for paths in our PanoGen environments\nwith a speaker built on a pre-trained vision-and-language model for VLN\npre-training, and augment the visual observation with our panoramic\nenvironments during agents' fine-tuning to avoid overfitting to seen\nenvironments. Empirically, learning with our PanoGen environments achieves the\nnew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.\nPre-training with our PanoGen speaker data is especially effective for CVDN,\nwhich has under-specified instructions and needs commonsense knowledge. Lastly,\nwe show that the agent can benefit from training with more generated panoramic\nenvironments, suggesting promising results for scaling up the PanoGen\nenvironments.",
        "translated": "视觉和语言导航(VLN)要求代理遵循语言指令在3D 环境中导航。VLN 的一个主要挑战是有限的真实感训练环境，这使得它很难推广到新的和看不见的环境。为了解决这个问题，我们提出 PanoGen，一种生成方法，可以潜在地创建无限数量的不同的全景环境的文本条件。具体来说，我们通过在现有 Matterport3D 环境中标注房间图像来收集房间描述，并利用最先进的文本到图像扩散模型来生成新的全景环境。我们使用递归绘制生成的图像来创建一致的360度全景视图。我们新的全景环境与原始环境有着相似的语义信息，通过文字描述来确保全景中物体的同时出现遵循人类的直觉，并创造了足够多样化的房间外观和布局图像。最后，我们探讨了利用 PanoGen 在 VLN 预训练和微调中的两种方法。我们在 PanoGen 环境中使用一个基于预先训练的 VLN 预训视觉和语言模型的扬声器生成路径指令，并在代理的微调期间通过我们的全景环境增强视觉观察，以避免过度适应可见环境。根据经验，使用 PanoGen 环境学习可以在 Room-to-Room、 Room-for-Room 和 CVDN 数据集上实现最新的技术水平。使用 PanoGen 扬声器数据进行预训练对于 CVDN 尤其有效，因为 CVDN 的指令不够详细，而且需要常识性知识。最后，我们表明，该代理可以受益于更多生成的全景环境的培训，提出了扩展 PanoGen 环境的有希望的结果。"
    },
    {
        "title": "Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video\n  Translation Using Conditional Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.19193v1",
        "pub_date": "2023-05-30",
        "summary": "In this study, we present an efficient and effective approach for achieving\ntemporally consistent synthetic-to-real video translation in videos of varying\nlengths. Our method leverages off-the-shelf conditional image diffusion models,\nallowing us to perform multiple synthetic-to-real image generations in\nparallel. By utilizing the available optical flow information from the\nsynthetic videos, our approach seamlessly enforces temporal consistency among\ncorresponding pixels across frames. This is achieved through joint noise\noptimization, effectively minimizing spatial and temporal discrepancies. To the\nbest of our knowledge, our proposed method is the first to accomplish diverse\nand temporally consistent synthetic-to-real video translation using conditional\nimage diffusion models. Furthermore, our approach does not require any training\nor fine-tuning of the diffusion models. Extensive experiments conducted on\nvarious benchmarks for synthetic-to-real video translation demonstrate the\neffectiveness of our approach, both quantitatively and qualitatively. Finally,\nwe show that our method outperforms other baseline methods in terms of both\ntemporal consistency and visual quality.",
        "translated": "在这项研究中，我们提出了一个有效的方法来实现时间一致的合成到真实的视频在不同长度的视频翻译。我们的方法利用现成的条件图像扩散模型，允许我们并行执行多个合成到真实的图像生成。该方法利用合成视频中可用的光流信息，实现了帧间对应像素之间的时间一致性。这是通过联合噪声优化，有效地减少空间和时间差异。据我们所知，我们提出的方法是第一个使用条件图像扩散模型来实现多样化和时间一致的合成到真实的视频翻译。此外，我们的方法不需要任何培训或扩散模型的微调。针对合成到真实视频翻译的各种基准进行了大量的实验，从定量和定性两方面证明了该方法的有效性。最后，我们证明了我们的方法在时间一致性和视觉质量方面都优于其他基线方法。"
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": "表检测(TD)是实现视觉丰富的文档理解的基本任务。目前的研究通常将 TD 问题表述为一个目标检测问题，然后利用基于交叉比联盟(IoU)的度量来评估模型的性能，并利用基于 IoU 的损失函数来优化模型。TD 应用程序通常要求预测结果覆盖表中的所有内容，避免信息丢失。然而，基于 IoU 和 IoU 的损失函数不能直接反映预测结果的信息损失程度。因此，我们提出将 IU 解耦为一个地面真实覆盖项和一个预测覆盖项，其中前者可用来度量预测结果的信息损失。此外，文档中的表格通常很大，分布很稀疏，没有重叠，因为它们旨在总结必要的信息，以便于人类读者阅读和解释。因此，在本研究中，我们以稀疏 R-CNN 为基础模型，并进一步改进模型，使用高斯噪声增强图像大小区域方案和多对一标签分配。为了验证所提方法的有效性，并与现有方法进行比较，我们进行了实验，并使用基于 IoU 的评价指标来评价模型的性能。实验结果表明，在不同的物联网度量下，该方法在各种数据集上的性能均优于目前最先进的方法。通过进一步的实验，我们证明了所提出的解耦 IU 对 TD 应用的优越性，将基于 IU 的损失函数和评估指标替换为所提出的解耦 IU 对应物。实验结果表明，我们提出的解耦的 IU 损失可以鼓励模型，以减轻信息损失。"
    },
    {
        "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
        "url": "http://arxiv.org/abs/2305.20091v1",
        "pub_date": "2023-05-31",
        "summary": "We present an approach to reconstruct humans and track them over time. At the\ncore of our approach, we propose a fully \"transformerized\" version of a network\nfor human mesh recovery. This network, HMR 2.0, advances the state of the art\nand shows the capability to analyze unusual poses that have in the past been\ndifficult to reconstruct from single images. To analyze video, we use 3D\nreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.\nThis enables us to deal with multiple people and maintain identities through\nocclusion events. Our complete approach, 4DHumans, achieves state-of-the-art\nresults for tracking people from monocular video. Furthermore, we demonstrate\nthe effectiveness of HMR 2.0 on the downstream task of action recognition,\nachieving significant improvements over previous pose-based action recognition\napproaches. Our code and models are available on the project website:\nhttps://shubham-goel.github.io/4dhumans/.",
        "translated": "我们提出了一种方法来重建人类，并随着时间的推移跟踪他们。在我们的方法的核心，我们提出了一个完全“转换”的网络版本的人类网格恢复。这个网络，HMR 2.0，提高了最先进的技术水平，并显示了分析不寻常姿势的能力，这些姿势在过去很难从单个图像重建。为了分析视频，我们使用来自 HMR 2.0的3D 重建作为输入到一个3D 跟踪系统中。这使我们能够处理多个人，并通过遮挡事件保持身份。我们的完整方法，4D 人类，实现了最先进的结果跟踪人从单目视频。此外，我们证明了 HMR 2.0在动作识别的下游任务上的有效性，实现了对以前基于姿势的动作识别方法的显著改进。我们的代码和模型可以在项目网站上找到:  https://shubham-goel.github.io/4dhumans/。"
    },
    {
        "title": "Learning Explicit Contact for Implicit Reconstruction of Hand-held\n  Objects from Monocular Images",
        "url": "http://arxiv.org/abs/2305.20089v1",
        "pub_date": "2023-05-31",
        "summary": "Reconstructing hand-held objects from monocular RGB images is an appealing\nyet challenging task. In this task, contacts between hands and objects provide\nimportant cues for recovering the 3D geometry of the hand-held objects. Though\nrecent works have employed implicit functions to achieve impressive progress,\nthey ignore formulating contacts in their frameworks, which results in\nproducing less realistic object meshes. In this work, we explore how to model\ncontacts in an explicit way to benefit the implicit reconstruction of hand-held\nobjects. Our method consists of two components: explicit contact prediction and\nimplicit shape reconstruction. In the first part, we propose a new subtask of\ndirectly estimating 3D hand-object contacts from a single image. The part-level\nand vertex-level graph-based transformers are cascaded and jointly learned in a\ncoarse-to-fine manner for more accurate contact probabilities. In the second\npart, we introduce a novel method to diffuse estimated contact states from the\nhand mesh surface to nearby 3D space and leverage diffused contact\nprobabilities to construct the implicit neural representation for the\nmanipulated object. Benefiting from estimating the interaction patterns between\nthe hand and the object, our method can reconstruct more realistic object\nmeshes, especially for object parts that are in contact with hands. Extensive\nexperiments on challenging benchmarks show that the proposed method outperforms\nthe current state of the arts by a great margin.",
        "translated": "从单目 RGB 图像重建手持物体是一个吸引人的但具有挑战性的任务。在这项任务中，手和物体之间的接触为恢复手持物体的三维几何形状提供了重要的线索。尽管最近的作品使用了隐式函数来取得令人印象深刻的进展，但是他们忽视了在框架中表述联系，这导致了产生不太真实的对象网格。在这项工作中，我们探讨了如何以显性的方式建模接触，以利于手持物体的隐式重建。该方法由显式接触预测和隐式形状重建两部分组成。在第一部分中，我们提出了一个新的子任务直接估计三维手-物体接触从一个单一的图像。基于部件级和顶点级图形的变压器以从粗到精的方式串联和联合学习，以获得更准确的接触概率。在第二部分中，我们提出了一种新的方法来扩散估计接触状态从手网格表面到附近的三维空间，并利用扩散接触概率来构造被操作物体的隐式神经表示。该方法通过估计手与物体之间的交互模式，可以重建出更加逼真的物体网格，特别是对于与手接触的物体部分。对具有挑战性的基准的大量实验表明，该方法的性能远远优于目前的技术水平。"
    },
    {
        "title": "Improving CLIP Training with Language Rewrites",
        "url": "http://arxiv.org/abs/2305.20088v1",
        "pub_date": "2023-05-31",
        "summary": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.",
        "translated": "对比语言-图像预训练(CLIP)是利用成对图像和文本数据训练可转移视觉模型的最有效和可扩展的方法之一。CLIP 模型使用对比损失进行训练，对比损失通常依赖于数据增强以防止过度拟合和快捷方式。然而，在 CLIP 训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了不同文本暴露于同一图像。本文介绍了语言增强 CLIP 技术，这是一种通过语言重写来增强 CLIP 训练的简单而高效的方法。利用大型语言模型的上下文学习能力，我们重写与每个图像相关的文本描述。这些改写后的文本在保留原有关键概念和意义的同时，表现出句子结构和词汇的多样性。在训练过程中，LaCLIP 随机选择原始文本或改写版本作为每幅图像的文本增强。在 CC3M、 CC12M、 RedCaps 和 LAION-400M 数据集上进行的大量实验表明，使用语言重写的 CLIP 预训练可以在不增加训练过程中的计算和内存开销的情况下显著提高传输性能。具体到 ImageNet 的零射精度，LaCLIP 在 CC12M 上比 CLIP 高出8.2% ，在 LAION-400M 上高出2.4% 。密码可于 https://github.com/lijiefan/laclip 索取。"
    },
    {
        "title": "Too Large; Data Reduction for Vision-Language Pre-Training",
        "url": "http://arxiv.org/abs/2305.20087v2",
        "pub_date": "2023-05-31",
        "summary": "This paper examines the problems of severe image-text misalignment and high\nredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)\ndatasets. To address these issues, we propose an efficient and straightforward\nVision-Language learning algorithm called TL;DR, which aims to compress the\nexisting large VLP data into a small, high-quality set. Our approach consists\nof two major steps. First, a codebook-based encoder-decoder captioner is\ndeveloped to select representative samples. Second, a new caption is generated\nto complement the original captions for selected samples, mitigating the\ntext-image misalignment problem while maintaining uniqueness. As the result,\nTL;DR enables us to reduce the large dataset into a small set of high-quality\ndata, which can serve as an alternative pre-training dataset. This algorithm\nsignificantly speeds up the time-consuming pretraining process. Specifically,\nTL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce\nwell-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M\nfrom 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP\nmodels over seven downstream tasks show that VLP model trained on the\ncompressed dataset provided by TL;DR can perform similar or even better results\ncompared with training on the full-scale dataset. The code will be made\navailable at \\url{https://github.com/showlab/data-centric.vlp}.",
        "translated": "本文研究了目前广泛使用的大规模视觉语言预训练(VLP)数据集中存在的严重图文错位和高冗余问题。为了解决这些问题，我们提出了一种高效、直观的视觉语言学习算法 TL; DR，该算法旨在将现有的大型 VLP 数据压缩成一个小型、高质量的集合。我们的方法包括两个主要步骤。首先，开发了一种基于码本的编解码字幕器来选择有代表性的样本。其次，生成一个新的标题以补充所选样本的原始标题，在保持唯一性的同时缓解文本-图像不对齐问题。结果，TL; DR 使我们能够将大数据集减少为一个小的高质量数据集，这可以作为一个替代的预训练数据集。该算法显著加快了耗时的预训练过程。具体来说，TL; DR 可以高比例地压缩主流 VLP 数据集，例如，将清洁良好的 CC3M 数据集从2.82 M 减少到0.67 M ($sim $24%) ，将噪音较大的 YFCC15M 从15M 减少到250 M ($sim $16.7%)。通过对三种流行的 VLP 模型在7个下游任务上的大量实验表明，VLP 模型在 TL 提供的压缩数据集上进行训练，与在全尺寸数据集上进行训练相比，DR 可以获得相似甚至更好的结果。代码将在 url { https://github.com/showlab/data-centric.vlp }提供。"
    },
    {
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "url": "http://arxiv.org/abs/2305.20086v1",
        "pub_date": "2023-05-31",
        "summary": "Images generated by diffusion models like Stable Diffusion are increasingly\nwidespread. Recent works and even lawsuits have shown that these models are\nprone to replicating their training data, unbeknownst to the user. In this\npaper, we first analyze this memorization problem in text-to-image diffusion\nmodels. While it is widely believed that duplicated images in the training set\nare responsible for content replication at inference time, we observe that the\ntext conditioning of the model plays a similarly important role. In fact, we\nsee in our experiments that data replication often does not happen for\nunconditional models, while it is common in the text-conditional case.\nMotivated by our findings, we then propose several techniques for reducing data\nreplication at both training and inference time by randomizing and augmenting\nimage captions in the training set.",
        "translated": "由稳定扩散等扩散模型产生的图像越来越广泛。最近的工作，甚至诉讼已经表明，这些模型倾向于复制他们的训练数据，用户不知道。本文首先分析了文本-图像扩散模型中的记忆问题。虽然人们普遍认为训练集中的重复图像负责推理时的内容复制，但是我们观察到模型的文本条件作用也起着类似的重要作用。事实上，在我们的实验中，我们看到数据复制通常不会发生在无条件模型中，而在文本条件的情况下却很常见。在我们的研究结果的激励下，我们提出了几种技术，通过在训练集中随机化和增强图像标题来减少训练和推理时间的数据复制。"
    },
    {
        "title": "Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D\n  Diffusion-based Editor",
        "url": "http://arxiv.org/abs/2305.20082v1",
        "pub_date": "2023-05-31",
        "summary": "Recent years have witnessed considerable achievements in editing images with\ntext instructions. When applying these editors to dynamic scene editing, the\nnew-style scene tends to be temporally inconsistent due to the frame-by-frame\nnature of these 2D editors. To tackle this issue, we propose Control4D, a novel\napproach for high-fidelity and temporally consistent 4D portrait editing.\nControl4D is built upon an efficient 4D representation with a 2D\ndiffusion-based editor. Instead of using direct supervisions from the editor,\nour method learns a 4D GAN from it and avoids the inconsistent supervision\nsignals. Specifically, we employ a discriminator to learn the generation\ndistribution based on the edited images and then update the generator with the\ndiscrimination signals. For more stable training, multi-level information is\nextracted from the edited images and used to facilitate the learning of the\ngenerator. Experimental results show that Control4D surpasses previous\napproaches and achieves more photo-realistic and consistent 4D editing\nperformances. The link to our project website is\nhttps://control4darxiv.github.io.",
        "translated": "近年来，在利用文本指令编辑图像方面取得了相当大的成就。当这些编辑器应用于动态场景编辑时，由于这些二维编辑器的逐帧性质，新风格的场景往往会出现时间上的不一致。为了解决这个问题，我们提出 Control4D，一种新颖的高保真度和时间一致的4D 肖像编辑方法。Control4D 是建立在一个有效的4D 表示与2D 扩散为基础的编辑器。该方法不需要编辑器的直接监控，而是从编辑器中学习一个4D GAN，避免了监控信号的不一致。具体地说，我们使用一个鉴别器来学习基于编辑后的图像的生成分布，然后用鉴别信号更新生成器。为了获得更稳定的训练，从编辑后的图像中提取多层次信息，以便于生成器的学习。实验结果表明，Control4D 编辑方法优于以往的编辑方法，具有更好的逼真度和一致性。我们项目网站的链接是 https://control4darxiv.github.io 的。"
    },
    {
        "title": "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation",
        "url": "http://arxiv.org/abs/2305.20074v1",
        "pub_date": "2023-05-31",
        "summary": "This paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
        "translated": "本文提出了分层函数最大相关算法(HFMCA) ，这是一种描述多视图系统中两个层次之间依赖关系的分层方法。通过将视图相似性框架为依赖关系并通过正交性确保对比度，HFMCA 在自监督学习中实现了更快的收敛和更高的稳定性。HFMCA 定义和度量图像层次结构中的依赖关系，从像素和补丁到完整图像。我们发现，近似网络拓扑标准正交基函数的方法与传统的有线电视新闻网(CNN)方法相一致，能够分解相邻特征映射层之间的密度比。这种方法提供了强大的可解释性，通过内部表征的透镜揭示了监督和自我监督之间的相似性。"
    },
    {
        "title": "Chatting Makes Perfect -- Chat-based Image Retrieval",
        "url": "http://arxiv.org/abs/2305.20062v1",
        "pub_date": "2023-05-31",
        "summary": "Chats emerge as an effective user-friendly approach for information\nretrieval, and are successfully employed in many domains, such as customer\nservice, healthcare, and finance. However, existing image retrieval approaches\ntypically address the case of a single query-to-image round, and the use of\nchats for image retrieval has been mostly overlooked. In this work, we\nintroduce ChatIR: a chat-based image retrieval system that engages in a\nconversation with the user to elicit information, in addition to an initial\nquery, in order to clarify the user's search intent. Motivated by the\ncapabilities of today's foundation models, we leverage Large Language Models to\ngenerate follow-up questions to an initial image description. These questions\nform a dialog with the user in order to retrieve the desired image from a large\ncorpus. In this study, we explore the capabilities of such a system tested on a\nlarge dataset and reveal that engaging in a dialog yields significant gains in\nimage retrieval. We start by building an evaluation pipeline from an existing\nmanually generated dataset and explore different modules and training\nstrategies for ChatIR. Our comparison includes strong baselines derived from\nrelated applications trained with Reinforcement Learning. Our system is capable\nof retrieving the target image from a pool of 50K images with over 78% success\nrate after 5 dialogue rounds, compared to 75% when questions are asked by\nhumans, and 64% for a single shot text-to-image retrieval. Extensive\nevaluations reveal the strong capabilities and examine the limitations of\nCharIR under different settings.",
        "translated": "聊天作为一种有效的用户友好的方式出现在信息检索，并成功地应用于许多领域，如客户服务，医疗保健和金融。然而，现有的图像检索方法通常只处理一轮图像查询，而且大多忽视了聊天对图像检索的作用。在这项工作中，我们介绍了 ChatIR: 一个基于聊天的图像检索系统，除了初始查询之外，它还与用户进行对话以获取信息，从而阐明用户的搜索意图。受到当今基础模型功能的启发，我们利用大型语言模型来生成对初始图像描述的后续问题。这些问题与用户形成一个对话框，以便从大型语料库中检索所需的图像。在这项研究中，我们探讨了这样一个系统的能力，测试了一个大型数据集，并揭示了从事对话产生显着的图像检索收益。我们首先从现有的手动生成的数据集构建评估流水线，并探索 ChatIR 的不同模块和培训策略。我们的比较包括来自受过强化学习培训的相关应用程序的强大基线。我们的系统能够从50K 图像池中检索目标图像，经过5轮对话后，成功率超过78% ，相比之下，人类提问时的成功率为75% ，单镜头文本到图像检索时的成功率为64% 。广泛的评估揭示了强大的能力，并审查了不同设置下的 CharIR 的局限性。"
    },
    {
        "title": "Exploring Regions of Interest: Visualizing Histological Image\n  Classification for Breast Cancer using Deep Learning",
        "url": "http://arxiv.org/abs/2305.20058v1",
        "pub_date": "2023-05-31",
        "summary": "Computer aided detection and diagnosis systems based on deep learning have\nshown promising performance in breast cancer detection. However, there are\ncases where the obtained results lack justification. In this study, our\nobjective is to highlight the regions of interest used by a convolutional\nneural network (CNN) for classifying histological images as benign or\nmalignant. We compare these regions with the regions identified by\npathologists. To achieve this, we employed the VGG19 architecture and tested\nthree visualization methods: Gradient, LRP Z, and LRP Epsilon. Additionally, we\nexperimented with three pixel selection methods: Bins, K-means, and MeanShift.\nBased on the results obtained, the Gradient visualization method and the\nMeanShift selection method yielded satisfactory outcomes for visualizing the\nimages.",
        "translated": "基于深度学习的计算机辅助检测与诊断系统在乳腺癌检测中表现出良好的应用前景。然而，在某些情况下，所得结果缺乏合理性。在这项研究中，我们的目标是突出卷积神经网络(CNN)用于将组织学图像分类为良性或恶性的感兴趣区域。我们将这些区域与病理学家鉴定的区域进行比较。为了实现这一点，我们采用了 VGG19架构，并测试了三种可视化方法: 梯度、 LRP Z 和 LRP Epsilon。此外，我们还试验了三种像素选择方法: Bins、 K- 均值和 mean Shift。在此基础上，采用梯度可视化方法和 MeanShift 选择方法对图像进行可视化处理，取得了满意的效果。"
    },
    {
        "title": "Cross-Domain Car Detection Model with Integrated Convolutional Block\n  Attention Mechanism",
        "url": "http://arxiv.org/abs/2305.20055v1",
        "pub_date": "2023-05-31",
        "summary": "Car detection, particularly through camera vision, has become a major focus\nin the field of computer vision and has gained widespread adoption. While\ncurrent car detection systems are capable of good detection, reliable detection\ncan still be challenging due to factors such as proximity between the car,\nlight intensity, and environmental visibility. To address these issues, we\npropose a cross-domain car detection model that we apply to car recognition for\nautonomous driving and other areas. Our model includes several novelties:\n1)Building a complete cross-domain target detection framework. 2)Developing an\nunpaired target domain picture generation module with an integrated\nconvolutional attention mechanism. 3)Adopting Generalized Intersection over\nUnion (GIOU) as the loss function of the target detection framework.\n4)Designing an object detection model integrated with two-headed Convolutional\nBlock Attention Module(CBAM). 5)Utilizing an effective data enhancement method.\nTo evaluate the model's effectiveness, we performed a reduced will resolution\nprocess on the data in the SSLAD dataset and used it as the benchmark dataset\nfor our task. Experimental results show that the performance of the\ncross-domain car target detection model improves by 40% over the model without\nour framework, and our improvements have a significant impact on cross-domain\ncar recognition.",
        "translated": "汽车检测，特别是通过摄像机视觉，已成为计算机视觉领域的一个主要焦点，并得到了广泛的采用。虽然目前的汽车检测系统能够很好的检测，可靠的检测仍然是具有挑战性的因素，如汽车之间的接近，光强度和环境能见度。为了解决这些问题，我们提出了一个跨领域的汽车检测模型，我们应用于汽车识别的自主驾驶和其他领域。该模型具有以下特点: 1)建立了一个完整的跨域目标检测框架。2)开发具有集成卷积注意机制的非配对目标域图像生成模块。3)采用广义交叉口作为目标检测框架的损失函数。4)设计一个集成了双头卷积块注意模块(CBAM)的目标检测模型。5)采用有效的数据增强方法。为了评估模型的有效性，我们对 SSLAD 数据集中的数据进行了简化的意愿分辨率处理，并将其作为我们任务的基准数据集。实验结果表明，本文提出的跨域汽车目标检测模型的性能比没有本文框架的模型提高了40% ，并且本文的改进对跨域汽车识别具有重要的影响。"
    },
    {
        "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
        "url": "http://arxiv.org/abs/2306.00989v1",
        "pub_date": "2023-06-01",
        "summary": "Modern hierarchical vision transformers have added several vision-specific\ncomponents in the pursuit of supervised classification performance. While these\ncomponents lead to effective accuracies and attractive FLOP counts, the added\ncomplexity actually makes these transformers slower than their vanilla ViT\ncounterparts. In this paper, we argue that this additional bulk is unnecessary.\nBy pretraining with a strong visual pretext task (MAE), we can strip out all\nthe bells-and-whistles from a state-of-the-art multi-stage vision transformer\nwithout losing accuracy. In the process, we create Hiera, an extremely simple\nhierarchical vision transformer that is more accurate than previous models\nwhile being significantly faster both at inference and during training. We\nevaluate Hiera on a variety of tasks for image and video recognition. Our code\nand models are available at https://github.com/facebookresearch/hiera.",
        "translated": ""
    },
    {
        "title": "StyleGAN knows Normal, Depth, Albedo, and More",
        "url": "http://arxiv.org/abs/2306.00987v1",
        "pub_date": "2023-06-01",
        "summary": "Intrinsic images, in the original sense, are image-like maps of scene\nproperties like depth, normal, albedo or shading. This paper demonstrates that\nStyleGAN can easily be induced to produce intrinsic images. The procedure is\nstraightforward. We show that, if StyleGAN produces $G({w})$ from latents\n${w}$, then for each type of intrinsic image, there is a fixed offset ${d}_c$\nso that $G({w}+{d}_c)$ is that type of intrinsic image for $G({w})$. Here\n${d}_c$ is {\\em independent of ${w}$}. The StyleGAN we used was pretrained by\nothers, so this property is not some accident of our training regime. We show\nthat there are image transformations StyleGAN will {\\em not} produce in this\nfashion, so StyleGAN is not a generic image regression engine.\n  It is conceptually exciting that an image generator should ``know'' and\nrepresent intrinsic images. There may also be practical advantages to using a\ngenerative model to produce intrinsic images. The intrinsic images obtained\nfrom StyleGAN compare well both qualitatively and quantitatively with those\nobtained by using SOTA image regression techniques; but StyleGAN's intrinsic\nimages are robust to relighting effects, unlike SOTA methods.",
        "translated": ""
    },
    {
        "title": "Continual Learning for Abdominal Multi-Organ and Tumor Segmentation",
        "url": "http://arxiv.org/abs/2306.00988v1",
        "pub_date": "2023-06-01",
        "summary": "The ability to dynamically extend a model to new data and classes is critical\nfor multiple organ and tumor segmentation. However, due to privacy regulations,\naccessing previous data and annotations can be problematic in the medical\ndomain. This poses a significant barrier to preserving the high segmentation\naccuracy of the old classes when learning from new classes because of the\ncatastrophic forgetting problem. In this paper, we first empirically\ndemonstrate that simply using high-quality pseudo labels can fairly mitigate\nthis problem in the setting of organ segmentation. Furthermore, we put forward\nan innovative architecture designed specifically for continuous organ and tumor\nsegmentation, which incurs minimal computational overhead. Our proposed design\ninvolves replacing the conventional output layer with a suite of lightweight,\nclass-specific heads, thereby offering the flexibility to accommodate newly\nemerging classes. These heads enable independent predictions for newly\nintroduced and previously learned classes, effectively minimizing the impact of\nnew classes on old ones during the course of continual learning. We further\npropose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings\ninto the organ-specific heads. These embeddings encapsulate the semantic\ninformation of each class, informed by extensive image-text co-training. The\nproposed method is evaluated on both in-house and public abdominal CT datasets\nunder organ and tumor segmentation tasks. Empirical results suggest that the\nproposed design improves the segmentation performance of a baseline neural\nnetwork on newly-introduced and previously-learned classes along the learning\ntrajectory.",
        "translated": ""
    },
    {
        "title": "Diffusion Self-Guidance for Controllable Image Generation",
        "url": "http://arxiv.org/abs/2306.00986v1",
        "pub_date": "2023-06-01",
        "summary": "Large-scale generative models are capable of producing high-quality images\nfrom detailed text descriptions. However, many aspects of an image are\ndifficult or impossible to convey through text. We introduce self-guidance, a\nmethod that provides greater control over generated images by guiding the\ninternal representations of diffusion models. We demonstrate that properties\nsuch as the shape, location, and appearance of objects can be extracted from\nthese representations and used to steer sampling. Self-guidance works similarly\nto classifier guidance, but uses signals present in the pretrained model\nitself, requiring no additional models or training. We show how a simple set of\nproperties can be composed to perform challenging image manipulations, such as\nmodifying the position or size of objects, merging the appearance of objects in\none image with the layout of another, composing objects from many images into\none, and more. We also show that self-guidance can be used to edit real images.\nFor results and an interactive demo, see our project page at\nhttps://dave.ml/selfguidance/",
        "translated": ""
    },
    {
        "title": "Using generative AI to investigate medical imagery models and datasets",
        "url": "http://arxiv.org/abs/2306.00985v1",
        "pub_date": "2023-06-01",
        "summary": "AI models have shown promise in many medical imaging tasks. However, our\nability to explain what signals these models have learned is severely lacking.\nExplanations are needed in order to increase the trust in AI-based models, and\ncould enable novel scientific discovery by uncovering signals in the data that\nare not yet known to experts. In this paper, we present a method for automatic\nvisual explanations leveraging team-based expertise by generating hypotheses of\nwhat visual signals in the images are correlated with the task. We propose the\nfollowing 4 steps: (i) Train a classifier to perform a given task (ii) Train a\nclassifier guided StyleGAN-based image generator (StylEx) (iii) Automatically\ndetect and visualize the top visual attributes that the classifier is sensitive\ntowards (iv) Formulate hypotheses for the underlying mechanisms, to stimulate\nfuture research. Specifically, we present the discovered attributes to an\ninterdisciplinary panel of experts so that hypotheses can account for social\nand structural determinants of health. We demonstrate results on eight\nprediction tasks across three medical imaging modalities: retinal fundus\nphotographs, external eye photographs, and chest radiographs. We showcase\nexamples of attributes that capture clinically known features, confounders that\narise from factors beyond physiological mechanisms, and reveal a number of\nphysiologically plausible novel attributes. Our approach has the potential to\nenable researchers to better understand, improve their assessment, and extract\nnew knowledge from AI-based models. Importantly, we highlight that attributes\ngenerated by our framework can capture phenomena beyond physiology or\npathophysiology, reflecting the real world nature of healthcare delivery and\nsocio-cultural factors. Finally, we intend to release code to enable\nresearchers to train their own StylEx models and analyze their predictive\ntasks.",
        "translated": ""
    },
    {
        "title": "StyleDrop: Text-to-Image Generation in Any Style",
        "url": "http://arxiv.org/abs/2306.00983v1",
        "pub_date": "2023-06-01",
        "summary": "Pre-trained large text-to-image models synthesize impressive images with an\nappropriate use of text prompts. However, ambiguities inherent in natural\nlanguage and out-of-distribution effects make it hard to synthesize image\nstyles, that leverage a specific design pattern, texture or material. In this\npaper, we introduce StyleDrop, a method that enables the synthesis of images\nthat faithfully follow a specific style using a text-to-image model. The\nproposed method is extremely versatile and captures nuances and details of a\nuser-provided style, such as color schemes, shading, design patterns, and local\nand global effects. It efficiently learns a new style by fine-tuning very few\ntrainable parameters (less than $1\\%$ of total model parameters) and improving\nthe quality via iterative training with either human or automated feedback.\nBetter yet, StyleDrop is able to deliver impressive results even when the user\nsupplies only a single image that specifies the desired style. An extensive\nstudy shows that, for the task of style tuning text-to-image models, StyleDrop\nimplemented on Muse convincingly outperforms other methods, including\nDreamBooth and textual inversion on Imagen or Stable Diffusion. More results\nare available at our project website: https://styledrop.github.io",
        "translated": ""
    },
    {
        "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual\n  Representation Learners",
        "url": "http://arxiv.org/abs/2306.00984v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate the potential of learning visual representations using\nsynthetic images generated by text-to-image models. This is a natural question\nin the light of the excellent performance of such models in generating\nhigh-quality images. We consider specifically the Stable Diffusion, one of the\nleading open source text-to-image models. We show that (1) when the generative\nmodel is configured with proper classifier-free guidance scale, training\nself-supervised methods on synthetic images can match or beat the real image\ncounterpart; (2) by treating the multiple images generated from the same text\nprompt as positives for each other, we develop a multi-positive contrastive\nlearning method, which we call StableRep. With solely synthetic images, the\nrepresentations learned by StableRep surpass the performance of representations\nlearned by SimCLR and CLIP using the same set of text prompts and corresponding\nreal images, on large scale datasets. When we further add language supervision,\nStableRep trained with 20M synthetic images achieves better accuracy than CLIP\ntrained with 50M real images.",
        "translated": ""
    },
    {
        "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two\n  Seconds",
        "url": "http://arxiv.org/abs/2306.00980v1",
        "pub_date": "2023-06-01",
        "summary": "Text-to-image diffusion models can create stunning images from natural\nlanguage descriptions that rival the work of professional artists and\nphotographers. However, these models are large, with complex network\narchitectures and tens of denoising iterations, making them computationally\nexpensive and slow to run. As a result, high-end GPUs and cloud-based inference\nare required to run diffusion models at scale. This is costly and has privacy\nimplications, especially when user data is sent to a third party. To overcome\nthese challenges, we present a generic approach that, for the first time,\nunlocks running text-to-image diffusion models on mobile devices in less than\n$2$ seconds. We achieve so by introducing efficient network architecture and\nimproving step distillation. Specifically, we propose an efficient UNet by\nidentifying the redundancy of the original model and reducing the computation\nof the image decoder via data distillation. Further, we enhance the step\ndistillation by exploring training strategies and introducing regularization\nfrom classifier-free guidance. Our extensive experiments on MS-COCO show that\nour model with $8$ denoising steps achieves better FID and CLIP scores than\nStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation\nby bringing powerful text-to-image diffusion models to the hands of users.",
        "translated": ""
    },
    {
        "title": "Building Rearticulable Models for Arbitrary 3D Objects from 4D Point\n  Clouds",
        "url": "http://arxiv.org/abs/2306.00979v1",
        "pub_date": "2023-06-01",
        "summary": "We build rearticulable models for arbitrary everyday man-made objects\ncontaining an arbitrary number of parts that are connected together in\narbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of such\neveryday objects, our method identifies the distinct object parts, what parts\nare connected to what other parts, and the properties of the joints connecting\neach part pair. We do this by jointly optimizing the part segmentation,\ntransformation, and kinematics using a novel energy minimization framework. Our\ninferred animatable models, enables retargeting to novel poses with sparse\npoint correspondences guidance. We test our method on a new articulating robot\ndataset, and the Sapiens dataset with common daily objects, as well as\nreal-world scans. Experiments show that our method outperforms two leading\nprior works on various metrics.",
        "translated": ""
    },
    {
        "title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation",
        "url": "http://arxiv.org/abs/2306.00977v1",
        "pub_date": "2023-06-01",
        "summary": "During interactive segmentation, a model and a user work together to\ndelineate objects of interest in a 3D point cloud. In an iterative process, the\nmodel assigns each data point to an object (or the background), while the user\ncorrects errors in the resulting segmentation and feeds them back into the\nmodel. From a machine learning perspective the goal is to design the model and\nthe feedback mechanism in a way that minimizes the required user input. The\ncurrent best practice segments objects one at a time, and asks the user to\nprovide positive clicks to indicate regions wrongly assigned to the background\nand negative clicks to indicate regions wrongly assigned to the object\n(foreground). Sequentially visiting objects is wasteful, since it disregards\nsynergies between objects: a positive click for a given object can, by\ndefinition, serve as a negative click for nearby objects, moreover a direct\ncompetition between adjacent objects can speed up the identification of their\ncommon boundary. We introduce AGILE3D, an efficient, attention-based model that\n(1) supports simultaneous segmentation of multiple 3D objects, (2) yields more\naccurate segmentation masks with fewer user clicks, and (3) offers faster\ninference. We encode the point cloud into a latent feature representation, and\nview user clicks as queries and employ cross-attention to represent contextual\nrelations between different click locations as well as between clicks and the\n3D point cloud features. Every time new clicks are added, we only need to run a\nlightweight decoder that produces updated segmentation masks. In experiments\nwith four different point cloud datasets, AGILE3D sets a new state of the art,\nmoreover, we also verify its practicality in real-world setups with a real user\nstudy.",
        "translated": ""
    },
    {
        "title": "OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection",
        "url": "http://arxiv.org/abs/2306.01738v1",
        "pub_date": "2023-06-02",
        "summary": "Multi-view 3D object detection is becoming popular in autonomous driving due\nto its high effectiveness and low cost. Most of the current state-of-the-art\ndetectors follow the query-based bird's-eye-view (BEV) paradigm, which benefits\nfrom both BEV's strong perception power and end-to-end pipeline. Despite\nachieving substantial progress, existing works model objects via globally\nleveraging temporal and spatial information of BEV features, resulting in\nproblems when handling the challenging complex and dynamic autonomous driving\nscenarios. In this paper, we proposed an Object-Centric query-BEV detector\nOCBEV, which can carve the temporal and spatial cues of moving targets more\neffectively. OCBEV comprises three designs: Object Aligned Temporal Fusion\naligns the BEV feature based on ego-motion and estimated current locations of\nmoving objects, leading to a precise instance-level feature fusion. Object\nFocused Multi-View Sampling samples more 3D features from an adaptive local\nheight ranges of objects for each scene to enrich foreground information.\nObject Informed Query Enhancement replaces part of pre-defined decoder queries\nin common DETR-style decoders with positional features of objects on\nhigh-confidence locations, introducing more direct object positional priors.\nExtensive experimental evaluations are conducted on the challenging nuScenes\ndataset. Our approach achieves a state-of-the-art result, surpassing the\ntraditional BEVFormer by 1.5 NDS points. Moreover, we have a faster convergence\nspeed and only need half of the training iterations to get comparable\nperformance, which further demonstrates its effectiveness.",
        "translated": "多视角三维目标检测由于其高效、低成本的特点，在自动驾驶领域越来越受到人们的青睐。目前大多数最先进的检测器遵循基于查询的鸟瞰(BEV)范式，这得益于 BEV 强大的感知能力和端到端流水线。尽管取得了重大进展，现有的工程模型通过全球利用 BEV 功能的时间和空间信息对对象进行建模，导致在处理具有挑战性的复杂和动态自主驾驶场景时出现问题。本文提出了一种以对象为中心的查询-BEV 检测器 OCBEV，它可以更有效地刻画运动目标的时间和空间线索。OCBEV 包括三种设计: 基于自我运动和运动物体当前位置估计的对象对齐时间融合(Object AlliedTimalFusion)对准 BEV 特征，实现精确的实例级特征融合。对象聚焦多视点采样从一个自适应的局部高度范围的对象更多的三维特征为每个场景，以丰富前景信息。对象知情查询增强在常见的 DETR 样式的解码器中替换了部分预定义的解码器查询，在高置信度位置上引入了对象的位置特征，引入了更直接的对象位置先验。广泛的实验评估是在具有挑战性的 nuScenes 数据集上进行的。我们的方法获得了最先进的结果，比传统的 BEV 方法提高了1.5个 NDS 点。此外，算法收敛速度较快，只需要一半的训练迭代就可以获得相当的性能，进一步证明了算法的有效性。"
    },
    {
        "title": "DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model",
        "url": "http://arxiv.org/abs/2306.01736v1",
        "pub_date": "2023-06-02",
        "summary": "Observing the close relationship among panoptic, semantic and instance\nsegmentation tasks, we propose to train a universal multi-dataset multi-task\nsegmentation model: DaTaSeg.We use a shared representation (mask proposals with\nclass predictions) for all tasks. To tackle task discrepancy, we adopt\ndifferent merge operations and post-processing for different tasks. We also\nleverage weak-supervision, allowing our segmentation model to benefit from\ncheaper bounding box annotations. To share knowledge across datasets, we use\ntext embeddings from the same semantic embedding space as classifiers and share\nall network parameters among datasets. We train DaTaSeg on ADE semantic, COCO\npanoptic, and Objects365 detection datasets. DaTaSeg improves performance on\nall datasets, especially small-scale datasets, achieving 54.0 mIoU on ADE\nsemantic and 53.5 PQ on COCO panoptic. DaTaSeg also enables weakly-supervised\nknowledge transfer on ADE panoptic and Objects365 instance segmentation.\nExperiments show DaTaSeg scales with the number of training datasets and\nenables open-vocabulary segmentation through direct transfer. In addition, we\nannotate an Objects365 instance segmentation set of 1,000 images and will\nrelease it as a public benchmark.",
        "translated": "考虑到视觉、语义和实例分割任务之间的密切关系，我们提出了一个通用的多数据集多任务分割模型: DataSeg. 我们对所有任务使用共享表示(带有类预测的掩码提议)。为了解决任务间的差异，我们针对不同的任务采用不同的合并操作和后处理。我们还利用弱监督，允许我们的细分模型受益于更便宜的边界框注释。为了在数据集之间共享知识，我们使用来自同一语义嵌入空间的文本嵌入作为分类器，并在数据集之间共享所有的网络参数。我们在 ADE 语义、 COCO 全景和 Objects365检测数据集上训练 DataSeg。DaTaSeg 提高了所有数据集的性能，特别是小规模数据集，在 ADE 语义上达到54.0 mIoU，在 COCO 泛光网络上达到53.5 PQ。DaTaSeg 还支持弱监督的 ADE 全景和 Objects365实例分割知识转移。实验结果表明，DataSeg 可以根据训练数据集的数量进行分割，并且可以通过直接传输实现开放词汇表的分割。此外，我们还注释了一个由1,000个图像组成的 Objects365实例分割集，并将其作为公共基准发布。"
    },
    {
        "title": "Multilingual Conceptual Coverage in Text-to-Image Models",
        "url": "http://arxiv.org/abs/2306.01735v1",
        "pub_date": "2023-06-02",
        "summary": "We propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique\nfor benchmarking the degree to which any generative text-to-image system\nprovides multilingual parity to its training language in terms of tangible\nnouns. For each model we can assess \"conceptual coverage\" of a given target\nlanguage relative to a source language by comparing the population of images\ngenerated for a series of tangible nouns in the source language to the\npopulation of images generated for each noun under translation in the target\nlanguage. This technique allows us to estimate how well-suited a model is to a\ntarget language as well as identify model-specific weaknesses, spurious\ncorrelations, and biases without a-priori assumptions. We demonstrate how it\ncan be used to benchmark T2I models in terms of multilinguality, and how\ndespite its simplicity it is a good proxy for impressive generalization.",
        "translated": "我们提出了“跨语言的概念覆盖”(CoCo-CroLa) ，一种基准测试的程度，任何生成性文本到图像系统提供多语言平等的训练语言在有形名词方面。对于每个模型，我们可以通过比较源语言中一系列有形名词生成的图像的总体与目标语言中翻译下的每个名词生成的图像的总体来评估给定目标语言相对于源语言的“概念覆盖”。这种技术使我们能够估计模型与目标语言的匹配程度，并且在没有先验假设的情况下识别特定于模型的弱点、虚假的相关性和偏差。我们展示了如何使用它来基准 T2I 模型的多语言性，以及如何尽管它的简单性，它是一个令人印象深刻的推广良好的代理。"
    },
    {
        "title": "DocFormerv2: Local Features for Document Understanding",
        "url": "http://arxiv.org/abs/2306.01733v1",
        "pub_date": "2023-06-02",
        "summary": "We propose DocFormerv2, a multi-modal transformer for Visual Document\nUnderstanding (VDU). The VDU domain entails understanding documents (beyond\nmere OCR predictions) e.g., extracting information from a form, VQA for\ndocuments and other tasks. VDU is challenging as it needs a model to make sense\nof multiple modalities (visual, language and spatial) to make a prediction. Our\napproach, termed DocFormerv2 is an encoder-decoder transformer which takes as\ninput - vision, language and spatial features. DocFormerv2 is pre-trained with\nunsupervised tasks employed asymmetrically i.e., two novel document tasks on\nencoder and one on the auto-regressive decoder. The unsupervised tasks have\nbeen carefully designed to ensure that the pre-training encourages\nlocal-feature alignment between multiple modalities. DocFormerv2 when evaluated\non nine datasets shows state-of-the-art performance over strong baselines e.g.\nTabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization\ncapabilities, on three VQA tasks involving scene-text, Doc- Formerv2\noutperforms previous comparably-sized models and even does better than much\nlarger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive\nablations show that due to its pre-training, DocFormerv2 understands multiple\nmodalities better than prior-art in VDU.",
        "translated": "我们提出 DocFormerv2，一个用于可视化文档理解(VDU)的多模式转换器。VDU 领域需要理解文档(超越单纯的 OCR 预测) ，例如，从表单中提取信息，文档的 VQA 和其他任务。VDU 是具有挑战性的，因为它需要一个模型来理解多种形式(视觉、语言和空间)来做出预测。我们的方法，称为 DocFormerv2是一个编码器-解码器转换器，它采取作为输入-视觉，语言和空间特征。DocFormerv2预先训练了非对称使用的非监督任务，即编码器上的两个新的文档任务和自动回归解码器上的一个任务。这些无监督的任务经过精心设计，以确保预先培训鼓励多种模式之间的局部特征对齐。对9个数据集进行评估后，DocFormerv2显示出超过强基线的最先进性能，例如 TabFact (4.3%) ，InfoVQA (1.4%) ，FUNSD (1%)。此外，为了显示泛化能力，在涉及场景文本的三个 VQA 任务中，Doc-Formerv2在一些任务中表现优于以前的同等大小的模型，甚至优于更大的模型(如 GIT2、 PaLi 和 Flamingo)。广泛的消融表明，由于其预先培训，DocFormerv2了解多种形式更好地比先前的技术在 VDU。"
    },
    {
        "title": "Video Colorization with Pre-trained Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2306.01732v1",
        "pub_date": "2023-06-02",
        "summary": "Video colorization is a challenging task that involves inferring plausible\nand temporally consistent colors for grayscale frames. In this paper, we\npresent ColorDiffuser, an adaptation of a pre-trained text-to-image latent\ndiffusion model for video colorization. With the proposed adapter-based\napproach, we repropose the pre-trained text-to-image model to accept input\ngrayscale video frames, with the optional text description, for video\ncolorization. To enhance the temporal coherence and maintain the vividness of\ncolorization across frames, we propose two novel techniques: the Color\nPropagation Attention and Alternated Sampling Strategy. Color Propagation\nAttention enables the model to refine its colorization decision based on a\nreference latent frame, while Alternated Sampling Strategy captures\nspatiotemporal dependencies by using the next and previous adjacent latent\nframes alternatively as reference during the generative diffusion sampling\nsteps. This encourages bidirectional color information propagation between\nadjacent video frames, leading to improved color consistency across frames. We\nconduct extensive experiments on benchmark datasets, and the results\ndemonstrate the effectiveness of our proposed framework. The evaluations show\nthat ColorDiffuser achieves state-of-the-art performance in video colorization,\nsurpassing existing methods in terms of color fidelity, temporal consistency,\nand visual quality.",
        "translated": "视频着色是一个具有挑战性的任务，涉及推断合理和时间一致的灰度帧颜色。在本文中，我们提出了一个比较适合于视频彩色化的文本到图像潜在扩散模型。采用基于适配器的方法，我们重新提出了预训练的文本到图像模型来接受输入的灰度视频帧，以及可选的文本描述，用于视频彩色化。为了提高时间一致性和保持跨帧彩色化的生动性，我们提出了两种新的技术: 色彩传播注意和交替采样策略。色彩传播注意力使模型能够基于参考潜框架精化其着色决策，而交替采样策略通过在生成扩散采样步骤中交替使用下一个和前一个相邻潜框架作为参考来捕获时空依赖性。这将鼓励相邻视频帧之间的双向颜色信息传播，从而提高帧间的颜色一致性。我们对基准数据集进行了广泛的实验，实验结果表明了我们提出的框架的有效性。评估结果表明，ColorDiffuser 在视频彩色化方面达到了最先进的水平，在色彩保真度、时间一致性和视觉质量方面都超过了现有的彩色化方法。"
    },
    {
        "title": "Denoising Diffusion Semantic Segmentation with Mask Prior Modeling",
        "url": "http://arxiv.org/abs/2306.01721v1",
        "pub_date": "2023-06-02",
        "summary": "The evolution of semantic segmentation has long been dominated by learning\nmore discriminative image representations for classifying each pixel. Despite\nthe prominent advancements, the priors of segmentation masks themselves, e.g.,\ngeometric and semantic constraints, are still under-explored. In this paper, we\npropose to ameliorate the semantic segmentation quality of existing\ndiscriminative approaches with a mask prior modeled by a recently-developed\ndenoising diffusion generative model. Beginning with a unified architecture\nthat adapts diffusion models for mask prior modeling, we focus this work on a\nspecific instantiation with discrete diffusion and identify a variety of key\ndesign choices for its successful application. Our exploratory analysis\nrevealed several important findings, including: (1) a simple integration of\ndiffusion models into semantic segmentation is not sufficient, and a\npoorly-designed diffusion process might lead to degradation in segmentation\nperformance; (2) during the training, the object to which noise is added is\nmore important than the type of noise; (3) during the inference, the strict\ndiffusion denoising scheme may not be essential and can be relaxed to a simpler\nscheme that even works better. We evaluate the proposed prior modeling with\nseveral off-the-shelf segmentors, and our experimental results on ADE20K and\nCityscapes demonstrate that our approach could achieve competitively\nquantitative performance and more appealing visual quality.",
        "translated": "长期以来，语义分割的发展主要是通过学习更具鉴别力的图像表示来对每个像素进行分类。尽管取得了显著的进步，但是先前的分割掩盖本身，例如，几何和语义约束，仍然没有得到充分的探索。在这篇文章中，我们建议改善现有的区分方法的语义分割质量，使用最近开发的去噪扩散生成模型建模的掩模。从一个统一的体系结构开始，该体系结构将扩散模型应用于掩模先验建模，我们将工作集中在一个具体的离散扩散实例上，并确定了各种关键的设计选择，以使其成功应用。我们的探索性分析揭示了几个重要的发现，包括: (1)扩散模型在语义分割中的简单整合是不够的，设计不当的扩散过程可能导致分割性能的下降; (2)在训练过程中，加入噪声的对象比噪声类型更重要; (3)在推断过程中，严格的扩散去噪方案可能不是必要的，可以放宽到一个更简单的方案，甚至更好的工作。我们用几个现成的分割器来评估提出的先验模型，我们在 ADE20K 和 Cityscape 上的实验结果表明，我们的方法可以实现有竞争力的定量性能和更吸引人的视觉质量。"
    },
    {
        "title": "Resolving Interference When Merging Models",
        "url": "http://arxiv.org/abs/2306.01708v1",
        "pub_date": "2023-06-02",
        "summary": "Transfer learning - i.e., further fine-tuning a pre-trained model on a\ndownstream task - can confer significant advantages, including improved\ndownstream performance, faster convergence, and better sample efficiency. These\nadvantages have led to a proliferation of task-specific fine-tuned models,\nwhich typically can only perform a single task and do not benefit from one\nanother. Recently, model merging techniques have emerged as a solution to\ncombine multiple task-specific models into a single multitask model without\nperforming additional training. However, existing merging methods often ignore\nthe interference between parameters of different models, resulting in large\nperformance drops when merging multiple models. In this paper, we demonstrate\nthat prior merging techniques inadvertently lose valuable information due to\ntwo major sources of interference: (a) interference due to redundant parameter\nvalues and (b) disagreement on the sign of a given parameter's values across\nmodels. To address this, we propose our method, TrIm, Elect Sign &amp; Merge\n(TIES-Merging), which introduces three novel steps when merging models: (1)\nresetting parameters that only changed a small amount during fine-tuning, (2)\nresolving sign conflicts, and (3) merging only the parameters that are in\nalignment with the final agreed-upon sign. We find that TIES-Merging\noutperforms several existing methods in diverse settings covering a range of\nmodalities, domains, number of tasks, model sizes, architectures, and\nfine-tuning settings. We further analyze the impact of different types of\ninterference on model parameters, highlight the importance of resolving sign\ninterference. Our code is available at\nhttps://github.com/prateeky2806/ties-merging",
        "translated": "转移学习——即进一步微调下游任务的预先训练的模型——可以带来显著的优势，包括改善下游性能、加快收敛速度和提高采样效率。这些优势导致了特定于任务的微调模型的激增，这些模型通常只能执行单个任务，并且不能从彼此中受益。最近，模型合并技术已经成为一种解决方案，可以将多个任务特定的模型合并成一个单一的多任务模型，而不需要进行额外的训练。然而，现有的合并方法往往忽略了不同模型参数之间的干扰，导致合并多个模型时性能大幅度下降。在本文中，我们证明了先前的合并技术无意中失去了有价值的信息，由于两个主要的干扰来源: (a)由于冗余参数值的干扰和(b)在给定的参数值的符号不一致跨模型。为了解决这个问题，我们提出了我们的方法，TrIm，Elect Sign & Merge (TIES-Merging) ，它在合并模型时引入了三个新的步骤: (1)重置在微调过程中只改变了很少量的参数，(2)解决符号冲突，(3)只合并与最终达成一致的符号一致的参数。我们发现 TIES-Merging 在不同的设置中优于几种现有的方法，包括一系列模式、领域、任务数量、模型大小、架构和微调设置。进一步分析了不同类型的干扰对模型参数的影响，强调了解决符号干扰的重要性。我们的代码可以在 https://github.com/prateeky2806/ties-merging 找到"
    },
    {
        "title": "Is Generative Modeling-based Stylization Necessary for Domain Adaptation\n  in Regression Tasks?",
        "url": "http://arxiv.org/abs/2306.01706v1",
        "pub_date": "2023-06-02",
        "summary": "Unsupervised domain adaptation (UDA) aims to bridge the gap between source\nand target domains in the absence of target domain labels using two main\ntechniques: input-level alignment (such as generative modeling and stylization)\nand feature-level alignment (which matches the distribution of the feature\nmaps, e.g. gradient reversal layers). Motivated from the success of generative\nmodeling for image classification, stylization-based methods were recently\nproposed for regression tasks, such as pose estimation. However, use of\ninput-level alignment via generative modeling and stylization incur additional\noverhead and computational complexity which limit their use in real-world DA\ntasks. To investigate the role of input-level alignment for DA, we ask the\nfollowing question: Is generative modeling-based stylization necessary for\nvisual domain adaptation in regression? Surprisingly, we find that\ninput-alignment has little effect on regression tasks as compared to\nclassification. Based on these insights, we develop a non-parametric\nfeature-level domain alignment method -- Implicit Stylization (ImSty) -- which\nresults in consistent improvements over SOTA regression task, without the need\nfor computationally intensive stylization and generative modeling. Our work\nconducts a critical evaluation of the role of generative modeling and\nstylization, at a time when these are also gaining popularity for domain\ngeneralization.",
        "translated": "无监督域自适应(UDA)的目标是在没有目标域标签的情况下，使用两种主要技术来弥合源域和目标域之间的差距: 输入级别对齐(如生成建模和样式化)和特征级别对齐(匹配特征映射的分布，如梯度反转层)。由于生成建模在图像分类中的成功，基于风格化的方法最近被提出用于回归任务，如姿态估计。然而，通过生成建模和样式化使用输入级对齐会带来额外的开销和计算复杂性，从而限制了它们在实际 DA 任务中的使用。为了研究输入水平对齐在 DA 中的作用，我们提出以下问题: 基于生成建模的程式化对于回归中的视觉领域适应是必要的吗？令人惊讶的是，我们发现与分类相比，输入对齐对回归任务的影响很小。基于这些见解，我们开发了一种非参数特征级别的领域对齐方法——隐式样式化(ImSty)——它能够在 SOTA 回归任务上取得一致的改进，而不需要计算密集型样式化和生成建模。我们的工作对生成建模和程式化的作用进行了批判性的评估，同时它们也因领域泛化而越来越受欢迎。"
    },
    {
        "title": "Unique Brain Network Identification Number for Parkinson's Individuals\n  Using Structural MRI",
        "url": "http://arxiv.org/abs/2306.01689v1",
        "pub_date": "2023-06-02",
        "summary": "We propose a novel algorithm called Unique Brain Network Identification\nNumber (UBNIN) for encoding brain networks of individual subject. To realize\nthis objective, we employed T1-weighted structural MRI of 180 Parkinson's\ndisease (PD) patients from National Institute of Mental Health and\nNeurosciences, India. We parcellated each subject's brain volume and\nconstructed individual adjacency matrix using correlation between grey matter\n(GM) volume of every pair of regions. The unique code is derived from values\nrepresenting connections of every node (i), weighted by a factor of 2^-(i-1).\nThe numerical representation UBNIN was observed to be distinct for each\nindividual brain network, which may also be applied to other neuroimaging\nmodalities. This model may be implemented as neural signature of a person's\nunique brain connectivity, thereby useful for brainprinting applications.\nAdditionally, we segregated the above dataset into five age-cohorts:\nA:22-32years, B:33-42years, C:43-52years, D:53-62years and E:63-72years to\nstudy the variation in network topology over age. Sparsity was adopted as the\nthreshold estimate to binarize each age-based correlation matrix. Connectivity\nmetrics were obtained using Brain Connectivity toolbox-based MATLAB functions.\nFor each age-cohort, a decreasing trend was observed in mean clustering\ncoefficient with increasing sparsity. Significantly different clustering\ncoefficient was noted between age-cohort B and C (sparsity: 0.63,0.66), C and E\n(sparsity: 0.66,0.69). Our findings suggest network connectivity patterns\nchange with age, indicating network disruption due to the underlying\nneuropathology. Varying clustering coefficient for different cohorts indicate\nthat information transfer between neighboring nodes change with age. This\nprovides evidence on age-related brain shrinkage and network degeneration.",
        "translated": "我们提出了一种新的算法称为唯一的脑网络识别号(UBNIN)编码的个人主题的大脑网络。为了实现这一目标，我们对来自印度国家精神卫生和神经科学研究所的180名帕金森病(PD)患者进行了 T1加权结构 MRI 检查。我们将每个受试者的大脑体积打包，并利用每对区域的灰质(GM)体积之间的相关性构建个体邻接矩阵。唯一的代码是从代表每个节点(i)的连接的值派生出来的，由2 ^-(i-1)的因子加权。数字代表 UBNIN 被观察到是不同的每个个人的大脑网络，这也可以应用于其他神经影像模式。这个模型可以作为一个人独特的大脑连通性的神经特征来实现，因此对于大脑印记应用是有用的。此外，我们将上述数据集分为五个年龄组: A: 22-32岁，B: 33-42岁，C: 43-52岁，D: 53-62岁和 E: 63-72岁，以研究网络拓扑随年龄的变化。采用稀疏度作为阈值估计，对每个基于年龄的相关矩阵进行二值化。使用基于 MATLAB 函数的脑连通性工具箱获得连通性度量。对于每个年龄组，随着稀疏程度的增加，平均集聚系数呈下降趋势。年龄组别乙与丙(稀疏度: 0.63,0.66)、丙与戊(稀疏度: 0.66,0.69)的集聚系数有显著差异。我们的研究结果表明，网络连接模式随着年龄的增长而改变，表明网络中断是由于潜在的神经病理学。不同群组的不同集聚系数表明，相邻节点之间的信息传递随着年龄的增长而变化。这为年龄相关的大脑萎缩和网络退化提供了证据。"
    },
    {
        "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1\n  Updates",
        "url": "http://arxiv.org/abs/2306.01685v1",
        "pub_date": "2023-06-02",
        "summary": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using\nRank-1 updates, called MKOR, that improves the training time and convergence\nproperties of deep neural networks (DNNs). Second-order techniques, while\nenjoying higher convergence rates vs first-order counterparts, have cubic\ncomplexity with respect to either the model size and/or the training batch\nsize. Hence they exhibit poor scalability and performance in transformer\nmodels, e.g. large language models (LLMs), because the batch sizes in these\nmodels scale by the attention mechanism sequence length, leading to large model\nsize and batch sizes. MKOR's complexity is quadratic with respect to the model\nsize, alleviating the computation bottlenecks in second-order methods. Because\nof their high computation complexity, state-of-the-art implementations of\nsecond-order methods can only afford to update the second order information\ninfrequently, and thus do not fully exploit the promise of better convergence\nfrom these updates. By reducing the communication complexity of the\nsecond-order updates as well as achieving a linear communication complexity,\nMKOR increases the frequency of second order updates. We also propose a hybrid\nversion of MKOR (called MKOR-H) that mid-training falls backs to a first order\noptimizer if the second order updates no longer accelerate convergence. Our\nexperiments show that MKOR outperforms state -of-the-art first order methods,\ne.g. the LAMB optimizer, and best implementations of second-order methods, i.e.\nKAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64\nGPUs.",
        "translated": "这项工作提出了一个动量启用 Kronecker 因子为基础的优化使用秩-1更新，称为 MKOR，改善训练时间和收敛性能的深层神经网络(DNN)。二阶技术虽然比一阶技术具有更高的收敛速度，但在模型大小和/或训练批量大小方面具有立方复杂度。因此，它们在变压器模型(例如大语言模型(LLM))中表现出较差的可扩展性和性能，因为这些模型中的批量大小是由注意机制序列长度决定的，从而导致模型大小和批量大小。MKOR 算法的复杂度与模型大小呈二次函数关系，减少了二阶方法的计算瓶颈。由于二阶方法的高计算复杂度，现有的二阶方法只能很少地更新二阶信息，因此无法充分利用这些更新所带来的更好收敛的前景。通过降低二阶更新的通信复杂度以及实现线性通信复杂度，MKOR 增加了二阶更新的频率。我们还提出了 MKOR 的一个混合版本(称为 MKOR-H) ，如果二阶更新不再加速收敛，则中间训练退回到一阶优化器。我们的实验表明，MKOR 优于最先进的一阶方法(例如 LAMB 优化器)和二阶方法(例如 KAISA/KFAC)的最佳实现，在 BERT-Large-Uncase 上分别达到2.57 x 和1.85 x，在64个 GPU 上。"
    },
    {
        "title": "Neuralangelo: High-Fidelity Neural Surface Reconstruction",
        "url": "http://arxiv.org/abs/2306.03092v1",
        "pub_date": "2023-06-05",
        "summary": "Neural surface reconstruction has been shown to be powerful for recovering\ndense 3D surfaces via image-based neural rendering. However, current methods\nstruggle to recover detailed structures of real-world scenes. To address the\nissue, we present Neuralangelo, which combines the representation power of\nmulti-resolution 3D hash grids with neural surface rendering. Two key\ningredients enable our approach: (1) numerical gradients for computing\nhigher-order derivatives as a smoothing operation and (2) coarse-to-fine\noptimization on the hash grids controlling different levels of details. Even\nwithout auxiliary inputs such as depth, Neuralangelo can effectively recover\ndense 3D surface structures from multi-view images with fidelity significantly\nsurpassing previous methods, enabling detailed large-scale scene reconstruction\nfrom RGB video captures.",
        "translated": "神经表面重建已被证明是有效的恢复密集的三维表面的图像基于神经绘制。然而，目前的方法很难恢复真实世界场景的详细结构。为了解决这个问题，我们提出了 Neuralangelo，它结合了多分辨率3D 哈希网格的表示能力和神经表面绘制。两个关键因素使我们的方法: (1)数值梯度计算高阶导数作为一个平滑运算和(2)粗细优化哈希网格控制不同层次的细节。即使没有像深度这样的辅助输入，Neuralangelo 也可以有效地从多视图图像中恢复密集的3D 表面结构，其逼真度远远超过以往的方法，从而能够利用 RGB 视频捕捉进行详细的大规模场景重建。"
    },
    {
        "title": "Brain Diffusion for Visual Exploration: Cortical Discovery using Large\n  Scale Generative Models",
        "url": "http://arxiv.org/abs/2306.03089v1",
        "pub_date": "2023-06-05",
        "summary": "A long standing goal in neuroscience has been to elucidate the functional\norganization of the brain. Within higher visual cortex, functional accounts\nhave remained relatively coarse, focusing on regions of interest (ROIs) and\ntaking the form of selectivity for broad categories such as faces, places,\nbodies, food, or words. Because the identification of such ROIs has typically\nrelied on manually assembled stimulus sets consisting of isolated objects in\nnon-ecological contexts, exploring functional organization without robust a\npriori hypotheses has been challenging. To overcome these limitations, we\nintroduce a data-driven approach in which we synthesize images predicted to\nactivate a given brain region using paired natural images and fMRI recordings,\nbypassing the need for category-specific stimuli. Our approach -- Brain\nDiffusion for Visual Exploration (\"BrainDiVE\") -- builds on recent generative\nmethods by combining large-scale diffusion models with brain-guided image\nsynthesis. Validating our method, we demonstrate the ability to synthesize\npreferred images with appropriate semantic specificity for well-characterized\ncategory-selective ROIs. We then show that BrainDiVE can characterize\ndifferences between ROIs selective for the same high-level category. Finally we\nidentify novel functional subdivisions within these ROIs, validated with\nbehavioral data. These results advance our understanding of the fine-grained\nfunctional organization of human visual cortex, and provide well-specified\nconstraints for further examination of cortical organization using\nhypothesis-driven methods.",
        "translated": "神经科学的一个长期目标是阐明大脑的功能组织。在高级视觉皮层中，功能性描述仍然相对粗糙，集中在感兴趣的区域(ROI) ，并采取广泛类别的选择性形式，如面孔、地点、身体、食物或词汇。由于这种 ROI 的识别通常依赖于在非生态环境中由孤立对象组成的人工组装的刺激集，因此在没有强有力的先验假设的情况下探索功能性组织一直是具有挑战性的。为了克服这些局限性，我们引入了一种数据驱动的方法，其中我们使用配对的自然图像和 fMRI 记录合成预测激活给定大脑区域的图像，绕过对类别特定刺激的需求。我们的方法——用于视觉探索的大脑扩散(BrainDiVE)——建立在最近的生成方法的基础上，将大规模扩散模型与大脑引导的图像合成相结合。通过验证我们的方法，我们证明了合成具有适当语义特异性的首选图像的能力，以及特征明确的类别选择性 ROI。然后，我们表明 BrainDiVE 可以表征不同的 ROI 选择相同的高级类别。最后，我们在这些 ROI 中识别新的功能细分，并用行为数据进行验证。这些结果提高了我们对人类视觉皮层细粒度功能组织的理解，并为使用假设驱动的方法进一步检查皮层组织提供了明确的约束条件。"
    },
    {
        "title": "Of Mice and Mates: Automated Classification and Modelling of Mouse\n  Behaviour in Groups using a Single Model across Cages",
        "url": "http://arxiv.org/abs/2306.03066v1",
        "pub_date": "2023-06-05",
        "summary": "Behavioural experiments often happen in specialised arenas, but this may\nconfound the analysis. To address this issue, we provide tools to study mice in\nthe homecage environment, equipping biologists with the possibility to capture\nthe temporal aspect of the individual's behaviour and model the interaction and\ninterdependence between cage-mates with minimal human intervention. We develop\nthe Activity Labelling Module (ALM) to automatically classify mouse behaviour\nfrom video, and a novel Group Behaviour Model (GBM) for summarising their joint\nbehaviour across cages, using a permutation matrix to match the mouse\nidentities in each cage to the model. We also release two datasets, ABODe for\ntraining behaviour classifiers and IMADGE for modelling behaviour.",
        "translated": "行为实验往往发生在专业领域，但这可能会混淆分析。为了解决这个问题，我们提供了在家庭环境中研究小鼠的工具，使生物学家有可能捕获个体行为的时间方面，并以最小的人类干预模拟笼友之间的相互作用和相互依赖。我们开发了活动标签模块(ALM)来自动分类视频中的老鼠行为，以及一个新的群体行为模型(GBM)来总结它们在笼子中的联合行为，使用一个置换矩阵来匹配每个笼子中的老鼠身份与模型。我们还发布了两个数据集，ABODe 用于训练行为分类器，IMADGE 用于建模行为。"
    },
    {
        "title": "ELEV-VISION: Automated Lowest Floor Elevation Estimation from Segmenting\n  Street View Images",
        "url": "http://arxiv.org/abs/2306.03050v1",
        "pub_date": "2023-06-05",
        "summary": "We propose an automated lowest floor elevation (LFE) estimation algorithm\nbased on computer vision techniques to leverage the latent information in\nstreet view images. Flood depth-damage models use a combination of LFE and\nflood depth for determining flood risk and extent of damage to properties. We\nused image segmentation for detecting door bottoms and roadside edges from\nGoogle Street View images. The characteristic of equirectangular projection\nwith constant spacing representation of horizontal and vertical angles allows\nextraction of the pitch angle from the camera to the door bottom. The depth\nfrom the camera to the door bottom was obtained from the depthmap paired with\nthe Google Street View image. LFEs were calculated from the pitch angle and the\ndepth. The testbed for application of the proposed method is Meyerland (Harris\nCounty, Texas). The results show that the proposed method achieved mean\nabsolute error of 0.190 m (1.18 %) in estimating LFE. The height difference\nbetween the street and the lowest floor (HDSL) was estimated to provide\ninformation for flood damage estimation. The proposed automatic LFE estimation\nalgorithm using Street View images and image segmentation provides a rapid and\ncost-effective method for LFE estimation compared with the surveys using total\nstation theodolite and unmanned aerial systems. By obtaining more accurate and\nup-to-date LFE data using the proposed method, city planners, emergency\nplanners and insurance companies could make a more precise estimation of flood\ndamage.",
        "translated": "提出了一种基于计算机视觉技术的自动最低楼层高度(LFE)估计算法，以利用街景图像中的潜在信息。洪水深度-损害模型使用 LFE 和洪水深度的组合来确定洪水风险和对财产的损害程度。我们使用图像分割来检测谷歌街景图像中的门底和路边边缘。等矩形投影的特点是水平和垂直角度的间距表示为恒定的，可以提取从摄像机到门底部的俯仰角。从相机到门底部的深度是通过谷歌街景图获得的。根据节距角和深度计算 LFEs。提出的方法的应用试验台是 Meyerland (德克萨斯州哈里斯县)。结果表明，该方法估计 LFE 的平均绝对误差为0.190 m (1.18%)。估算了街道与最低层之间的高度差，为洪水损失估算提供了依据。与使用图像分割和无人驾驶航空系统进行的调查相比，建议的街景图像自动估算算法提供了一个快速和具成本效益的估算全站仪生命周期的方法。通过使用这种方法获得更准确和最新的 LFE 数据，城市规划者、应急规划者和保险公司可以对洪水损失进行更精确的估计。"
    },
    {
        "title": "HeadSculpt: Crafting 3D Head Avatars with Text",
        "url": "http://arxiv.org/abs/2306.03038v1",
        "pub_date": "2023-06-05",
        "summary": "Recently, text-guided 3D generative methods have made remarkable advancements\nin producing high-quality textures and geometry, capitalizing on the\nproliferation of large vision-language and image diffusion models. However,\nexisting methods still struggle to create high-fidelity 3D head avatars in two\naspects: (1) They rely mostly on a pre-trained text-to-image diffusion model\nwhilst missing the necessary 3D awareness and head priors. This makes them\nprone to inconsistency and geometric distortions in the generated avatars. (2)\nThey fall short in fine-grained editing. This is primarily due to the inherited\nlimitations from the pre-trained 2D image diffusion models, which become more\npronounced when it comes to 3D head avatars. In this work, we address these\nchallenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt\nfor crafting (i.e., generating and editing) 3D head avatars from textual\nprompts. Specifically, we first equip the diffusion model with 3D awareness by\nleveraging landmark-based control and a learned textual embedding representing\nthe back view appearance of heads, enabling 3D-consistent head avatar\ngenerations. We further propose a novel identity-aware editing score\ndistillation strategy to optimize a textured mesh with a high-resolution\ndifferentiable rendering technique. This enables identity preservation while\nfollowing the editing instruction. We showcase HeadSculpt's superior fidelity\nand editing capabilities through comprehensive experiments and comparisons with\nexisting methods.",
        "translated": "最近，文本引导的3D 生成方法在生成高质量的纹理和几何图形方面取得了显著的进展，利用了大型视觉语言和图像扩散模型的增殖。然而，现有的方法仍然难以在两个方面创建高保真度的3D 头像: (1)他们主要依赖于预先训练的文本到图像的扩散模型，同时缺乏必要的3D 意识和头部先验。这使得他们容易产生不一致性和几何失真的化身。(2)在细粒度编辑方面存在不足。这主要是由于预先训练的2D 图像扩散模型遗传的局限性，当涉及到3D 头像时，这种局限性变得更加明显。在这项工作中，我们通过引入一个多功能的从粗到精的流水线，称为 HeadSculpt，用于从文本提示中生成(即，生成和编辑)3D 头部化身，来解决这些挑战。具体来说，我们首先通过利用基于里程碑的控制和代表头部背景外观的学习文本嵌入，使扩散模型具有3D 感知能力，从而实现3D 一致的头部头像生成。我们进一步提出了一种新的身份感知编辑分数提取策略，以优化纹理网格与高分辨率可微渲染技术。这样可以在按照编辑指令进行编辑的同时进行食品生产履历。我们通过全面的实验和与现有方法的比较，展示了 HeadScult 卓越的保真度和编辑能力。"
    },
    {
        "title": "Interpretable Alzheimer's Disease Classification Via a Contrastive\n  Diffusion Autoencoder",
        "url": "http://arxiv.org/abs/2306.03022v1",
        "pub_date": "2023-06-05",
        "summary": "In visual object classification, humans often justify their choices by\ncomparing objects to prototypical examples within that class. We may therefore\nincrease the interpretability of deep learning models by imbuing them with a\nsimilar style of reasoning. In this work, we apply this principle by\nclassifying Alzheimer's Disease based on the similarity of images to training\nexamples within the latent space. We use a contrastive loss combined with a\ndiffusion autoencoder backbone, to produce a semantically meaningful latent\nspace, such that neighbouring latents have similar image-level features. We\nachieve a classification accuracy comparable to black box approaches on a\ndataset of 2D MRI images, whilst producing human interpretable model\nexplanations. Therefore, this work stands as a contribution to the pertinent\ndevelopment of accurate and interpretable deep learning within medical imaging.",
        "translated": "在可视对象分类中，人们经常通过比较对象和类中的原型例子来证明他们的选择是正确的。因此，我们可以通过灌输类似的推理风格来增加深度学习模型的可解释性。在这项工作中，我们应用这个原则，通过分类阿尔茨海默氏病的基础上相似的图像训练例子在潜在的空间。我们使用对比度损失结合扩散自动编码骨干，产生一个语义上有意义的潜在空间，使邻近的潜在具有相似的图像级特征。我们在二维 MRI 图像的数据集上获得了与黑匣子方法相当的分类精度，同时产生了人类可解释的模型解释。因此，这项工作是对准确和可解释的医学影像深度学习的相关发展的一个贡献。"
    },
    {
        "title": "Automating Style Analysis and Visualization With Explainable AI -- Case\n  Studies on Brand Recognition",
        "url": "http://arxiv.org/abs/2306.03021v1",
        "pub_date": "2023-06-05",
        "summary": "Incorporating style-related objectives into shape design has been centrally\nimportant to maximize product appeal. However, stylistic features such as\naesthetics and semantic attributes are hard to codify even for experts. As\nsuch, algorithmic style capture and reuse have not fully benefited from\nautomated data-driven methodologies due to the challenging nature of design\ndescribability. This paper proposes an AI-driven method to fully automate the\ndiscovery of brand-related features. Our approach introduces BIGNet, a two-tier\nBrand Identification Graph Neural Network (GNN) to classify and analyze scalar\nvector graphics (SVG). First, to tackle the scarcity of vectorized product\nimages, this research proposes two data acquisition workflows: parametric\nmodeling from small curve-based datasets, and vectorization from large\npixel-based datasets. Secondly, this study constructs a novel hierarchical GNN\narchitecture to learn from both SVG's curve-level and chunk-level parameters.\nIn the first case study, BIGNet not only classifies phone brands but also\ncaptures brand-related features across multiple scales, such as the location of\nthe lens, the height-width ratio, and the screen-frame gap, as confirmed by AI\nevaluation. In the second study, this paper showcases the generalizability of\nBIGNet learning from a vectorized car image dataset and validates the\nconsistency and robustness of its predictions given four scenarios. The results\nmatch the difference commonly observed in luxury vs. economy brands in the\nautomobile market. Finally, this paper also visualizes the activation maps\ngenerated from a convolutional neural network and shows BIGNet's advantage of\nbeing a more human-friendly, explainable, and explicit style-capturing agent.\nCode and dataset can be found on Github:\n  1. Phone case study: github.com/parksandrecfan/bignet-phone 2. Car case\nstudy: github.com/parksandrecfan/bignet-car",
        "translated": "将与风格相关的目标整合到形状设计中，对于最大限度地提高产品的吸引力至关重要。然而，诸如美学和语义属性之类的文体特征即使对于专家来说也很难编纂成文。因此，由于设计可描述性的挑战性，算法样式捕获和重用并没有完全受益于自动化的数据驱动方法。本文提出了一种人工智能驱动的方法来完全自动发现品牌相关的特征。该方法引入双层品牌识别图神经网络 BIGNet 对标量向量图进行分类和分析。首先，针对产品图像矢量化的不足，本研究提出了两种数据采集工作流程: 基于小型曲线数据集的参数化建模和基于大型像素数据集的矢量化。其次，本研究建构了一个新颖的层次式 GNN 架构，以学习 SVG 的曲线层和组块层参数。在第一个案例研究中，BIGNet 不仅对手机品牌进行分类，而且还在多个尺度上捕获与品牌相关的特征，如镜头的位置、高宽比和屏幕框架间隙，这一点得到了 AI 评估的证实。在第二个研究中，本文展示了 BIGNet 从一个向量化的汽车图像数据集中学习的可推广性，并在四个场景中验证了其预测的一致性和鲁棒性。研究结果与汽车市场上普遍观察到的奢侈品牌与经济型品牌之间的差异相吻合。最后，本文还展示了由卷积神经网络生成的激活地图，并展示了 BIGNet 作为一个更人性化、更易于解释和更明确的风格捕获代理的优势。代码和数据集可以在 Github 上找到: 1。电话案例研究:  github.com/parksandrecfan/bignet-Phone 2。汽车案例研究:  github.com/parksandrecfan/bignet-Car"
    },
    {
        "title": "Nonparametric Iterative Machine Teaching",
        "url": "http://arxiv.org/abs/2306.03007v2",
        "pub_date": "2023-06-05",
        "summary": "In this paper, we consider the problem of Iterative Machine Teaching (IMT),\nwhere the teacher provides examples to the learner iteratively such that the\nlearner can achieve fast convergence to a target model. However, existing IMT\nalgorithms are solely based on parameterized families of target models. They\nmainly focus on convergence in the parameter space, resulting in difficulty\nwhen the target models are defined to be functions without dependency on\nparameters. To address such a limitation, we study a more general task --\nNonparametric Iterative Machine Teaching (NIMT), which aims to teach\nnonparametric target models to learners in an iterative fashion. Unlike\nparametric IMT that merely operates in the parameter space, we cast NIMT as a\nfunctional optimization problem in the function space. To solve it, we propose\nboth random and greedy functional teaching algorithms. We obtain the iterative\nteaching dimension (ITD) of the random teaching algorithm under proper\nassumptions, which serves as a uniform upper bound of ITD in NIMT. Further, the\ngreedy teaching algorithm has a significantly lower ITD, which reaches a\ntighter upper bound of ITD in NIMT. Finally, we verify the correctness of our\ntheoretical findings with extensive experiments in nonparametric scenarios.",
        "translated": "本文研究迭代机器教学(IMT)问题，教师迭代地向学习者提供例子，使学习者能够快速收敛到目标模型。然而，现有的 IMT 算法仅仅基于目标模型的参数化族。这些问题主要集中在参数空间的收敛性上，给目标模型定义为不依赖于参数的函数带来困难。为了解决这一局限性，我们研究了一个更为普遍的课题——非参数迭代机器教学(NIMT) ，其目的是以迭代的方式向学习者传授非参数目标模型。与仅在参数空间中运行的参数 IMT 不同，我们将 NIMT 作为函数空间中的函数最佳化问题。为了解决这个问题，我们提出了随机函数教学算法和贪婪函数教学算法。在适当的假设条件下，得到了随机教学算法的迭代教学维数(ITD) ，作为 NIMT 中迭代教学维数的一个统一上界。此外，贪婪教学算法的 ITD 显著降低，在 NIMT 中达到更紧的 ITD 上限。最后，我们通过在非参数情景下的大量实验验证了理论结果的正确性。"
    },
    {
        "title": "Unveiling the Two-Faced Truth: Disentangling Morphed Identities for Face\n  Morphing Detection",
        "url": "http://arxiv.org/abs/2306.03002v1",
        "pub_date": "2023-06-05",
        "summary": "Morphing attacks keep threatening biometric systems, especially face\nrecognition systems. Over time they have become simpler to perform and more\nrealistic, as such, the usage of deep learning systems to detect these attacks\nhas grown. At the same time, there is a constant concern regarding the lack of\ninterpretability of deep learning models. Balancing performance and\ninterpretability has been a difficult task for scientists. However, by\nleveraging domain information and proving some constraints, we have been able\nto develop IDistill, an interpretable method with state-of-the-art performance\nthat provides information on both the identity separation on morph samples and\ntheir contribution to the final prediction. The domain information is learnt by\nan autoencoder and distilled to a classifier system in order to teach it to\nseparate identity information. When compared to other methods in the literature\nit outperforms them in three out of five databases and is competitive in the\nremaining.",
        "translated": "变形攻击不断威胁着生物识别系统，尤其是人脸识别系统。随着时间的推移，它们的执行变得更加简单，也更加现实，因此，使用深度学习系统来检测这些攻击已经增长。与此同时，深度学习模型缺乏可解释性一直是人们关注的问题。平衡性能和可解释性一直是科学家的一项艰巨任务。然而，通过利用领域信息和证明一些限制，我们已经能够开发 IDistill，一种可解释的方法，具有最先进的性能，提供变形样本的身份分离和它们对最终预测的贡献的信息。领域信息由自动编码器学习，并提取到分类器系统中，以便教它分离身份信息。与文献中的其他方法相比，它在五个数据库中的三个数据库中表现优于其他方法，并在其余数据库中具有竞争力。"
    },
    {
        "title": "BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance\n  Fields",
        "url": "http://arxiv.org/abs/2306.03000v1",
        "pub_date": "2023-06-05",
        "summary": "Neural rendering combines ideas from classical computer graphics and machine\nlearning to synthesize images from real-world observations. NeRF, short for\nNeural Radiance Fields, is a recent innovation that uses AI algorithms to\ncreate 3D objects from 2D images. By leveraging an interpolation approach, NeRF\ncan produce new 3D reconstructed views of complicated scenes. Rather than\ndirectly restoring the whole 3D scene geometry, NeRF generates a volumetric\nrepresentation called a ``radiance field,'' which is capable of creating color\nand density for every point within the relevant 3D space. The broad appeal and\nnotoriety of NeRF make it imperative to examine the existing research on the\ntopic comprehensively. While previous surveys on 3D rendering have primarily\nfocused on traditional computer vision-based or deep learning-based approaches,\nonly a handful of them discuss the potential of NeRF. However, such surveys\nhave predominantly focused on NeRF's early contributions and have not explored\nits full potential. NeRF is a relatively new technique continuously being\ninvestigated for its capabilities and limitations. This survey reviews recent\nadvances in NeRF and categorizes them according to their architectural designs,\nespecially in the field of novel view synthesis.",
        "translated": "神经渲染结合了经典计算机图形学和机器学习的思想，从真实世界的观察中合成图像。NeRF 是神经辐射场的简称，是最近的一项创新，它使用人工智能算法从2D 图像中创建3D 对象。通过利用插值方法，NERF 可以生成复杂场景的新的三维重建视图。NERF 不直接恢复整个3D 场景的几何形状，而是生成一个称为“辐射场”的体积表示，它能够为相关3D 空间中的每个点创建颜色和密度。由于 NERF 的广泛吸引力和恶名，因此必须全面审查关于该专题的现有研究。虽然之前的3D 渲染调查主要集中在传统的基于计算机视觉或基于深度学习的方法，但只有少数人讨论了 NeRF 的潜力。然而，这些调查主要集中在 NERF 的早期贡献，并没有充分挖掘其潜力。NERF 是一种相对较新的技术，由于其能力和局限性，正在不断地被研究。这项调查回顾了最近在 NERF 的进展，并根据他们的建筑设计进行分类，特别是在新视图合成领域。"
    },
    {
        "title": "SAM3D: Segment Anything in 3D Scenes",
        "url": "http://arxiv.org/abs/2306.03908v1",
        "pub_date": "2023-06-06",
        "summary": "In this work, we propose SAM3D, a novel framework that is able to predict\nmasks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB\nimages without further training or finetuning. For a point cloud of a 3D scene\nwith posed RGB images, we first predict segmentation masks of RGB images with\nSAM, and then project the 2D masks into the 3D points. Later, we merge the 3D\nmasks iteratively with a bottom-up merging approach. At each step, we merge the\npoint cloud masks of two adjacent frames with the bidirectional merging\napproach. In this way, the 3D masks predicted from different frames are\ngradually merged into the 3D masks of the whole 3D scene. Finally, we can\noptionally ensemble the result from our SAM3D with the over-segmentation\nresults based on the geometric information of the 3D scenes. Our approach is\nexperimented with ScanNet dataset and qualitative results demonstrate that our\nSAM3D achieves reasonable and fine-grained 3D segmentation results without any\ntraining or finetuning of SAM.",
        "translated": "在这项工作中，我们提出了 SAM3D，一个新颖的框架，能够通过利用分段任何模型(SAM)在 RGB 图像预测掩码，而不需要进一步的训练或微调。对于三维场景中的点云，我们首先用 SAM 预测 RGB 图像的分割模板，然后将二维模板投影到三维点上。随后，我们使用自底向上的合并方法迭代地合并3D 掩码。在每一步，我们合并两个相邻帧点云掩码的双向合并方法。这样，由不同帧预测的3D 掩模逐渐合并到整个3D 场景的3D 掩模中。最后，我们可以根据三维场景的几何信息，选择性地将来自 SAM3D 的结果与过度分割的结果进行集成。该方法在 ScanNet 数据集上进行了实验，定性结果表明，我们的 SAM3D 不需要对 SAM 进行任何训练或微调，就可以获得合理的、细粒度的三维分割结果。"
    },
    {
        "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
        "url": "http://arxiv.org/abs/2306.03899v1",
        "pub_date": "2023-06-06",
        "summary": "Vision foundation models such as Contrastive Vision-Language Pre-training\n(CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot\nperformance on image classification and segmentation tasks. However, the\nincorporation of CLIP and SAM for label-free scene understanding has yet to be\nexplored. In this paper, we investigate the potential of vision foundation\nmodels in enabling networks to comprehend 2D and 3D worlds without labelled\ndata. The primary challenge lies in effectively supervising networks under\nextremely noisy pseudo labels, which are generated by CLIP and further\nexacerbated during the propagation from the 2D to the 3D domain. To tackle\nthese challenges, we propose a novel Cross-modality Noisy Supervision (CNS)\nmethod that leverages the strengths of CLIP and SAM to supervise 2D and 3D\nnetworks simultaneously. In particular, we introduce a prediction consistency\nregularization to co-train 2D and 3D networks, then further impose the\nnetworks' latent space consistency using the SAM's robust feature\nrepresentation. Experiments conducted on diverse indoor and outdoor datasets\ndemonstrate the superior performance of our method in understanding 2D and 3D\nopen environments. Our 2D and 3D network achieves label-free semantic\nsegmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%,\nrespectively. And for nuScenes dataset, our performance is 26.8% with an\nimprovement of 6%. Code will be released\n(https://github.com/runnanchen/Label-Free-Scene-Understanding).",
        "translated": "对比视觉语言预训练(CLIP)和分段任意(SAM)等视觉基础模型在图像分类和分割任务中表现出了令人印象深刻的零镜头性能。然而，将 CLIP 和 SAM 结合起来用于无标签场景理解还有待探索。在本文中，我们研究的潜力，视觉基础模型，使网络理解二维和三维世界没有标记的数据。主要的挑战在于在极其嘈杂的伪标签下有效地监督网络，伪标签是由 CLIP 生成的，并在从2D 到3D 域的传播过程中进一步加剧。为了应对这些挑战，我们提出了一种新的跨模态噪声监控(CNS)方法，利用 CLIP 和 SAM 的优势，同时监控2D 和3D 网络。特别地，我们引入预测一致性正则化来协同训练二维和三维网络，然后利用 SAM 的鲁棒特征表示进一步强化网络的潜在空间一致性。在不同的室内和室外数据集上进行的实验表明，该方法在理解二维和三维开放环境方面具有优越的性能。我们的二维和三维网络在 ScanNet 上分别实现了28.4% 和33.5% 的无标签语义分割，分别提高了4.7% 和7.9% 。对于 nuScenes 数据集，我们的性能是26.8% ，提高了6% 。代码将被发布( https://github.com/runnanchen/label-free-scene-understanding )。"
    },
    {
        "title": "Emergent Correspondence from Image Diffusion",
        "url": "http://arxiv.org/abs/2306.03881v1",
        "pub_date": "2023-06-06",
        "summary": "Finding correspondences between images is a fundamental problem in computer\nvision. In this paper, we show that correspondence emerges in image diffusion\nmodels without any explicit supervision. We propose a simple strategy to\nextract this implicit knowledge out of diffusion networks as image features,\nnamely DIffusion FeaTures (DIFT), and use them to establish correspondences\nbetween real images. Without any additional fine-tuning or supervision on the\ntask-specific data or annotations, DIFT is able to outperform both\nweakly-supervised methods and competitive off-the-shelf features in identifying\nsemantic, geometric, and temporal correspondences. Particularly for semantic\ncorrespondence, DIFT from Stable Diffusion is able to outperform DINO and\nOpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k\nbenchmark. It even outperforms the state-of-the-art supervised methods on 9 out\nof 18 categories while remaining on par for the overall performance. Project\npage: https://diffusionfeatures.github.io",
        "translated": "寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何显式监督的情况下，在图像扩散模型中出现了对应。提出了一种简单的从扩散网络中提取隐含知识作为图像特征的方法，即扩散特征(DIFT) ，并利用它们建立真实图像之间的对应关系。在没有对任务特定数据或注释进行任何额外的微调或监督的情况下，DIFT 能够在识别语义、几何和时间对应方面胜过弱监督方法和竞争性现成特征。特别是在语义对应方面，在具有挑战性的 SPair-71k 基准测试中，来自稳定扩散的 DIFT 能够比 DINO 和 OpenCLIP 分别高出19和14个精度点。它甚至在18个类别中的9个方面表现优于最先进的监督方法，同时在整体表现上保持同等水平。项目主页:  https://diffusionfeatures.github.io"
    },
    {
        "title": "Conditional Diffusion Models for Weakly Supervised Medical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2306.03878v1",
        "pub_date": "2023-06-06",
        "summary": "Recent advances in denoising diffusion probabilistic models have shown great\nsuccess in image synthesis tasks. While there are already works exploring the\npotential of this powerful tool in image semantic segmentation, its application\nin weakly supervised semantic segmentation (WSSS) remains relatively\nunder-explored. Observing that conditional diffusion models (CDM) is capable of\ngenerating images subject to specific distributions, in this work, we utilize\ncategory-aware semantic information underlied in CDM to get the prediction mask\nof the target object with only image-level annotations. More specifically, we\nlocate the desired class by approximating the derivative of the output of CDM\nw.r.t the input condition. Our method is different from previous diffusion\nmodel methods with guidance from an external classifier, which accumulates\nnoises in the background during the reconstruction process. Our method\noutperforms state-of-the-art CAM and diffusion model methods on two public\nmedical image segmentation datasets, which demonstrates that CDM is a promising\ntool in WSSS. Also, experiment shows our method is more time-efficient than\nexisting diffusion model methods, making it practical for wider applications.",
        "translated": "近年来，扩散概率模型在图像合成任务中取得了很大的成功。虽然已经有工作探索这一强大的工具在图像语义分割的潜力，它在弱监督语义分割(WSSS)的应用仍然相对缺乏探索。考虑到条件扩散模型能够生成受特定分布影响的图像，在这项工作中，我们利用条件扩散模型下的类别感知语义信息来获得目标对象的预测掩码，只使用图像级别的注释。更具体地说，我们通过近似于输入条件 CDM w.r.t 输出的导数来定位所需的类。该方法不同于以往采用外部分类器引导的扩散模型方法，在重建过程中会在背景中积累噪声。我们的方法在两个公共医疗图像分割数据集上优于最先进的 CAM 和扩散模型方法，这表明 CDM 是 WSSS 中一个很有前途的工具。实验表明，该方法比现有的扩散模型方法具有更高的时间效率，具有更广泛的应用前景。"
    },
    {
        "title": "Learning with a Mole: Transferable latent spatial representations for\n  navigation without reconstruction",
        "url": "http://arxiv.org/abs/2306.03857v1",
        "pub_date": "2023-06-06",
        "summary": "Agents navigating in 3D environments require some form of memory, which\nshould hold a compact and actionable representation of the history of\nobservations useful for decision taking and planning. In most end-to-end\nlearning approaches the representation is latent and usually does not have a\nclearly defined interpretation, whereas classical robotics addresses this with\nscene reconstruction resulting in some form of map, usually estimated with\ngeometry and sensor models and/or learning. In this work we propose to learn an\nactionable representation of the scene independently of the targeted downstream\ntask and without explicitly optimizing reconstruction. The learned\nrepresentation is optimized by a blind auxiliary agent trained to navigate with\nit on multiple short sub episodes branching out from a waypoint and, most\nimportantly, without any direct visual observation. We argue and show that the\nblindness property is important and forces the (trained) latent representation\nto be the only means for planning. With probing experiments we show that the\nlearned representation optimizes navigability and not reconstruction. On\ndownstream tasks we show that it is robust to changes in distribution, in\nparticular the sim2real gap, which we evaluate with a real physical robot in a\nreal office building, significantly improving performance.",
        "translated": "在3D 环境中导航的代理需要某种形式的存储器，这种存储器应该能够对观测历史进行紧凑的、可操作的表示，这对于决策制定和规划非常有用。在大多数端到端的学习方法中，这种表示是潜在的，通常没有明确的解释，而经典的机器人通过场景重建来解决这个问题，从而产生某种形式的地图，通常用几何和传感器模型估计和/或学习。在这项工作中，我们建议学习一个独立于目标下游任务的可操作的场景表示，而不需要显式地优化重建。经过训练的盲辅助代理优化了学习表示，使其能够在多个从路径点分支出来的短子集中导航，最重要的是，没有任何直接的视觉观察。我们认为，并表明，盲性性质是重要的，迫使(训练)潜在表征是唯一的手段规划。通过探索性实验，我们发现学习表示优化了导航性，而不是重构性。在下游的任务，我们表明它是鲁棒的分布变化，特别是模拟真实的差距，我们评估了一个真实的物理机器人在一个真实的办公楼，显着提高性能。"
    },
    {
        "title": "Learning Human Mesh Recovery in 3D Scenes",
        "url": "http://arxiv.org/abs/2306.03847v1",
        "pub_date": "2023-06-06",
        "summary": "We present a novel method for recovering the absolute pose and shape of a\nhuman in a pre-scanned scene given a single image. Unlike previous methods that\nperform sceneaware mesh optimization, we propose to first estimate absolute\nposition and dense scene contacts with a sparse 3D CNN, and later enhance a\npretrained human mesh recovery network by cross-attention with the derived 3D\nscene cues. Joint learning on images and scene geometry enables our method to\nreduce the ambiguity caused by depth and occlusion, resulting in more\nreasonable global postures and contacts. Encoding scene-aware cues in the\nnetwork also allows the proposed method to be optimization-free, and opens up\nthe opportunity for real-time applications. The experiments show that the\nproposed network is capable of recovering accurate and physically-plausible\nmeshes by a single forward pass and outperforms state-of-the-art methods in\nterms of both accuracy and speed.",
        "translated": "我们提出了一种新的方法来恢复绝对位置和形状的人在一个预先扫描场景给定的一个单一的图像。与以前执行场景网格优化的方法不同，我们建议首先用稀疏的3D CNN 估计绝对位置和密集场景接触，然后通过与派生的3D 场景线索交叉注意来增强预先训练的人类网格恢复网络。通过对图像和场景几何的联合学习，该方法可以减少深度和遮挡引起的模糊，从而得到更合理的全局姿态和接触。在网络中对场景感知线索进行编码也允许提出的方法无需优化，并为实时应用提供了机会。实验结果表明，该网络能够通过单次前传恢复精确的和物理上合理的网格，并且在精度和速度方面都优于最先进的方法。"
    },
    {
        "title": "Atrial Septal Defect Detection in Children Based on Ultrasound Video\n  Using Multiple Instances Learning",
        "url": "http://arxiv.org/abs/2306.03835v1",
        "pub_date": "2023-06-06",
        "summary": "Purpose: Congenital heart defect (CHD) is the most common birth defect.\nThoracic echocardiography (TTE) can provide sufficient cardiac structure\ninformation, evaluate hemodynamics and cardiac function, and is an effective\nmethod for atrial septal defect (ASD) examination. This paper aims to study a\ndeep learning method based on cardiac ultrasound video to assist in ASD\ndiagnosis. Materials and methods: We select two standard views of the atrial\nseptum (subAS) and low parasternal four-compartment view (LPS4C) as the two\nviews to identify ASD. We enlist data from 300 children patients as part of a\ndouble-blind experiment for five-fold cross-validation to verify the\nperformance of our model. In addition, data from 30 children patients (15\npositives and 15 negatives) are collected for clinician testing and compared to\nour model test results (these 30 samples do not participate in model training).\nWe propose an echocardiography video-based atrial septal defect diagnosis\nsystem. In our model, we present a block random selection, maximal agreement\ndecision and frame sampling strategy for training and testing respectively,\nresNet18 and r3D networks are used to extract the frame features and aggregate\nthem to build a rich video-level representation. Results: We validate our model\nusing our private dataset by five-cross validation. For ASD detection, we\nachieve 89.33 AUC, 84.95 accuracy, 85.70 sensitivity, 81.51 specificity and\n81.99 F1 score. Conclusion: The proposed model is multiple instances\nlearning-based deep learning model for video atrial septal defect detection\nwhich effectively improves ASD detection accuracy when compared to the\nperformances of previous networks and clinical doctors.",
        "translated": "目的: 先天性心脏病是最常见的先天缺陷。胸超声心动图能提供足够的心脏结构资料、评估血流动力学及心脏功能，是检查心房中隔缺损的有效方法。本文旨在研究一种基于心脏超声视频的深度学习方法，以辅助 ASD 的诊断。材料和方法: 我们选择房间隔(subAS)和胸骨旁低位四室视(LPS4C)两种标准视图作为房间隔缺损的鉴别视图。作为双盲实验的一部分，我们征集了300名儿童患者的数据，用5倍交叉验证来验证我们模型的性能。此外，从30名儿童患者(15个阳性和15个阴性)收集数据用于临床医生测试，并与我们的模型测试结果进行比较(这30个样本不参加模型培训)。我们提出一个基于超声心动图视频的心房中隔缺损诊断系统。该模型分别采用块随机选择、最大一致性决策和帧采样策略进行训练和测试，利用 resNet18网络和 r3D 网络提取帧特征，并将其聚合成丰富的视频级表示。结果: 我们使用我们的私有数据集通过五交叉验证来验证我们的模型。对于 ASD 检测，我们获得了89.33 AUC，84.95准确性，85.70敏感性，81.51特异性和81.99 F1评分。结论: 该模型是基于多实例学习的深度学习视频心房中隔缺损检测模型，与以前的网络和临床医生相比，有效地提高了 ASD 检测的准确性。"
    },
    {
        "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
        "url": "http://arxiv.org/abs/2306.03831v1",
        "pub_date": "2023-06-06",
        "summary": "Recent progress in self-supervision has shown that pre-training large neural\nnetworks on vast amounts of unsupervised data can lead to substantial increases\nin generalization to downstream tasks. Such models, recently coined foundation\nmodels, have been transformational to the field of natural language processing.\nVariants have also been proposed for image data, but their applicability to\nremote sensing tasks is limited. To stimulate the development of foundation\nmodels for Earth monitoring, we propose a benchmark comprised of six\nclassification and six segmentation tasks, which were carefully curated and\nadapted to be both relevant to the field and well-suited for model evaluation.\nWe accompany this benchmark with a robust methodology for evaluating models and\nreporting aggregated results to enable a reliable assessment of progress.\nFinally, we report results for 20 baselines to gain information about the\nperformance of existing models. We believe that this benchmark will be a driver\nof progress across a variety of Earth monitoring tasks.",
        "translated": "最近在自我监督方面的进展表明，在大量无监督数据上预先训练大型神经网络可以导致对下游任务的一般化程度的大幅提高。这些模型，最近创造的基础模型，已经转化为自然语言处理领域。对于图像数据也提出了不同的方案，但是它们对于遥感任务的适用性是有限的。为了促进地球监测基础模型的发展，我们提出了一个由六个分类和六个分段任务组成的基准，这些任务经过精心策划和调整，既与实地相关又适合于模型评估。我们在这一基准的同时，还采用了一种强有力的方法来评估模型和报告总体结果，以便能够对进展情况进行可靠的评估。最后，我们报告20个基线的结果，以获得关于现有模型性能的信息。我们相信，这一基准将推动各种地球监测任务取得进展。"
    },
    {
        "title": "X-Align++: cross-modal cross-view alignment for Bird's-eye-view\n  segmentation",
        "url": "http://arxiv.org/abs/2306.03810v1",
        "pub_date": "2023-06-06",
        "summary": "Bird's-eye-view (BEV) grid is a typical representation of the perception of\nroad components, e.g., drivable area, in autonomous driving. Most existing\napproaches rely on cameras only to perform segmentation in BEV space, which is\nfundamentally constrained by the absence of reliable depth information. The\nlatest works leverage both camera and LiDAR modalities but suboptimally fuse\ntheir features using simple, concatenation-based mechanisms. In this paper, we\naddress these problems by enhancing the alignment of the unimodal features in\norder to aid feature fusion, as well as enhancing the alignment between the\ncameras' perspective view (PV) and BEV representations. We propose X-Align, a\nnovel end-to-end cross-modal and cross-view learning framework for BEV\nsegmentation consisting of the following components: (i) a novel Cross-Modal\nFeature Alignment (X-FA) loss, (ii) an attention-based Cross-Modal Feature\nFusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an\nauxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA)\nlosses to improve the PV-to-BEV transformation. We evaluate our proposed method\nacross two commonly used benchmark datasets, i.e., nuScenes and KITTI-360.\nNotably, X-Align significantly outperforms the state-of-the-art by 3 absolute\nmIoU points on nuScenes. We also provide extensive ablation studies to\ndemonstrate the effectiveness of the individual components.",
        "translated": "鸟瞰图(BEV)网格是自主驾驶中道路组成部分(如可行驶区域)感知的典型表示。大多数现有的方法仅仅依靠摄像机在 BEV 空间中进行分割，这是由于缺乏可靠的深度信息而造成的。最新的作品利用相机和激光雷达模式，但次优融合他们的功能使用简单，串联为基础的机制。本文通过提高单峰特征的对齐度来辅助特征融合，以及提高摄像机透视图(PV)和 BEV 表示之间的对齐度来解决这些问题。我们提出了 X-Align，一个新颖的端到端跨模态和跨视图的 BEV 分割学习框架，它包括以下组成部分: (i)一个新的跨模态特征对齐(X-FA)损失，(ii)一个基于注意力的跨模态特征融合(X-FF)模块隐式地对齐多模态 BEV 特征，和(iii)一个辅助的 PV 分割分支与跨视图分割对齐(X-SA)损失，以改善 PV-to-BEV 转换。我们通过两个常用的基准数据集，即 nuScenes 和 KITTI-360来评估我们提出的方法。值得注意的是，X-Align 在 nuScenes 上的性能明显比最先进的技术高出3个绝对 mIoU 点。我们还提供了广泛的消融研究，以证明个别组件的有效性。"
    },
    {
        "title": "Learning to Ground Instructional Articles in Videos through Narrations",
        "url": "http://arxiv.org/abs/2306.03802v1",
        "pub_date": "2023-06-06",
        "summary": "In this paper we present an approach for localizing steps of procedural\nactivities in narrated how-to videos. To deal with the scarcity of labeled data\nat scale, we source the step descriptions from a language knowledge base\n(wikiHow) containing instructional articles for a large variety of procedural\ntasks. Without any form of manual supervision, our model learns to temporally\nground the steps of procedural articles in how-to videos by matching three\nmodalities: frames, narrations, and step descriptions. Specifically, our method\naligns steps to video by fusing information from two distinct pathways: i) {\\em\ndirect} alignment of step descriptions to frames, ii) {\\em indirect} alignment\nobtained by composing steps-to-narrations with narrations-to-video\ncorrespondences. Notably, our approach performs global temporal grounding of\nall steps in an article at once by exploiting order information, and is trained\nwith step pseudo-labels which are iteratively refined and aggressively\nfiltered. In order to validate our model we introduce a new evaluation\nbenchmark -- HT-Step -- obtained by manually annotating a 124-hour subset of\nHowTo100M\\footnote{A test server is accessible at\n\\url{https://eval.ai/web/challenges/challenge-page/2082}.} with steps sourced\nfrom wikiHow articles. Experiments on this benchmark as well as zero-shot\nevaluations on CrossTask demonstrate that our multi-modality alignment yields\ndramatic gains over several baselines and prior works. Finally, we show that\nour inner module for matching narration-to-video outperforms by a large margin\nthe state of the art on the HTM-Align narration-video alignment benchmark.",
        "translated": "在本文中，我们提出了一种方法来本地化的程序性活动的步骤叙述如何视频。为了处理大规模标记数据的稀缺性，我们从一个语言知识库(wikiHow)中获取步骤描述，该知识库包含用于大量程序性任务的教学文章。没有任何形式的人工监督，我们的模型学会了通过匹配三种模式(框架、叙述和步骤描述)在时间上将程序性文章的步骤置于 how-to 视频中。具体来说，我们的方法通过融合来自两个不同途径的信息来将步骤与视频对齐: i){ em 直接}步骤描述与帧的对齐，ii){ em 间接}通过将步骤到叙述与叙述到视频的对应组合而获得的步骤到叙述的对齐。值得注意的是，我们的方法通过利用订单信息一次性执行文章中所有步骤的全局时间基础，并使用迭代改进和积极过滤的步骤伪标签进行训练。为了验证我们的模型，我们引入了一个新的评估基准—— HT-step ——通过手动注释 HowTo100M 脚注的一个124小时子集获得的(一个测试服务器可以在 url { https://eval.ai/web/challenges/challenge-page/2082}访问)步骤来源于 wikiHow 文章。在这个基准上的实验以及在 CrossTask 上的零射击评估表明，我们的多模态对齐比几个基线和以前的工作产生了巨大的收益。最后，我们表明，我们的内部模块的匹配叙述视频优于一个很大的差距的艺术状态的 HTM-Align 叙述视频对齐基准。"
    },
    {
        "title": "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image\n  Translation",
        "url": "http://arxiv.org/abs/2306.04636v1",
        "pub_date": "2023-06-07",
        "summary": "Recent advances in deep learning have witnessed many successful unsupervised\nimage-to-image translation models that learn correspondences between two visual\ndomains without paired data. However, it is still a great challenge to build\nrobust mappings between various domains especially for those with drastic\nvisual discrepancies. In this paper, we introduce a novel versatile framework,\nGenerative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), that\nimproves the quality, applicability and controllability of the existing\ntranslation models. The key idea of GP-UNIT is to distill the generative prior\nfrom pre-trained class-conditional GANs to build coarse-level cross-domain\ncorrespondences, and to apply the learned prior to adversarial translations to\nexcavate fine-level correspondences. With the learned multi-level content\ncorrespondences, GP-UNIT is able to perform valid translations between both\nclose domains and distant domains. For close domains, GP-UNIT can be\nconditioned on a parameter to determine the intensity of the content\ncorrespondences during translation, allowing users to balance between content\nand style consistency. For distant domains, semi-supervised learning is\nexplored to guide GP-UNIT to discover accurate semantic correspondences that\nare hard to learn solely from the appearance. We validate the superiority of\nGP-UNIT over state-of-the-art translation models in robust, high-quality and\ndiversified translations between various domains through extensive experiments.",
        "translated": "深度学习的最新进展见证了许多成功的无监督的图像到图像的转换模型，它们在没有配对数据的情况下学习两个视觉域之间的对应关系。然而，在不同领域之间建立健壮的映射仍然是一个巨大的挑战，特别是对于那些具有严重视觉差异的领域。本文介绍了一种新的通用翻译框架——生成先验引导的无监督图像到图像翻译(GP-UNIT) ，它提高了现有翻译模型的质量、适用性和可控性。GP-UNIT 的核心思想是从预先训练的类条件 GAN 中提取生成先验，构建粗级跨域通信，并应用在对抗性翻译之前学到的知识挖掘细级通信。通过学习多级内容对应，GP-UNIT 能够在近域和远域之间进行有效的翻译。对于封闭领域，GP-UNIT 可以根据一个参数来确定翻译过程中内容对应的强度，使用户能够在内容和风格一致性之间取得平衡。对于遥远的领域，探索半监督学习来指导 GP-UNIT 发现准确的语义对应，这些对应很难仅仅从外观上学习。我们通过大量的实验验证了 GP-UNIT 相对于最先进的翻译模型在不同领域之间稳健、高质量和多样化的翻译方面的优越性。"
    },
    {
        "title": "Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast\n  Contrastive Fusion",
        "url": "http://arxiv.org/abs/2306.04633v1",
        "pub_date": "2023-06-07",
        "summary": "Instance segmentation in 3D is a challenging task due to the lack of\nlarge-scale annotated datasets. In this paper, we show that this task can be\naddressed effectively by leveraging instead 2D pre-trained models for instance\nsegmentation. We propose a novel approach to lift 2D segments to 3D and fuse\nthem by means of a neural field representation, which encourages multi-view\nconsistency across frames. The core of our approach is a slow-fast clustering\nobjective function, which is scalable and well-suited for scenes with a large\nnumber of objects. Unlike previous approaches, our method does not require an\nupper bound on the number of objects or object tracking across frames. To\ndemonstrate the scalability of the slow-fast clustering, we create a new\nsemi-realistic dataset called the Messy Rooms dataset, which features scenes\nwith up to 500 objects per scene. Our approach outperforms the state-of-the-art\non challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well\nas on our newly created Messy Rooms dataset, demonstrating the effectiveness\nand scalability of our slow-fast clustering method.",
        "translated": "由于缺乏大规模的注释数据集，三维实例分割是一项具有挑战性的任务。在本文中，我们表明，这项任务可以有效地解决，利用而不是2D 预训练模型的实例分割。我们提出了一种新的方法，提升二维段到三维和融合它们的手段的神经场表示，鼓励多视图一致性跨帧。我们的方法的核心是一个慢-快聚类目标函数，它是可扩展的，并且非常适合有大量对象的场景。与以前的方法不同，我们的方法不需要对对象数量或对象跨帧跟踪的上限。为了演示慢-快聚类的可伸缩性，我们创建了一个新的半现实数据集，称为 Messy Rooms 数据集，它的特点是每个场景最多有500个对象。我们的方法在具有挑战性的场景扫描网络，Hypersim 和副本数据集，以及我们新创建的混乱的房间数据集优于最先进的表现，展示了我们的缓慢快速聚类方法的有效性和可扩展性。"
    },
    {
        "title": "Designing a Better Asymmetric VQGAN for StableDiffusion",
        "url": "http://arxiv.org/abs/2306.04632v1",
        "pub_date": "2023-06-07",
        "summary": "StableDiffusion is a revolutionary text-to-image generator that is causing a\nstir in the world of image generation and editing. Unlike traditional methods\nthat learn a diffusion model in pixel space, StableDiffusion learns a diffusion\nmodel in the latent space via a VQGAN, ensuring both efficiency and quality. It\nnot only supports image generation tasks, but also enables image editing for\nreal images, such as image inpainting and local editing. However, we have\nobserved that the vanilla VQGAN used in StableDiffusion leads to significant\ninformation loss, causing distortion artifacts even in non-edited image\nregions. To this end, we propose a new asymmetric VQGAN with two simple\ndesigns. Firstly, in addition to the input from the encoder, the decoder\ncontains a conditional branch that incorporates information from task-specific\npriors, such as the unmasked image region in inpainting. Secondly, the decoder\nis much heavier than the encoder, allowing for more detailed recovery while\nonly slightly increasing the total inference cost. The training cost of our\nasymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoder\nwhile keeping the vanilla VQGAN encoder and StableDiffusion unchanged. Our\nasymmetric VQGAN can be widely used in StableDiffusion-based inpainting and\nlocal editing methods. Extensive experiments demonstrate that it can\nsignificantly improve the inpainting and editing performance, while maintaining\nthe original text-to-image capability. The code is available at\n\\url{https://github.com/buxiangzhiren/Asymmetric_VQGAN}.",
        "translated": "稳定扩散是一个革命性的文本到图像生成器，它在图像生成和编辑领域引起了轰动。不像传统的方法在像素空间学习扩散模型，稳定扩散通过 VQGAN 在潜在空间学习扩散模型，确保效率和质量。它不仅支持图像生成任务，还支持对真实图像进行图像编辑，如图像修补和局部编辑。然而，我们已经观察到，在 StableDefusion 中使用的普通 VQGAN 会导致显著的信息丢失，甚至在未经编辑的图像区域中也会造成失真。为此，我们提出了一种新的非对称 VQGAN 的两个简单的设计。首先，除了来自编码器的输入外，解码器还包含一个条件分支，该分支包含来自任务特定先验的信息，例如内绘中未掩盖的图像区域。其次，解码器比编码器重得多，允许更详细的恢复，同时只略微增加总的推理成本。我们的非对称 VQGAN 的训练成本是低廉的，我们只需要重新训练一个新的非对称解码器，同时保持普通的 VQGAN 编码器和稳定扩散不变。我们的非对称 VQGAN 可以广泛应用于基于稳定扩散的修补和局部编辑方法。大量实验表明，该算法能够显著提高修补和编辑性能，同时保持原始的文本到图像的能力。该代码可在 url { https://github.com/buxiangzhiren/asymmetric_vqgan }获得。"
    },
    {
        "title": "Yet Another Algorithm for Supervised Principal Component Analysis:\n  Supervised Linear Centroid-Encoder",
        "url": "http://arxiv.org/abs/2306.04622v1",
        "pub_date": "2023-06-07",
        "summary": "We propose a new supervised dimensionality reduction technique called\nSupervised Linear Centroid-Encoder (SLCE), a linear counterpart of the\nnonlinear Centroid-Encoder (CE) \\citep{ghosh2022supervised}. SLCE works by\nmapping the samples of a class to its class centroid using a linear\ntransformation. The transformation is a projection that reconstructs a point\nsuch that its distance from the corresponding class centroid, i.e.,\ncentroid-reconstruction loss, is minimized in the ambient space. We derive a\nclosed-form solution using an eigendecomposition of a symmetric matrix. We did\na detailed analysis and presented some crucial mathematical properties of the\nproposed approach. %We also provide an iterative solution approach based\nsolving the optimization problem using a descent method. We establish a\nconnection between the eigenvalues and the centroid-reconstruction loss. In\ncontrast to Principal Component Analysis (PCA) which reconstructs a sample in\nthe ambient space, the transformation of SLCE uses the instances of a class to\nrebuild the corresponding class centroid. Therefore the proposed method can be\nconsidered a form of supervised PCA. Experimental results show the performance\nadvantage of SLCE over other supervised methods.",
        "translated": "我们提出了一种新的监督降维技术，称为监督线性质心编码器(SLCE) ，它是非线性质心编码器(CE) citep { ghosh2022监督}的线性副本。SLCE 的工作方式是使用一个线性映射将一个类的样本映射到它的类 centroid。该变换是一种投影，它重建一个点，使其与相应类质心的距离(即质心重建损失)在环境空间中最小化。我们利用对称矩阵的特征分解得到一个封闭形式的解。我们做了一个详细的分析，并提出了一些关键的数学性质的建议的方法。% 我们还提供了一种基于迭代解法的方法，用下降法求解最佳化问题。我们建立了特征值与质心重构损失之间的联系。与在环境空间中重建样本的主成分分析(PCA)不同，SLCE 的转换使用一个类的实例来重建相应的类 centroid。因此，该方法可以看作是一种有监督的主成分分析方法。实验结果表明，SLCE 算法的性能优于其他监督方法。"
    },
    {
        "title": "Align, Distill, and Augment Everything All at Once for Imbalanced\n  Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2306.04621v1",
        "pub_date": "2023-06-07",
        "summary": "Addressing the class imbalance in long-tailed semi-supervised learning (SSL)\nposes a few significant challenges stemming from differences between the\nmarginal distributions of unlabeled data and the labeled data, as the former is\noften unknown and potentially distinct from the latter. The first challenge is\nto avoid biasing the pseudo-labels towards an incorrect distribution, such as\nthat of the labeled data or a balanced distribution, during training. However,\nwe still wish to ensure a balanced unlabeled distribution during inference,\nwhich is the second challenge. To address both of these challenges, we propose\na three-faceted solution: a flexible distribution alignment that progressively\naligns the classifier from a dynamically estimated unlabeled prior towards a\nbalanced distribution, a soft consistency regularization that exploits\nunderconfident pseudo-labels discarded by threshold-based methods, and a schema\nfor expanding the unlabeled set with input data from the labeled partition.\nThis last facet comes in as a response to the commonly-overlooked fact that\ndisjoint partitions of labeled and unlabeled data prevent the benefits of\nstrong data augmentation on the labeled set. Our overall framework requires no\nadditional training cycles, so it will align, distill, and augment everything\nall at once (ADALLO). Our extensive evaluations of ADALLO on imbalanced SSL\nbenchmark datasets, including CIFAR10-LT, CIFAR100-LT, and STL10-LT with\nvarying degrees of class imbalance, amount of labeled data, and distribution\nmismatch, demonstrate significant improvements in the performance of imbalanced\nSSL under large distribution mismatch, as well as competitiveness with\nstate-of-the-art methods when the labeled and unlabeled data follow the same\nmarginal distribution. Our code will be released upon paper acceptance.",
        "translated": "解决长尾半监督学习中的类别失衡问题，是由于未标记数据的边际分布与标记数据之间的差异而带来的一些重大挑战，因为前者往往是未知的，而且可能与后者不同。第一个挑战是在训练期间避免使伪标签偏向于不正确的分布，例如标记数据或平衡分布。然而，我们仍然希望在推理过程中确保一个平衡的无标记分布，这是第二个挑战。为了解决这两个挑战，我们提出了一个三方面的解决方案: 一个灵活的分布对齐，逐步将分类器从一个动态估计的未标记先于一个平衡的分布，一个软一致性正则化，利用基于阈值的方法丢弃的不自信的伪标签，和一个模式来扩展未标记的集从标记的分区输入数据。最后一个方面是对一个常被忽视的事实的回应，即标记数据和未标记数据的不相交分区阻碍了标记集上强大数据增强的好处。我们的总体框架不需要额外的培训周期，因此它将一次性对齐、提取和增强所有内容(ADALLO)。我们对 ADallo 在不平衡 SSL 基准数据集上的广泛评估，包括 CIFAR10-LT、 CIFAR100-LT 和 STL10-LT，这些数据集具有不同程度的类不平衡、标记数据的数量和分布不匹配，表明在大的分布不匹配情况下，不平衡 SSL 的性能有显著改善，当标记和未标记数据遵循相同的边缘分布时，与最先进的方法竞争。我们的代码将在书面验收后发布。"
    },
    {
        "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image\n  Collections",
        "url": "http://arxiv.org/abs/2306.04619v1",
        "pub_date": "2023-06-07",
        "summary": "Estimating 3D articulated shapes like animal bodies from monocular images is\ninherently challenging due to the ambiguities of camera viewpoint, pose,\ntexture, lighting, etc. We propose ARTIC3D, a self-supervised framework to\nreconstruct per-instance 3D shapes from a sparse image collection in-the-wild.\nSpecifically, ARTIC3D is built upon a skeleton-based surface representation and\nis further guided by 2D diffusion priors from Stable Diffusion. First, we\nenhance the input images with occlusions/truncation via 2D diffusion to obtain\ncleaner mask estimates and semantic features. Second, we perform\ndiffusion-guided 3D optimization to estimate shape and texture that are of\nhigh-fidelity and faithful to input images. We also propose a novel technique\nto calculate more stable image-level gradients via diffusion models compared to\nexisting alternatives. Finally, we produce realistic animations by fine-tuning\nthe rendered shape and texture under rigid part transformations. Extensive\nevaluations on multiple existing datasets as well as newly introduced noisy web\nimage collections with occlusions and truncation demonstrate that ARTIC3D\noutputs are more robust to noisy images, higher quality in terms of shape and\ntexture details, and more realistic when animated. Project page:\nhttps://chhankyao.github.io/artic3d/",
        "translated": "由于摄像机视角、姿势、纹理、光照等的模糊性，从单目图像中估计像动物身体这样的三维铰接形状本身就具有挑战性。我们提出 ARTIC3D，一个自我监督的框架来重建每个实例的三维形状从稀疏的图像收集在野外。具体来说，ARTIC3D 是建立在一个基于骨架的表面表示，并进一步指导二维扩散从稳定扩散的先决条件。首先，通过二维扩散增强遮挡/截断输入图像，获得更清晰的掩模估计和语义特征。其次，我们进行扩散引导的三维优化，以估计形状和纹理的高保真度和忠实的输入图像。我们还提出了一种新的技术，计算更稳定的图像水平梯度通过扩散模型相比，现有的替代方案。最后，我们通过在刚性部分变换下微调渲染的形状和纹理来生成逼真的动画。对多个现有数据集的广泛评估以及新引入的具有遮挡和截断的噪声网络图像集表明，ARTIC3D 输出对噪声图像更加稳健，在形状和纹理细节方面更高质量，并且在动画时更加真实。项目主页:  https://chhankyao.github.io/artic3d/"
    },
    {
        "title": "ICON$^2$: Reliably Benchmarking Predictive Inequity in Object Detection",
        "url": "http://arxiv.org/abs/2306.04482v1",
        "pub_date": "2023-06-07",
        "summary": "As computer vision systems are being increasingly deployed at scale in\nhigh-stakes applications like autonomous driving, concerns about social bias in\nthese systems are rising. Analysis of fairness in real-world vision systems,\nsuch as object detection in driving scenes, has been limited to observing\npredictive inequity across attributes such as pedestrian skin tone, and lacks a\nconsistent methodology to disentangle the role of confounding variables e.g.\ndoes my model perform worse for a certain skin tone, or are such scenes in my\ndataset more challenging due to occlusion and crowds? In this work, we\nintroduce ICON$^2$, a framework for robustly answering this question. ICON$^2$\nleverages prior knowledge on the deficiencies of object detection systems to\nidentify performance discrepancies across sub-populations, compute correlations\nbetween these potential confounders and a given sensitive attribute, and\ncontrol for the most likely confounders to obtain a more reliable estimate of\nmodel bias. Using our approach, we conduct an in-depth study on the performance\nof object detection with respect to income from the BDD100K driving dataset,\nrevealing useful insights.",
        "translated": "随着计算机视觉系统在自动驾驶等高风险应用领域的大规模应用，人们越来越担心这些系统中存在社会偏见。对现实世界视觉系统中公平性的分析，例如驾驶场景中的目标检测，仅限于观察行人肤色等属性的预测不公平性，并且缺乏一个一致的方法来解决混杂变量的作用，例如，我的模型在某种肤色下表现更差，还是由于遮挡和拥挤，这些场景在我的数据集中更具挑战性？在这项工作中，我们介绍了图标 $^ 2 $，一个框架，以稳健地回答这个问题。ICON $^ 2 $利用对目标检测系统缺陷的先前知识来识别不同亚群之间的性能差异，计算这些潜在混杂因素与给定敏感属性之间的相关性，并控制最可能的混杂因素以获得更可靠的模型偏差估计。利用我们的方法，我们进行了一个深入的研究表现的目标检测与收入方面的 BDD100k 驾驶数据集，揭示了有用的见解。"
    },
    {
        "title": "Integrating Geometric Control into Text-to-Image Diffusion Models for\n  High-Quality Detection Data Generation via Text Prompt",
        "url": "http://arxiv.org/abs/2306.04607v2",
        "pub_date": "2023-06-07",
        "summary": "Diffusion models have attracted significant attention due to their remarkable\nability to create content and generate data for tasks such as image\nclassification. However, the usage of diffusion models to generate high-quality\nobject detection data remains an underexplored area, where not only the\nimage-level perceptual quality but also geometric conditions such as bounding\nboxes and camera views are essential. Previous studies have utilized either\ncopy-paste synthesis or layout-to-image (L2I) generation with specifically\ndesigned modules to encode semantic layouts. In this paper, we propose\nGeoDiffusion, a simple framework that can flexibly translate various geometric\nconditions into text prompts and empower the pre-trained text-to-image (T2I)\ndiffusion models for high-quality detection data generation. Unlike previous\nL2I methods, our GeoDiffusion is able to encode not only bounding boxes but\nalso extra geometric conditions such as camera views in self-driving scenes.\nExtensive experiments demonstrate GeoDiffusion outperforms previous L2I methods\nwhile maintaining 4x training time faster. To the best of our knowledge, this\nis the first work to adopt diffusion models for layout-to-image generation with\ngeometric conditions and demonstrate that L2I-generated images can be\nbeneficial for improving the performance of object detectors.",
        "translated": "扩散模型由于具有为图像分类等任务创建内容和生成数据的显著能力而引起了人们的高度重视。然而，使用扩散模型来生成高质量的目标检测数据仍然是一个探索不足的领域，不仅图像水平的感知质量，而且几何条件，如边界框和相机视图都是必不可少的。先前的研究已经利用复制粘贴合成或布局到图像(L2I)与专门设计的模块生成来编码语义布局。本文提出了一个简单的 GeoDefusion 框架，该框架可以灵活地将各种几何条件转换为文本提示，并赋予预先训练的文本到图像(T2I)扩散模型以高质量的检测数据生成能力。与以前的 L2I 方法不同，我们的 GeoDefusion 不仅能够编码边界框，而且还能够编码额外的几何条件，例如自动驾驶场景中的摄像机视图。大量的实验表明，GeoDiffusion 比以前的 L2I 方法更好，同时保持4倍的训练时间更快。据我们所知，这是第一个采用扩散模型的布局图像生成的几何条件，并证明了 L2I 生成的图像可以有利于提高性能的目标检测器。"
    },
    {
        "title": "MarineVRS: Marine Video Retrieval System with Explainability via\n  Semantic Understanding",
        "url": "http://arxiv.org/abs/2306.04593v1",
        "pub_date": "2023-06-07",
        "summary": "Building a video retrieval system that is robust and reliable, especially for\nthe marine environment, is a challenging task due to several factors such as\ndealing with massive amounts of dense and repetitive data, occlusion,\nblurriness, low lighting conditions, and abstract queries. To address these\nchallenges, we present MarineVRS, a novel and flexible video retrieval system\ndesigned explicitly for the marine domain. MarineVRS integrates\nstate-of-the-art methods for visual and linguistic object representation to\nenable efficient and accurate search and analysis of vast volumes of underwater\nvideo data. In addition, unlike the conventional video retrieval system, which\nonly permits users to index a collection of images or videos and search using a\nfree-form natural language sentence, our retrieval system includes an\nadditional Explainability module that outputs the segmentation masks of the\nobjects that the input query referred to. This feature allows users to identify\nand isolate specific objects in the video footage, leading to more detailed\nanalysis and understanding of their behavior and movements. Finally, with its\nadaptability, explainability, accuracy, and scalability, MarineVRS is a\npowerful tool for marine researchers and scientists to efficiently and\naccurately process vast amounts of data and gain deeper insights into the\nbehavior and movements of marine species.",
        "translated": "建立一个健壮可靠的视频检索系统，特别是对于海洋环境来说，是一个具有挑战性的任务，因为有几个因素，如处理大量密集和重复的数据，遮挡，模糊，低照明条件和抽象查询。为了应对这些挑战，我们提出了 MarineVRS，一个新颖的和灵活的视频检索系统，明确地为海洋领域设计。MarineVRS 集成了最先进的视觉和语言对象表示方法，能够高效、准确地搜索和分析海量水下视频数据。此外，与传统的视频检索系统不同，传统的视频检索系统只允许用户索引一组图像或视频并使用自由格式的自然语言句子进行搜索，我们的检索系统包括一个额外的可解释性模块，该模块输出输入查询引用的对象的分割掩码。这个功能允许用户识别和隔离视频画面中的特定物体，从而对它们的行为和动作进行更详细的分析和理解。最后，凭借其适应性、可解释性、准确性和可扩展性，MarineVRS 是海洋研究人员和科学家有效和准确地处理大量数据并获得对海洋物种行为和运动的更深刻见解的强大工具。"
    },
    {
        "title": "A Dataset for Deep Learning-based Bone Structure Analyses in Total Hip\n  Arthroplasty",
        "url": "http://arxiv.org/abs/2306.04579v1",
        "pub_date": "2023-06-07",
        "summary": "Total hip arthroplasty (THA) is a widely used surgical procedure in\northopedics. For THA, it is of clinical significance to analyze the bone\nstructure from the CT images, especially to observe the structure of the\nacetabulum and femoral head, before the surgical procedure. For such bone\nstructure analyses, deep learning technologies are promising but require\nhigh-quality labeled data for the learning, while the data labeling is costly.\nWe address this issue and propose an efficient data annotation pipeline for\nproducing a deep learning-oriented dataset. Our pipeline consists of\nnon-learning-based bone extraction (BE) and acetabulum and femoral head\nsegmentation (AFS) and active-learning-based annotation refinement (AAR). For\nBE we use the classic graph-cut algorithm. For AFS we propose an improved\nalgorithm, including femoral head boundary localization using first-order and\nsecond-order gradient regularization, line-based non-maximum suppression, and\nanatomy prior-based femoral head extraction. For AAR, we refine the\nalgorithm-produced pseudo labels with the help of trained deep models: we\nmeasure the uncertainty based on the disagreement between the original pseudo\nlabels and the deep model predictions, and then find out the samples with the\nlargest uncertainty to ask for manual labeling. Using the proposed pipeline, we\nconstruct a large-scale bone structure analyses dataset from more than 300\nclinical and diverse CT scans. We perform careful manual labeling for the test\nset of our data. We then benchmark multiple state-of-the art deep\nlearning-based methods of medical image segmentation using the training and\ntest sets of our data. The extensive experimental results validate the efficacy\nof the proposed data annotation pipeline. The dataset, related codes and models\nwill be publicly available at https://github.com/hitachinsk/THA.",
        "translated": "全髋关节置换术(THA)是一种广泛应用于骨科的外科手术。对于全髋关节置换术来说，术前从 CT 图像上分析骨结构，特别是观察髋臼和股骨头的结构具有重要的临床意义。对于这样的骨结构分析，深度学习技术是有前途的，但需要高质量的标记数据进行学习，而数据标记是昂贵的。我们解决了这个问题，并提出了一个有效的数据注释流水线，以产生一个面向深度学习的数据集。我们的流水线包括基于非学习的骨提取(BE)和髋臼和股骨头分割(AFS)以及基于主动学习的注释细化(AAR)。对于 BE，我们使用经典的图割算法。对于 AFS，我们提出了一种改进的算法，包括使用一阶和二阶梯度正则化的股骨头边界定位、基于线的非最大抑制和基于解剖学先验的股骨头提取。对于 AAR，我们利用训练好的深度模型对算法生成的伪标签进行改进: 我们根据原始伪标签与深度模型预测的不一致性来测量不确定性，然后找出不确定性最大的样本进行人工标签。使用该流水线，我们构建了一个来自300多个临床和不同 CT 扫描的大规模骨结构分析数据集。我们对数据的测试集执行仔细的手动标记。然后，我们利用数据的训练和测试集，对基于深度学习的多种医学图像分割方法进行评估。广泛的实验结果验证了所提出的数据注释流水线的有效性。数据集、相关代码和模型将在 https://github.com/hitachinsk/tha 公开发布。"
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "url": "http://arxiv.org/abs/2306.05427v1",
        "pub_date": "2023-06-08",
        "summary": "Driven by scalable diffusion models trained on large-scale paired text-image\ndatasets, text-to-image synthesis methods have shown compelling results.\nHowever, these models still fail to precisely follow the text prompt when\nmultiple objects, attributes, and spatial compositions are involved in the\nprompt. In this paper, we identify the potential reasons in both the\ncross-attention and self-attention layers of the diffusion model. We propose\ntwo novel losses to refocus the attention maps according to a given layout\nduring the sampling process. We perform comprehensive experiments on the\nDrawBench and HRS benchmarks using layouts synthesized by Large Language\nModels, showing that our proposed losses can be integrated easily and\neffectively into existing text-to-image methods and consistently improve their\nalignment between the generated images and the text prompts.",
        "translated": "在大规模成对文本-图像数据集上训练的可扩展扩散模型的驱动下，文本-图像合成方法已经显示出引人注目的结果。但是，当提示包含多个对象、属性和空间组合时，这些模型仍然不能精确地跟随文本提示。本文从扩散模型的交叉注意层和自我注意层两个方面分析了影响扩散模型的潜在原因。我们提出了两个新的损失重新聚焦的注意图根据给定的布局在采样过程中。我们在 DrawBench 和 HRS 基准上进行了全面的实验，使用了大型语言模型合成的布局，表明我们提出的损失可以很容易和有效地整合到现有的文本到图像的方法中，并持续改善生成的图像和文本提示符之间的对齐。"
    },
    {
        "title": "Background Prompting for Improved Object Depth",
        "url": "http://arxiv.org/abs/2306.05428v1",
        "pub_date": "2023-06-08",
        "summary": "Estimating the depth of objects from a single image is a valuable task for\nmany vision, robotics, and graphics applications. However, current methods\noften fail to produce accurate depth for objects in diverse scenes. In this\nwork, we propose a simple yet effective Background Prompting strategy that\nadapts the input object image with a learned background. We learn the\nbackground prompts only using small-scale synthetic object datasets. To infer\nobject depth on a real image, we place the segmented object into the learned\nbackground prompt and run off-the-shelf depth networks. Background Prompting\nhelps the depth networks focus on the foreground object, as they are made\ninvariant to background variations. Moreover, Background Prompting minimizes\nthe domain gap between synthetic and real object images, leading to better\nsim2real generalization than simple finetuning. Results on multiple synthetic\nand real datasets demonstrate consistent improvements in real object depths for\na variety of existing depth networks. Code and optimized background prompts can\nbe found at: https://mbaradad.github.io/depth_prompt.",
        "translated": "对于许多视觉、机器人和图形应用程序来说，从单幅图像估计物体的深度是一项非常有价值的任务。然而，目前的方法往往不能产生准确的深度对物体在不同的场景。在这项工作中，我们提出了一个简单而有效的背景提示策略，适应输入的物体图像与学习背景。我们只使用小规模的合成对象数据集来学习背景提示。为了在真实图像上推断目标深度，我们将分割后的目标放入学习后的背景提示中，运行现成的深度网络。背景提示有助于深度网络聚焦于前景物体，因为它们对背景变化具有不变性。此外，背景提示最小化了合成和真实物体图像之间的域间隔，从而比简单的微调更好地实现了模拟和真实图像的泛化。对多个合成和真实数据集的结果表明，对于现有的各种深度网络，在实际目标深度方面有一致的改进。代码和优化后的背景提示可以在以下 https://mbaradad.github.io/depth_prompt 找到:。"
    },
    {
        "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and\n  Language Models",
        "url": "http://arxiv.org/abs/2306.05424v1",
        "pub_date": "2023-06-08",
        "summary": "Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the underexplored\nfield of video-based conversation by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with a LLM. The\nmodel is capable of understanding and generating human-like conversations about\nvideos. We introduce a new dataset of 100,000 video-instruction pairs used to\ntrain Video-ChatGPT acquired via manual and semi-automated pipeline that is\neasily scalable and robust to label noise. We also develop a quantiative\nevaluation framework for video-based dialogue models to objectively analyse the\nstrengths and weaknesses of proposed models. Our code, models, instruction-sets\nand demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "translated": "由大型语言模型(LLM)驱动的会话代理提供了一种与可视化数据交互的新方法。虽然基于图像的会话模型已经有了初步的尝试，但是本文通过引入 Video-ChatGPT 解决了基于视频的会话中未被充分发掘的领域。它是一个多模态模型，融合了视频适应的视觉编码器和 LLM。该模型能够理解和生成关于视频的类人对话。我们引入了一个新的数据集100,000视频指令对用于训练视频聊天 GPT 获得通过手动和半自动管道，这是很容易扩展和鲁棒的标签噪声。我们还开发了一个基于视频对话模型的定量评估框架，以客观地分析所提出模型的优缺点。我们的代码、模型、指令集和演示 https://github.com/mbzuai-oryx/video-chatgpt 全部发布。"
    },
    {
        "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
        "url": "http://arxiv.org/abs/2306.05425v1",
        "pub_date": "2023-06-08",
        "summary": "High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.",
        "translated": "高质量的指令和响应对于大型语言模型在交互式自然语言任务中的零点性能至关重要。对于涉及复杂视觉场景的交互式视觉语言任务，必须调优视觉语言模型(VLM)。然而，目前视觉-语言教学-反应对在数量、多样性和创造性方面的可用性仍然有限，对交互式 VLM 的普及提出了挑战。在这里，我们介绍了多模态上下文指令调优(MIMIC-IT) ，一个包含280万个多模态指令-响应对的数据集，其中有220万个来自图像和视频的独特指令。每一对都伴随着多模态的语境信息，形成旨在赋予 VLM 感知、推理和计划能力的会话语境。这个被称为 Syphus 的指令-响应收集过程使用一个自动注释管道进行扩展，该管道将人类的专业知识与 GPT 的功能结合在一起。使用 MIMIC-IT 数据集，我们训练了一个名为 Otter 的大型 VLM。基于对视觉语言基准的广泛评估，我们发现 Otter 在多模态知觉、推理和语境学习方面表现出显著的能力。人工评估显示它有效地与用户的意图保持一致。我们发布 MIMIC-IT 数据集、指令-响应收集管道、基准测试和 Otter 模型。"
    },
    {
        "title": "ADDP: Learning General Representations for Image Recognition and\n  Generation with Alternating Denoising Diffusion Process",
        "url": "http://arxiv.org/abs/2306.05423v1",
        "pub_date": "2023-06-08",
        "summary": "Image recognition and generation have long been developed independently of\neach other. With the recent trend towards general-purpose representation\nlearning, the development of general representations for both recognition and\ngeneration tasks is also promoted. However, preliminary attempts mainly focus\non generation performance, but are still inferior on recognition tasks. These\nmethods are modeled in the vector-quantized (VQ) space, whereas leading\nrecognition methods use pixels as inputs. Our key insights are twofold: (1)\npixels as inputs are crucial for recognition tasks; (2) VQ tokens as\nreconstruction targets are beneficial for generation tasks. These observations\nmotivate us to propose an Alternating Denoising Diffusion Process (ADDP) that\nintegrates these two spaces within a single representation learning framework.\nIn each denoising step, our method first decodes pixels from previous VQ\ntokens, then generates new VQ tokens from the decoded pixels. The diffusion\nprocess gradually masks out a portion of VQ tokens to construct the training\nsamples. The learned representations can be used to generate diverse\nhigh-fidelity images and also demonstrate excellent transfer performance on\nrecognition tasks. Extensive experiments show that our method achieves\ncompetitive performance on unconditional generation, ImageNet classification,\nCOCO detection, and ADE20k segmentation. Importantly, our method represents the\nfirst successful development of general representations applicable to both\ngeneration and dense recognition tasks. Code shall be released.",
        "translated": "图像识别和生成技术一直是相互独立发展的。随着通用表征学习的发展，识别任务和生成任务的通用表征也得到了进一步的发展。然而，初步的尝试主要集中在生成性能上，但在识别任务上仍然较差。这些方法建模在矢量量化(VQ)空间，而领先的识别方法使用像素作为输入。我们的主要见解有两个方面: (1)像素作为输入对识别任务是至关重要的; (2) VQ 标记作为重构目标对生成任务是有益的。这些观察促使我们提出一种交替去噪扩散过程(ADDP) ，它将这两个空间集成在一个单一的表示学习框架中。在每一个去噪步骤中，我们的方法首先从先前的 VQ 标记中解码像素，然后从解码的像素中生成新的 VQ 标记。扩散过程逐渐掩盖一部分 VQ 标记来构造训练样本。学习表征可以用来生成不同的高保真图像，并且在识别任务中表现出优异的传递性能。大量实验表明，该方法在无条件生成、 ImageNet 分类、 COCO 检测和 ADE20k 分割等方面具有较好的性能。重要的是，我们的方法首次成功地开发了适用于生成和密集识别任务的通用表示。密码将被公布。"
    },
    {
        "title": "Tracking Everything Everywhere All at Once",
        "url": "http://arxiv.org/abs/2306.05422v1",
        "pub_date": "2023-06-08",
        "summary": "We present a new test-time optimization method for estimating dense and\nlong-range motion from a video sequence. Prior optical flow or particle video\ntracking algorithms typically operate within limited temporal windows,\nstruggling to track through occlusions and maintain global consistency of\nestimated motion trajectories. We propose a complete and globally consistent\nmotion representation, dubbed OmniMotion, that allows for accurate, full-length\nmotion estimation of every pixel in a video. OmniMotion represents a video\nusing a quasi-3D canonical volume and performs pixel-wise tracking via\nbijections between local and canonical space. This representation allows us to\nensure global consistency, track through occlusions, and model any combination\nof camera and object motion. Extensive evaluations on the TAP-Vid benchmark and\nreal-world footage show that our approach outperforms prior state-of-the-art\nmethods by a large margin both quantitatively and qualitatively. See our\nproject page for more results: http://omnimotion.github.io/",
        "translated": "我们提出了一种新的测试时间优化方法来估计密集和远程运动从一个视频序列。先前的光流或粒子视频跟踪算法通常在有限的时间窗内运行，努力跟踪通过遮挡和维持估计的运动轨迹的全局一致性。我们提出了一个完整的和全球一致的运动表示，称为 OmniMotion，允许准确，全长运动估计每个像素的视频。OmniMotion 使用准3D 规范体积表示视频，并通过局部和规范空间之间的双向投影执行像素级跟踪。这种表示使我们能够确保全局一致性，跟踪通过遮挡，并建模任何组合的摄像机和对象的运动。对 TAP-Vid 基准和现实世界连续镜头的广泛评估表明，我们的方法在数量和质量上都大大优于先进的方法。更多结果请参见我们的项目页面:  http://omnimotion.github.io/"
    },
    {
        "title": "Stochastic Multi-Person 3D Motion Forecasting",
        "url": "http://arxiv.org/abs/2306.05421v1",
        "pub_date": "2023-06-08",
        "summary": "This paper aims to deal with the ignored real-world complexities in prior\nwork on human motion forecasting, emphasizing the social properties of\nmulti-person motion, the diversity of motion and social interactions, and the\ncomplexity of articulated motion. To this end, we introduce a novel task of\nstochastic multi-person 3D motion forecasting. We propose a dual-level\ngenerative modeling framework that separately models independent individual\nmotion at the local level and social interactions at the global level. Notably,\nthis dual-level modeling mechanism can be achieved within a shared generative\nmodel, through introducing learnable latent codes that represent intents of\nfuture motion and switching the codes' modes of operation at different levels.\nOur framework is general; we instantiate it with different generative models,\nincluding generative adversarial networks and diffusion models, and various\nmulti-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,\nand SoMoF benchmarks show that our approach produces diverse and accurate\nmulti-person predictions, significantly outperforming the state of the art.",
        "translated": "本文针对以往人体运动预测工作中所忽视的现实世界的复杂性，强调了多人运动的社会属性、运动和社会交互的多样性以及关节运动的复杂性。为此，我们提出了一个新颖的任务随机多人三维运动预测。我们提出了一个双层生成建模框架，分别在局部层面上模拟独立的个体运动和在全球层面上模拟社会互动。值得注意的是，这种双层建模机制可以通过引入代表未来运动意图的可学习的潜在代码，以及在不同层次上切换代码的操作模式，在一个共享的生成模型内实现。我们的框架是通用的; 我们用不同的生成模型来实例化它，包括生成对抗网络和扩散模型，以及各种多人预测模型。在 CMU-Mocap、 MuPoTS-3D 和 SoMoF 基准上进行的大量实验表明，我们的方法能够产生多样化和准确的多人预测，明显优于最先进的预测。"
    },
    {
        "title": "Scaling Spherical CNNs",
        "url": "http://arxiv.org/abs/2306.05420v1",
        "pub_date": "2023-06-08",
        "summary": "Spherical CNNs generalize CNNs to functions on the sphere, by using spherical\nconvolutions as the main linear operation. The most accurate and efficient way\nto compute spherical convolutions is in the spectral domain (via the\nconvolution theorem), which is still costlier than the usual planar\nconvolutions. For this reason, applications of spherical CNNs have so far been\nlimited to small problems that can be approached with low model capacity. In\nthis work, we show how spherical CNNs can be scaled for much larger problems.\nTo achieve this, we make critical improvements including novel variants of\ncommon model components, an implementation of core operations to exploit\nhardware accelerator characteristics, and application-specific input\nrepresentations that exploit the properties of our model. Experiments show our\nlarger spherical CNNs reach state-of-the-art on several targets of the QM9\nmolecular benchmark, which was previously dominated by equivariant graph neural\nnetworks, and achieve competitive performance on multiple weather forecasting\ntasks. Our code is available at\nhttps://github.com/google-research/spherical-cnn.",
        "translated": "球面 CNN 通过球面卷积作为主要的线性运算，将 CNN 推广到球面上的函数。计算球面卷积最精确、最有效的方法是在谱域(通过卷积定理) ，这仍然比通常的平面卷积要昂贵。由于这个原因，迄今为止，球形 CNN 的应用仅限于可以用低模型容量来处理的小问题。在这项工作中，我们展示了如何球面 CNN 可以规模更大的问题。为了实现这一点，我们进行了关键的改进，包括通用模型组件的新变体、利用硬件加速器特性的核心操作的实现，以及利用模型特性的特定于应用程序的输入表示。实验表明，我们的大型球形神经网络在几个 QM9分子基准指标上达到了最先进的水平，以前主要由等变图形神经网络控制，并在多个天气预报任务上达到了具有竞争力的性能。我们的代码可以在 https://github.com/google-research/spherical-cnn 找到。"
    },
    {
        "title": "2D Supervised Monocular 3D Object Detection by Global-to-Local 3D\n  Reconstruction",
        "url": "http://arxiv.org/abs/2306.05418v1",
        "pub_date": "2023-06-08",
        "summary": "With the advent of the big model era, the demand for data has become more\nimportant. Especially in monocular 3D object detection, expensive manual\nannotations potentially limit further developments. Existing works have\ninvestigated weakly supervised algorithms with the help of LiDAR modality to\ngenerate 3D pseudo labels, which cannot be applied to ordinary videos. In this\npaper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea\nof global-to-local 3D reconstruction for 2D supervised monocular 3D object\ndetection. Specifically, we recover 3D structures from monocular videos by\nscene-level global reconstruction with global bundle adjustment (BA) and obtain\nobject clusters by the DoubleClustering algorithm. Learning from completely\nreconstructed objects in global BA, GBA-Learner predicts pseudo labels for\noccluded objects. Finally, we train an LBA-Learner with object-centric local BA\nto generalize the generated 3D pseudo labels to moving objects. Experiments on\nthe large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is\non par with the fully-supervised BA-Det trained with 10% videos and even\noutperforms some pioneer fully-supervised methods. We also show the great\npotential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.\nThe code will be made available. Project page: https://ba2det.site .",
        "translated": "随着大模型时代的到来，对数据的需求变得越来越重要。特别是在单目3D 目标检测中，昂贵的手动注释可能会限制进一步的开发。现有的研究都是利用激光雷达模态对弱监督算法进行研究，以生成不能应用于普通视频的三维伪标签。在本文中，我们提出了一个新的范例，称为 BA $^ 2 $- Det，利用全局到局部的二维监督单目三维目标检测的三维重建思想。具体来说，我们使用全局光束法平差(BA)进行场景级别的全局重建，从单目视频中恢复出三维结构，并使用双重聚类算法获得目标聚类。GBA-Learner 从全局 BA 的完全重构对象中学习，预测被遮挡对象的伪标签。最后，我们训练一个以对象为中心的 LBA 学习者，将生成的三维伪标签推广到移动对象。在大规模 Waymo Open Dataset 上的实验表明，BA $^ 2 $- Det 的性能与10% 视频训练的全监督 BA-Det 相当，甚至优于一些先进的全监督方法。我们还展示了 BA $^ 2 $- Det 在检测复杂场景中开放集合的3D 对象方面的巨大潜力。代码将可用。项目主页:  https://ba2det.site。"
    },
    {
        "title": "TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem\n  via Transformer-Based Architecture",
        "url": "http://arxiv.org/abs/2306.05419v1",
        "pub_date": "2023-06-08",
        "summary": "Driving scene understanding task involves detecting static elements such as\nlanes, traffic signs, and traffic lights, and their relationships with each\nother. To facilitate the development of comprehensive scene understanding\nsolutions using multiple camera views, a new dataset called Road Genome\n(OpenLane-V2) has been released. This dataset allows for the exploration of\ncomplex road connections and situations where lane markings may be absent.\nInstead of using traditional lane markings, the lanes in this dataset are\nrepresented by centerlines, which offer a more suitable representation of lanes\nand their connections. In this study, we have introduced a new approach called\nTopoMask for predicting centerlines in road topology. Unlike existing\napproaches in the literature that rely on keypoints or parametric methods,\nTopoMask utilizes an instance-mask based formulation with a transformer-based\narchitecture and, in order to enrich the mask instances with flow information,\na direction label representation is proposed. TopoMask have ranked 4th in the\nOpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction\nin OpenLane Topology Challenge 2023. In comparison to the current\nstate-of-the-art method, TopoNet, the proposed method has achieved similar\nperformance in Frechet-based lane detection and outperformed TopoNet in\nChamfer-based lane detection without utilizing its scene graph neural network.",
        "translated": "驾驶场景理解任务包括检测车道、交通标志、交通灯等静态要素及其相互关系。为了便于开发全面的场景理解解决方案使用多摄像机视图，一个新的数据集称为道路基因组(OpenLane-V2)已经发布。这个数据集允许探索复杂的道路连接和车道标记可能不存在的情况。该数据集中的车道由中心线表示，而不是使用传统的车道标记，中心线为车道及其连接提供了更合适的表示。在这项研究中，我们介绍了一种新的方法，称为拓扑掩模预测中心线的道路拓扑。与文献中现有的依赖关键点或参数方法的方法不同，TopoMask 利用基于实例掩模的公式和基于变压器的体系结构，为了用流量信息丰富掩模实例，提出了一种方向标签表示。TopoMask 在 OpenLane-V2得分(OLS)中排名第4，在 OpenLane 拓扑挑战赛2023的中心线预测 F1得分中排名第2。与目前最先进的 TopoNet 方法相比，该方法在基于 Frechet 的车道检测方面取得了相似的性能，在不使用场景图神经网络的情况下，在基于倒角的车道检测方面优于 TopoNet。"
    },
    {
        "title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding",
        "url": "http://arxiv.org/abs/2306.06094v1",
        "pub_date": "2023-06-09",
        "summary": "Recently, large language models (LLMs) have made significant advancements in\nnatural language understanding and generation. However, their potential in\ncomputer vision remains largely unexplored. In this paper, we introduce a new,\nexploratory approach that enables LLMs to process images using the Scalable\nVector Graphics (SVG) format. By leveraging the XML-based textual descriptions\nof SVG representations instead of raster images, we aim to bridge the gap\nbetween the visual and textual modalities, allowing LLMs to directly understand\nand manipulate images without the need for parameterized visual components. Our\nmethod facilitates simple image classification, generation, and in-context\nlearning using only LLM capabilities. We demonstrate the promise of our\napproach across discriminative and generative tasks, highlighting its (i)\nrobustness against distribution shift, (ii) substantial improvements achieved\nby tapping into the in-context learning abilities of LLMs, and (iii) image\nunderstanding and generation capabilities with human guidance. Our code, data,\nand models can be found here https://github.com/mu-cai/svg-llm.",
        "translated": "近年来，大型语言模型(LLM)在自然语言理解和生成方面取得了显著的进展。然而，它们在计算机视觉方面的潜力在很大程度上仍未得到开发。在这篇文章中，我们介绍了一种新的探索性的方法，使 LLM 能够使用可缩放向量图形(SVG)格式来处理图像。通过利用基于 XML 的 SVG 表示的文本描述而不是栅格图像，我们的目标是弥合视觉和文本模式之间的差距，允许 LLM 直接理解和操作图像，而不需要参数化的视觉组件。我们的方法有助于简单的图像分类，生成，并在上下文学习使用 LLM 的能力。我们展示了我们在歧视性和生成性任务中的方法的前景，强调了其(i)对分布转移的稳健性，(ii)通过利用 LLM 的上下文学习能力实现的实质性改进，以及(iii)图像理解和生成能力与人类指导。我们的代码、数据和模型可以在这里找到 https://github.com/mu-cai/svg-llm。"
    },
    {
        "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
        "url": "http://arxiv.org/abs/2306.06093v1",
        "pub_date": "2023-06-09",
        "summary": "Neural Radiance Fields (NeRF) have become an increasingly popular\nrepresentation to capture high-quality appearance and shape of scenes and\nobjects. However, learning generalizable NeRF priors over categories of scenes\nor objects has been challenging due to the high dimensionality of network\nweight space. To address the limitations of existing work on generalization,\nmulti-view consistency and to improve quality, we propose HyP-NeRF, a latent\nconditioning method for learning generalizable category-level NeRF priors using\nhypernetworks. Rather than using hypernetworks to estimate only the weights of\na NeRF, we estimate both the weights and the multi-resolution hash encodings\nresulting in significant quality gains. To improve quality even further, we\nincorporate a denoise and finetune strategy that denoises images rendered from\nNeRFs estimated by the hypernetwork and finetunes it while retaining multiview\nconsistency. These improvements enable us to use HyP-NeRF as a generalizable\nprior for multiple downstream tasks including NeRF reconstruction from\nsingle-view or cluttered scenes and text-to-NeRF. We provide qualitative\ncomparisons and evaluate HyP-NeRF on three tasks: generalization, compression,\nand retrieval, demonstrating our state-of-the-art results.",
        "translated": "神经辐射场(NeRF)已成为一种越来越流行的表示，以捕捉高品质的外观和形状的场景和对象。然而，由于网络权重空间的高维性，在场景或对象类别上学习可推广的 NRF 先验一直是一个挑战。为了解决现有工作在泛化、多视图一致性和提高质量方面的局限性，我们提出了 HyP-NeRF 方法，这是一种利用超网络学习可泛化类别级 NeRF 先验的潜在条件方法。与使用超网络估计 NERF 的权重不同，我们同时估计权重和多分辨率哈希编码，从而获得显著的质量增益。为了进一步提高图像的质量，我们引入了一种去噪和微调策略，该策略对由超网络估计的 NERF 渲染的图像进行去噪，并对其进行微调，同时保持多视图的一致性。这些改进使我们能够使用 HyP-NeRF 作为多个下游任务(包括从单视图或混乱场景的 NERF 重建和文本到 NERF)之前的推广。我们提供定性比较和评估 HyP-NeRF 的三个任务: 概括，压缩和检索，展示了我们的最先进的结果。"
    },
    {
        "title": "Realistic Saliency Guided Image Enhancement",
        "url": "http://arxiv.org/abs/2306.06092v1",
        "pub_date": "2023-06-09",
        "summary": "Common editing operations performed by professional photographers include the\ncleanup operations: de-emphasizing distracting elements and enhancing subjects.\nThese edits are challenging, requiring a delicate balance between manipulating\nthe viewer's attention while maintaining photo realism. While recent approaches\ncan boast successful examples of attention attenuation or amplification, most\nof them also suffer from frequent unrealistic edits. We propose a realism loss\nfor saliency-guided image enhancement to maintain high realism across varying\nimage types, while attenuating distractors and amplifying objects of interest.\nEvaluations with professional photographers confirm that we achieve the dual\nobjective of realism and effectiveness, and outperform the recent approaches on\ntheir own datasets, while requiring a smaller memory footprint and runtime. We\nthus offer a viable solution for automating image enhancement and photo cleanup\noperations.",
        "translated": "专业摄影师通常进行的编辑操作包括清理操作: 去强调分散注意力的元素和强化主题。这些编辑是具有挑战性的，需要在操纵观众注意力和保持照片真实感之间做出人海万花筒(电影)。虽然最近的方法可以吹嘘注意力衰减或放大的成功例子，但它们中的大多数也经常遭受不切实际的编辑。我们提出了一种显著性引导的图像增强的现实主义损失，以保持高真实度跨不同的图像类型，同时衰减干扰物和放大对象的兴趣。与专业摄影师的评估证实，我们实现了现实主义和有效性的双重目标，并优于最近的方法对自己的数据集，同时需要较小的内存占用和运行时间。因此，我们提供了一个可行的解决方案，自动图像增强和照片清理操作。"
    },
    {
        "title": "Computational Flash Photography through Intrinsics",
        "url": "http://arxiv.org/abs/2306.06089v1",
        "pub_date": "2023-06-09",
        "summary": "Flash is an essential tool as it often serves as the sole controllable light\nsource in everyday photography. However, the use of flash is a binary decision\nat the time a photograph is captured with limited control over its\ncharacteristics such as strength or color. In this work, we study the\ncomputational control of the flash light in photographs taken with or without\nflash. We present a physically motivated intrinsic formulation for flash\nphotograph formation and develop flash decomposition and generation methods for\nflash and no-flash photographs, respectively. We demonstrate that our intrinsic\nformulation outperforms alternatives in the literature and allows us to\ncomputationally control flash in in-the-wild images.",
        "translated": "闪光灯是一个必不可少的工具，因为它往往作为唯一的可控光源在日常摄影。然而，使用闪光灯是一个二元决定时，一张照片是捕捉有限的控制其特点，如强度或颜色。在这项工作中，我们研究计算控制闪光灯在照片中拍摄与否闪光灯。我们提出了一个物理动机的闪光照片形成的内在公式和发展闪光分解和闪光照片的生成方法和无闪光照片分别。我们证明，我们的内在公式优于文献中的替代品，并允许我们计算控制在野外的图像闪光灯。"
    },
    {
        "title": "SENS: Sketch-based Implicit Neural Shape Modeling",
        "url": "http://arxiv.org/abs/2306.06088v1",
        "pub_date": "2023-06-09",
        "summary": "We present SENS, a novel method for generating and editing 3D models from\nhand-drawn sketches, including those of an abstract nature. Our method allows\nusers to quickly and easily sketch a shape, and then maps the sketch into the\nlatent space of a part-aware neural implicit shape architecture. SENS analyzes\nthe sketch and encodes its parts into ViT patch encoding, then feeds them into\na transformer decoder that converts them to shape embeddings, suitable for\nediting 3D neural implicit shapes. SENS not only provides intuitive\nsketch-based generation and editing, but also excels in capturing the intent of\nthe user's sketch to generate a variety of novel and expressive 3D shapes, even\nfrom abstract sketches. We demonstrate the effectiveness of our model compared\nto the state-of-the-art using objective metric evaluation criteria and a\ndecisive user study, both indicating strong performance on sketches with a\nmedium level of abstraction. Furthermore, we showcase its intuitive\nsketch-based shape editing capabilities.",
        "translated": "我们提出了 SENS，一种新的方法生成和编辑三维模型从手绘草图，包括那些抽象的性质。该方法允许用户快速、简单地绘制形状草图，然后将草图映射到部分感知的神经隐式形状体系结构的潜在空间中。SENS 对草图进行分析，并将其部分编码为 ViT 补丁编码，然后将其输入变压器解码器，再将其转换为形状嵌入，适用于编辑三维神经隐式形状。SENS 不仅提供直观的基于草图的生成和编辑，而且擅长捕捉用户草图的意图，以生成各种新颖和富有表现力的3D 形状，甚至从抽象的草图。我们证明了我们的模型的有效性相比，国家的最先进的使用客观的度量评价标准和决定性的用户研究，两者都表明强大的表现与中等抽象水平的草图。此外，我们展示了其直观的基于草图的形状编辑能力。"
    },
    {
        "title": "Exploring the Impact of Image Resolution on Chest X-ray Classification\n  Performance",
        "url": "http://arxiv.org/abs/2306.06051v1",
        "pub_date": "2023-06-09",
        "summary": "Deep learning models for image classification have often used a resolution of\n$224\\times224$ pixels for computational reasons.\n  This study investigates the effect of image resolution on chest X-ray\nclassification performance, using the ChestX-ray14 dataset.\n  The results show that a higher image resolution, specifically\n$1024\\times1024$ pixels, has the best overall classification performance, with\na slight decline in performance between $256\\times256$ to $512\\times512$ pixels\nfor most of the pathological classes.\n  Comparison of saliency map-generated bounding boxes revealed that commonly\nused resolutions are insufficient for finding most pathologies.",
        "translated": "深度学习模型的图像分类往往使用的分辨率为 $224乘以224 $像素的计算原因。本研究使用 ChestX-ray14数据集，探讨图像分辨率对胸部 X 线分类性能的影响。结果显示，一个更高的图像分辨率，特别是 $1024乘以1024 $像素，具有最好的整体分类性能，性能略有下降，在 $256乘以256 $到 $512乘以512 $像素之间的大多数病理类。显著性图生成的边界框的比较显示，常用的分辨率不足以发现大多数病理。"
    },
    {
        "title": "How Does Fine-Tuning Impact Out-of-Distribution Detection for\n  Vision-Language Models?",
        "url": "http://arxiv.org/abs/2306.06048v1",
        "pub_date": "2023-06-09",
        "summary": "Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.",
        "translated": "最近的大型视觉语言模型，例如 CLIP，已经显示出显著的分布外(OOD)检测和泛化性能。然而，对于下游数据集，它们的零点分布(ID)精度往往受到限制。最近基于 CLIP 的微调方法，例如快速学习，已经显示出在提供 OOD 标签的 ID 分类和 OOD 泛化方面的重大改进。尽管如此，在没有 OOD 标签的情况下，该模型对于语义转换是否可靠尚不清楚。在本文中，我们的目标是弥合差距，并提出了一个全面的研究，以了解如何微调影响面向对象的检测为少拍下游任务。通过将面向对象的检测定义为多模态概念匹配，建立了微调方法与不同面向对象得分之间的关系。我们的研究结果表明，正确选择面向对象分数对于基于 CLIP 的微调至关重要。特别是，最大概念匹配(MCM)得分一致地提供了一个有希望的解决方案。我们还表明，快速学习证明了最先进的面向对象的检测性能超过零拍对应。"
    },
    {
        "title": "GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2306.06044v1",
        "pub_date": "2023-06-09",
        "summary": "Neural Radiance Fields (NeRF) have shown impressive novel view synthesis\nresults; nonetheless, even thorough recordings yield imperfections in\nreconstructions, for instance due to poorly observed areas or minor lighting\nchanges. Our goal is to mitigate these imperfections from various sources with\na joint solution: we take advantage of the ability of generative adversarial\nnetworks (GANs) to produce realistic images and use them to enhance realism in\n3D scene reconstruction with NeRFs. To this end, we learn the patch\ndistribution of a scene using an adversarial discriminator, which provides\nfeedback to the radiance field reconstruction, thus improving realism in a\n3D-consistent fashion. Thereby, rendering artifacts are repaired directly in\nthe underlying 3D representation by imposing multi-view path rendering\nconstraints. In addition, we condition a generator with multi-resolution NeRF\nrenderings which is adversarially trained to further improve rendering quality.\nWe demonstrate that our approach significantly improves rendering quality,\ne.g., nearly halving LPIPS scores compared to Nerfacto while at the same time\nimproving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.",
        "translated": "神经辐射场(NeRF)已经显示出令人印象深刻的新颖视图合成结果; 然而，即使是全面的记录也会在重建中产生缺陷，例如由于观察不到的区域或微小的光线变化。我们的目标是通过一个联合的解决方案来减轻各种来源的这些缺陷: 我们利用生成对抗网络(GAN)的能力来生成逼真的图像，并使用它们来增强使用 NERF 进行3D 场景重建的逼真度。为此，我们使用一个对抗性鉴别器来学习场景的补丁分布，它为辐射场重建提供反馈，从而以一种三维一致的方式提高真实感。因此，通过施加多视图路径绘制约束，可以直接在底层3D 表示中修复绘制伪影。此外，我们使用多分辨率的 NERF 渲染来调节一个生成器，该生成器经过反向训练以进一步提高渲染质量。我们证明了我们的方法显著提高了渲染质量，例如，与 Nerfacto 相比，LPIPS 得分几乎减半，同时在坦克和神庙的高级室内场景上提高了1.4分贝的峰值信噪比。"
    },
    {
        "title": "WindowNet: Learnable Windows for Chest X-ray Classification",
        "url": "http://arxiv.org/abs/2306.06038v1",
        "pub_date": "2023-06-09",
        "summary": "Chest X-ray (CXR) images are commonly compressed to a lower resolution and\nbit depth to reduce their size, potentially altering subtle diagnostic\nfeatures.\n  Radiologists use windowing operations to enhance image contrast, but the\nimpact of such operations on CXR classification performance is unclear.\n  In this study, we show that windowing can improve CXR classification\nperformance, and propose WindowNet, a model that learns optimal window\nsettings.\n  We first investigate the impact of bit-depth on classification performance\nand find that a higher bit-depth (12-bit) leads to improved performance.\n  We then evaluate different windowing settings and show that training with a\ndistinct window generally improves pathology-wise classification performance.\n  Finally, we propose and evaluate WindowNet, a model that learns optimal\nwindow settings, and show that it significantly improves performance compared\nto the baseline model without windowing.",
        "translated": "胸部 X 线(CXR)图像通常压缩到较低的分辨率和位深度，以减少其大小，潜在地改变微妙的诊断功能。放射科医生使用窗口操作来增强图像对比度，但这种操作对 CXR 分类性能的影响尚不清楚。在本研究中，我们发现视窗可以改善 CXR 分类器的分类性能，并提出视窗网路模型，学习最佳视窗设定。我们首先研究了位深度对分类性能的影响，发现更高的位深度(12位)可以提高性能。然后，我们评估不同的窗口设置，并表明训练与不同的窗口一般提高病理分类性能。最后，我们提出并评估了 WindowNet，一个学习最佳窗口设置的模型，并且表明与没有窗口的基线模型相比，它显著地提高了性能。"
    },
    {
        "title": "DetZero: Rethinking Offboard 3D Object Detection with Long-term\n  Sequential Point Clouds",
        "url": "http://arxiv.org/abs/2306.06023v1",
        "pub_date": "2023-06-09",
        "summary": "Existing offboard 3D detectors always follow a modular pipeline design to\ntake advantage of unlimited sequential point clouds. We have found that the\nfull potential of offboard 3D detectors is not explored mainly due to two\nreasons: (1) the onboard multi-object tracker cannot generate sufficient\ncomplete object trajectories, and (2) the motion state of objects poses an\ninevitable challenge for the object-centric refining stage in leveraging the\nlong-term temporal context representation. To tackle these problems, we propose\na novel paradigm of offboard 3D object detection, named DetZero. Concretely, an\noffline tracker coupled with a multi-frame detector is proposed to focus on the\ncompleteness of generated object tracks. An attention-mechanism refining module\nis proposed to strengthen contextual information interaction across long-term\nsequential point clouds for object refining with decomposed regression methods.\nExtensive experiments on Waymo Open Dataset show our DetZero outperforms all\nstate-of-the-art onboard and offboard 3D detection methods. Notably, DetZero\nranks 1st place on Waymo 3D object detection leaderboard with 85.15 mAPH (L2)\ndetection performance. Further experiments validate the application of taking\nthe place of human labels with such high-quality results. Our empirical study\nleads to rethinking conventions and interesting findings that can guide future\nresearch on offboard 3D object detection.",
        "translated": "现有的船外3D 探测器总是遵循模块化的流水线设计，以利用无限的顺序点云。我们发现机载三维探测器的潜力没有得到充分发挥，主要有两个原因: (1)机载多目标跟踪器不能产生足够完整的目标轨迹; (2)目标的运动状态对以目标为中心的精化阶段利用长期的时间上下文表示提出了不可避免的挑战。为了解决这些问题，我们提出了一个新颖的船外3D 目标检测的范例，名为 DetZero。具体地，提出了一种耦合多帧检测器的离线跟踪器，以保证生成的目标轨迹的完整性。提出了一种基于分解回归方法的注意机制细化模块，用于加强长期序列点云间的上下文信息交互。在 Waymo 开放数据集上的大量实验表明，我们的 DetZero 优于所有最先进的船上和船外3D 检测方法。值得注意的是，DetZero 以85.15 mAPH (L2)的检测性能在 Waymo 3 d 目标检测排行榜上名列第一。进一步的实验验证了用这种高质量的结果代替人类标签的应用。我们的实证研究引导我们重新思考惯例和有趣的发现，这些发现可以指导未来关于离岸三维目标检测的研究。"
    },
    {
        "title": "No Free Lunch: The Hazards of Over-Expressive Representations in Anomaly\n  Detection",
        "url": "http://arxiv.org/abs/2306.07284v1",
        "pub_date": "2023-06-12",
        "summary": "Anomaly detection methods, powered by deep learning, have recently been\nmaking significant progress, mostly due to improved representations. It is\ntempting to hypothesize that anomaly detection can improve indefinitely by\nincreasing the scale of our networks, making their representations more\nexpressive. In this paper, we provide theoretical and empirical evidence to the\ncontrary. In fact, we empirically show cases where very expressive\nrepresentations fail to detect even simple anomalies when evaluated beyond the\nwell-studied object-centric datasets. To investigate this phenomenon, we begin\nby introducing a novel theoretical toy model for anomaly detection performance.\nThe model uncovers a fundamental trade-off between representation sufficiency\nand over-expressivity. It provides evidence for a no-free-lunch theorem in\nanomaly detection stating that increasing representation expressivity will\neventually result in performance degradation. Instead, guidance must be\nprovided to focus the representation on the attributes relevant to the\nanomalies of interest. We conduct an extensive empirical investigation\ndemonstrating that state-of-the-art representations often suffer from\nover-expressivity, failing to detect many types of anomalies. Our investigation\ndemonstrates how this over-expressivity impairs image anomaly detection in\npractical settings. We conclude with future directions for mitigating this\nissue.",
        "translated": "在深度学习的推动下，异常检测方法最近取得了重大进展，这主要归功于改进的表征。人们很容易假设，异常检测可以通过增加我们网络的规模，让它们的表现更具表现力，从而无限期地改善。在本文中，我们提供了相反的理论和经验证明。事实上，我们经验性地展示了这样的情况: 当评估超出已经研究过的以对象为中心的数据集时，非常具有表现力的表示甚至连简单的异常都检测不到。为了研究这种现象，我们首先引入了一个新的理论玩具模型，用于异常检测表演。该模型揭示了表示充分性和过度表达性之间的基本权衡。它为异常检测中的不免费午餐定理提供了证据，该定理指出，表现表达能力的提高最终将导致性能下降。相反，必须提供指导，将表示集中在与感兴趣的异常相关的属性上。我们进行了广泛的实证调查，表明最先进的表现往往受到表达过度，未能发现许多类型的异常。我们的研究证明了这种过度表现是如何在实际环境中损害图像异常检测的。最后，我们提出了缓解这一问题的未来方向。"
    },
    {
        "title": "Waffling around for Performance: Visual Classification with Random Words\n  and Broad Concepts",
        "url": "http://arxiv.org/abs/2306.07282v1",
        "pub_date": "2023-06-12",
        "summary": "The visual classification performance of vision-language models such as CLIP\ncan benefit from additional semantic knowledge, e.g. via large language models\n(LLMs) such as GPT-3. Further extending classnames with LLM-generated class\ndescriptors, e.g. ``waffle, \\textit{which has a round shape}'', or averaging\nretrieval scores over multiple such descriptors, has been shown to improve\ngeneralization performance. In this work, we study this behavior in detail and\npropose \\texttt{Waffle}CLIP, a framework for zero-shot visual classification\nwhich achieves similar performance gains on a large number of visual\nclassification tasks by simply replacing LLM-generated descriptors with random\ncharacter and word descriptors \\textbf{without} querying external models. We\nextend these results with an extensive experimental study on the impact and\nshortcomings of additional semantics introduced via LLM-generated descriptors,\nand showcase how semantic context is better leveraged by automatically querying\nLLMs for high-level concepts, while jointly resolving potential class name\nambiguities. Link to the codebase: https://github.com/ExplainableML/WaffleCLIP.",
        "translated": "可视化语言模型(如 CLIP)的可视化分类性能可以受益于额外的语义知识，例如通过大型语言模型(LLM)(如 GPT-3)。使用 LLM 生成的类描述符进一步扩展类名，例如“ waffle，texttit { which has a round form }”，或者在多个这样的描述符上平均检索得分，已经被证明可以提高泛化性能。在这项工作中，我们详细研究了这种行为，并提出了 texttt { Waffle } CLIP，一个零拍视觉分类的框架，通过简单地用随机字符和词描述符替换 LLM 生成的描述符，在大量的视觉分类任务中获得相似的性能提高。我们通过对通过 LLM 生成的描述符引入的附加语义的影响和缺点进行广泛的实验研究来扩展这些结果，并展示如何通过自动查询 LLM 来更好地利用语义上下文的高级概念，同时共同解决潜在的类名歧义。链接到代码库:  https://github.com/explainableml/waffleclip。"
    },
    {
        "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
        "url": "http://arxiv.org/abs/2306.07280v1",
        "pub_date": "2023-06-12",
        "summary": "Large text-to-image diffusion models have impressive capabilities in\ngenerating photorealistic images from text prompts. How to effectively guide or\ncontrol these powerful models to perform different downstream tasks becomes an\nimportant open problem. To tackle this challenge, we introduce a principled\nfinetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image\ndiffusion models to downstream tasks. Unlike existing methods, OFT can provably\npreserve hyperspherical energy which characterizes the pairwise neuron\nrelationship on the unit hypersphere. We find that this property is crucial for\npreserving the semantic generation ability of text-to-image diffusion models.\nTo improve finetuning stability, we further propose Constrained Orthogonal\nFinetuning (COFT) which imposes an additional radius constraint to the\nhypersphere. Specifically, we consider two important finetuning text-to-image\ntasks: subject-driven generation where the goal is to generate subject-specific\nimages given a few images of a subject and a text prompt, and controllable\ngeneration where the goal is to enable the model to take in additional control\nsignals. We empirically show that our OFT framework outperforms existing\nmethods in generation quality and convergence speed.",
        "translated": "大型文本到图像的扩散模型在从文本提示生成逼真的图像方面具有令人印象深刻的能力。如何有效地引导或控制这些强大的模型来执行不同的下游任务成为一个重要的公开问题。为了解决这一问题，我们引入了一种原理性的微调方法——正交微调(OFT) ，用于调整文本到图像的扩散模型以适应下游任务。与现有的方法不同，OFT 可以证明保持超球面能量的特点是单位超球面上的成对神经元关系。我们发现这一特性对于保持文本-图像扩散模型的语义生成能力至关重要。为了提高微调的稳定性，我们进一步提出了约束正交微调(COFT) ，它对超球面增加了一个额外的半径约束。具体来说，我们考虑两个重要的文本到图像的微调任务: 主题驱动的生成，其目标是生成给定主题和文本提示的一些图像的主题特定的图像，以及可控的生成，其目标是使模型能够采取额外的控制信号。我们的实验表明，我们的 OFT 框架在生成质量和收敛速度方面优于现有的方法。"
    },
    {
        "title": "Scalable 3D Captioning with Pretrained Models",
        "url": "http://arxiv.org/abs/2306.07279v1",
        "pub_date": "2023-06-12",
        "summary": "We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.",
        "translated": "我们介绍 Cap3D，一种为3D 对象生成描述性文本的自动方法。这种方法利用来自图像字幕、图像-文本对齐和 LLM 的预先训练的模型来合并来自3D 资产的多个视图的字幕，完全避开了手动注释的耗时和昂贵的过程。我们将 Cap3D 应用到最近引入的大规模3D 数据集 Objaverse 中，得到了660k 的3D 文本对。我们使用来自同一数据集的41k 人工注释进行的评估表明，Cap3D 在质量、成本和速度方面都超过了人工描述。通过有效的快速工程，Cap3D 在从 ABO 数据集中收集的17k 注释生成几何描述方面与人类的性能相当。最后，我们微调 Cap3D 和人工字幕上的文本到3D 模型，并显示 Cap3D 的优异表现; 并基准 SOTA 包括 Point-E，Shape-E 和 DreamFusion。"
    },
    {
        "title": "Transcendental Idealism of Planner: Evaluating Perception from Planning\n  Perspective for Autonomous Driving",
        "url": "http://arxiv.org/abs/2306.07276v1",
        "pub_date": "2023-06-12",
        "summary": "Evaluating the performance of perception modules in autonomous driving is one\nof the most critical tasks in developing the complex intelligent system. While\nmodule-level unit test metrics adopted from traditional computer vision tasks\nare feasible to some extent, it remains far less explored to measure the impact\nof perceptual noise on the driving quality of autonomous vehicles in a\nconsistent and holistic manner. In this work, we propose a principled framework\nthat provides a coherent and systematic understanding of the impact an error in\nthe perception module imposes on an autonomous agent's planning that actually\ncontrols the vehicle. Specifically, the planning process is formulated as\nexpected utility maximisation, where all input signals from upstream modules\njointly provide a world state description, and the planner strives for the\noptimal action by maximising the expected utility determined by both world\nstates and actions. We show that, under practical conditions, the objective\nfunction can be represented as an inner product between the world state\ndescription and the utility function in a Hilbert space. This geometric\ninterpretation enables a novel way to analyse the impact of noise in world\nstate estimation on planning and leads to a universal metric for evaluating\nperception. The whole framework resembles the idea of transcendental idealism\nin the classical philosophical literature, which gives the name to our\napproach.",
        "translated": "自主驾驶感知模块的性能评价是复杂智能系统开发的关键问题之一。传统的计算机视觉任务所采用的模块级单元测试指标在一定程度上是可行的，但是如何以一致性和整体性的方式来衡量感知噪声对自主驾驶车辆行驶质量的影响，目前尚缺乏深入的研究。在这项工作中，我们提出了一个原则性的框架，提供了一个连贯和系统的理解的影响，在感知模块的错误所施加的自主主体的规划，实际控制车辆。具体地说，规划过程被表述为期望效用最大化，其中来自上游模块的所有输入信号共同提供一个世界状态描述，规划者通过最大化由世界状态和行动决定的期望效用来争取最优行动。证明了在实际条件下，目标函数可以表示为希尔伯特空间中世界状态描述与效用函数之间的内积。这种几何解释为分析世界状态估计中噪声对规划的影响提供了一种新的方法，并导致了一种评估感知的通用度量。整个框架类似于古典哲学文献中的先验唯心主义的思想，这给我们的方法起了名字。"
    },
    {
        "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2306.07971v1",
        "pub_date": "2023-06-13",
        "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
        "translated": "在大型视觉语言模型方面的最新突破，如巴德和 GPT-4，展示了在执行广泛任务方面的非凡能力。这样的模型是在包含数十亿公共图像-文本对和不同任务的大量数据集上进行训练的。然而，由于缺乏对生物医学图像的深入理解，它们在特定任务领域(如放射学)的表现仍然没有得到充分的研究，并可能受到限制。另一方面，会话医学模式取得了显著的成功，但主要集中在基于文本的分析。本文介绍了一种新型的医学会话视觉语言模型 XrayGPT，它可以分析和回答有关胸片的开放性问题。具体来说，我们使用一个简单的线性映射，将两个医学视觉编码器(MedClip)与一个经过微调的大型语言模型(Vicuna)对齐。这种排列使我们的模型具有卓越的视觉对话能力，根据深刻的理解射线照相和医学领域的知识。为了提高 LLM 在医学环境中的性能，我们从自由文本放射学报告中生成约217k 交互式和高质量的摘要。这些总结有助于通过微调过程提高 LLM 的性能。我们的方法为推进胸片自动化分析的研究开辟了新的途径。我们的开源演示、模型和指令集可在以下 https://github.com/mbzuai-oryx/xraygpt 获得:。"
    },
    {
        "title": "GeneCIS: A Benchmark for General Conditional Image Similarity",
        "url": "http://arxiv.org/abs/2306.07969v1",
        "pub_date": "2023-06-13",
        "summary": "We argue that there are many notions of 'similarity' and that models, like\nhumans, should be able to adapt to these dynamically. This contrasts with most\nrepresentation learning methods, supervised or self-supervised, which learn a\nfixed embedding function and hence implicitly assume a single notion of\nsimilarity. For instance, models trained on ImageNet are biased towards object\ncategories, while a user might prefer the model to focus on colors, textures or\nspecific elements in the scene. In this paper, we propose the GeneCIS\n('genesis') benchmark, which measures models' ability to adapt to a range of\nsimilarity conditions. Extending prior work, our benchmark is designed for\nzero-shot evaluation only, and hence considers an open-set of similarity\nconditions. We find that baselines from powerful CLIP models struggle on\nGeneCIS and that performance on the benchmark is only weakly correlated with\nImageNet accuracy, suggesting that simply scaling existing methods is not\nfruitful. We further propose a simple, scalable solution based on automatically\nmining information from existing image-caption datasets. We find our method\noffers a substantial boost over the baselines on GeneCIS, and further improves\nzero-shot performance on related image retrieval benchmarks. In fact, though\nevaluated zero-shot, our model surpasses state-of-the-art supervised models on\nMIT-States. Project page at https://sgvaze.github.io/genecis/.",
        "translated": "我们认为有许多“相似性”的概念，模型，像人类一样，应该能够动态地适应这些概念。这与大多数有监督或自监督的表示学习方法相反，这些方法学习一个固定的嵌入函数，因此隐式地假定一个单一的相似性概念。例如，在 ImageNet 上训练的模型偏向于对象类别，而用户可能更喜欢模型关注场景中的颜色、纹理或特定元素。在这篇文章中，我们提出了 GeneCIS (“创生”)基准，它衡量模型适应一系列相似条件的能力。扩展了先前的工作，我们的基准设计只用于零点评估，因此考虑了一个开放的相似性条件集。我们发现，来自强大的 CLIP 模型的基线在 GeneCIS 上挣扎，基准的性能与 ImageNet 的准确性只有微弱的相关性，这表明简单地扩展现有方法是没有成效的。我们进一步提出了一个简单的，可扩展的解决方案的基础上自动挖掘信息从现有的图像标题数据集。我们发现我们的方法在 GeneCIS 基线上提供了实质性的改进，并且进一步提高了相关图像检索基准的零镜头性能。事实上，虽然评估的零拍摄，我们的模型超过国家的最先进的监督麻省理工学院的模型。Https://sgvaze.github.io/genecis/项目网页。"
    },
    {
        "title": "Neural Scene Chronology",
        "url": "http://arxiv.org/abs/2306.07970v1",
        "pub_date": "2023-06-13",
        "summary": "In this work, we aim to reconstruct a time-varying 3D model, capable of\nrendering photo-realistic renderings with independent control of viewpoint,\nillumination, and time, from Internet photos of large-scale landmarks. The core\nchallenges are twofold. First, different types of temporal changes, such as\nillumination and changes to the underlying scene itself (such as replacing one\ngraffiti artwork with another) are entangled together in the imagery. Second,\nscene-level temporal changes are often discrete and sporadic over time, rather\nthan continuous. To tackle these problems, we propose a new scene\nrepresentation equipped with a novel temporal step function encoding method\nthat can model discrete scene-level content changes as piece-wise constant\nfunctions over time. Specifically, we represent the scene as a space-time\nradiance field with a per-image illumination embedding, where\ntemporally-varying scene changes are encoded using a set of learned step\nfunctions. To facilitate our task of chronology reconstruction from Internet\nimagery, we also collect a new dataset of four scenes that exhibit various\nchanges over time. We demonstrate that our method exhibits state-of-the-art\nview synthesis results on this dataset, while achieving independent control of\nviewpoint, time, and illumination.",
        "translated": "在这项工作中，我们的目标是重建一个时变的三维模型，能够渲染照片真实的渲染与独立控制的观点，照明和时间，从互联网上的大规模地标照片。核心挑战有两方面。首先，不同类型的时间变化，如照明和变化的基础场景本身(如取代一个涂鸦艺术品与另一个)纠缠在一起的意象。其次，场景级别的时间变化往往是离散的，随着时间的推移是零星的，而不是连续的。为了解决这些问题，我们提出了一种新的场景表示方法，该方法配备了一种新的时间步长函数编码方法，可以将离散场景层次的内容随时间的变化建模为分段常数函数。具体地说，我们将场景表示为一个时空辐射场，每个图像都嵌入了照明，其中使用一组学习的步长函数对时变场景变化进行编码。为了方便我们的任务，从互联网图像的年表重建，我们还收集了一个新的数据集的四个场景，显示随着时间的推移各种变化。我们证明了我们的方法展示了最先进的视图合成结果在这个数据集，同时实现了独立的控制视点，时间和照明。"
    },
    {
        "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
        "url": "http://arxiv.org/abs/2306.07967v1",
        "pub_date": "2023-06-13",
        "summary": "We present Generalized LoRA (GLoRA), an advanced approach for universal\nparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),\nGLoRA employs a generalized prompt module to optimize pre-trained model weights\nand adjust intermediate activations, providing more flexibility and capability\nacross diverse tasks and datasets. Moreover, GLoRA facilitates efficient\nparameter adaptation by employing a scalable, modular, layer-wise structure\nsearch that learns individual adapter of each layer. Originating from a unified\nmathematical formulation, GLoRA exhibits strong transfer learning, few-shot\nlearning and domain generalization abilities, as it adjusts to new tasks\nthrough additional dimensions on weights and activations. Comprehensive\nexperiments demonstrate that GLoRA outperforms all previous methods in natural,\nspecialized, and structured benchmarks, achieving superior accuracy with fewer\nparameters and computations on various datasets. Furthermore, our structural\nre-parameterization design ensures that GLoRA incurs no extra inference cost,\nrendering it a practical solution for resource-limited applications. Code is\navailable at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "translated": "提出了一种通用参数有效微调任务的先进方法——广义 LoRA (GLoRA)。增强低级适应(LoRA) ，GLoRA 采用广义提示模块优化预先训练的模型权重和调整中间激活，提供更多的灵活性和能力跨不同的任务和数据集。此外，GLoRA 通过采用可伸缩的、模块化的、分层的结构搜索，学习每一层的单个适配器，促进了有效的参数适配。起源于统一的数学公式，GLoRA 表现出强大的转移学习，少镜头学习和领域概括能力，因为它通过权重和激活的额外维度来适应新的任务。综合实验表明，GLoRA 在自然的、专门的和结构化的基准测试中优于以前的所有方法，在各种数据集上以更少的参数和计算获得更高的准确性。此外，我们的结构重新参数化设计确保了 GLoRA 不会产生额外的推理成本，从而为资源有限的应用程序提供了一个实用的解决方案。密码可于以下 https://github.com/arnav0400/vit-slim/tree/master/glora 索取:。"
    },
    {
        "title": "Parting with Misconceptions about Learning-based Vehicle Motion Planning",
        "url": "http://arxiv.org/abs/2306.07962v1",
        "pub_date": "2023-06-13",
        "summary": "The release of nuPlan marks a new era in vehicle motion planning research,\noffering the first large-scale real-world dataset and evaluation schemes\nrequiring both precise short-term planning and long-horizon ego-forecasting.\nExisting systems struggle to simultaneously meet both requirements. Indeed, we\nfind that these tasks are fundamentally misaligned and should be addressed\nindependently. We further assess the current state of closed-loop planning in\nthe field, revealing the limitations of learning-based methods in complex\nreal-world scenarios and the value of simple rule-based priors such as\ncenterline selection through lane graph search algorithms. More surprisingly,\nfor the open-loop sub-task, we observe that the best results are achieved when\nusing only this centerline as scene context (\\ie, ignoring all information\nregarding the map and other agents). Combining these insights, we propose an\nextremely simple and efficient planner which outperforms an extensive set of\ncompetitors, winning the nuPlan planning challenge 2023.",
        "translated": "NuPlan 的发布标志着汽车运动规划研究的一个新时代，它提供了第一个大规模的现实世界数据集和评估方案，既需要精确的短期规划，也需要长期的自我预测。现有系统很难同时满足这两个需求。事实上，我们发现这些任务从根本上是错位的，应该独立处理。我们进一步评估了闭环规划领域的现状，揭示了基于学习的方法在复杂现实场景中的局限性，以及基于规则的简单先验(如通过车道图搜索算法进行中心线选择)的价值。更令人惊讶的是，对于开环子任务，我们观察到只使用这个中心线作为场景上下文(即，忽略关于地图和其他代理的所有信息)可以获得最佳结果。结合这些见解，我们提出了一个非常简单和高效的规划，优于广泛的一套竞争对手，赢得了2023年的 nuPlan 规划挑战。"
    },
    {
        "title": "Seeing the World through Your Eyes",
        "url": "http://arxiv.org/abs/2306.09348v1",
        "pub_date": "2023-06-15",
        "summary": "The reflective nature of the human eye is an underappreciated source of\ninformation about what the world around us looks like. By imaging the eyes of a\nmoving person, we can collect multiple views of a scene outside the camera's\ndirect line of sight through the reflections in the eyes. In this paper, we\nreconstruct a 3D scene beyond the camera's line of sight using portrait images\ncontaining eye reflections. This task is challenging due to 1) the difficulty\nof accurately estimating eye poses and 2) the entangled appearance of the eye\niris and the scene reflections. Our method jointly refines the cornea poses,\nthe radiance field depicting the scene, and the observer's eye iris texture. We\nfurther propose a simple regularization prior on the iris texture pattern to\nimprove reconstruction quality. Through various experiments on synthetic and\nreal-world captures featuring people with varied eye colors, we demonstrate the\nfeasibility of our approach to recover 3D scenes using eye reflections.",
        "translated": "人类眼睛的反射性本质是一个被低估的信息来源，关于我们周围的世界是什么样子。通过对移动人物的眼睛进行成像，我们可以通过眼睛的反射来收集摄像机直接视线以外的场景的多种视图。在本文中，我们利用包含眼睛反射的人像图像重建摄像机视线以外的三维场景。这项任务是具有挑战性的，因为1)准确估计眼睛的姿势和2)眼睛虹膜的纠缠外观和场景反射的困难。我们的方法共同提炼角膜的姿态，光场描述的场景，和观察者的眼睛虹膜纹理。我们进一步提出了一种简单的正则化先验的虹膜纹理模式，以提高重建质量。通过对不同眼睛颜色的人的合成和真实世界捕捉的各种实验，我们证明了我们的方法的可行性恢复三维场景使用眼睛的反射。"
    },
    {
        "title": "UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video",
        "url": "http://arxiv.org/abs/2306.09349v1",
        "pub_date": "2023-06-15",
        "summary": "We show how to build a model that allows realistic, free-viewpoint renderings\nof a scene under novel lighting conditions from video. Our method -- UrbanIR:\nUrban Scene Inverse Rendering -- computes an inverse graphics representation\nfrom the video. UrbanIR jointly infers shape, albedo, visibility, and sun and\nsky illumination from a single video of unbounded outdoor scenes with unknown\nlighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many\nviews of the same points in typical NeRF-style estimation). As a result,\nstandard methods produce poor geometry estimates (for example, roofs), and\nthere are numerous ''floaters''. Errors in inverse graphics inference can\nresult in strong rendering artifacts. UrbanIR uses novel losses to control\nthese and other sources of error. UrbanIR uses a novel loss to make very good\nestimates of shadow volumes in the original scene. The resulting\nrepresentations facilitate controllable editing, delivering photorealistic\nfree-viewpoint renderings of relit scenes and inserted objects. Qualitative\nevaluation demonstrates strong improvements over the state-of-the-art.",
        "translated": "我们展示了如何建立一个模型，允许现实，自由视点渲染一个场景下新的照明条件从视频。我们的方法—— UrbanIR: 城市场景反向渲染——从视频中计算一个反向图形表示。UrbanIR 共同推断的形状，反照率，能见度，以及太阳和天空照明从一个单一的视频无界户外场景与未知的照明。UrbanIR 使用安装在汽车上的摄像头拍摄的视频(与典型的 NERF 风格估算中的许多同一点的视图形成对比)。因此，标准的方法产生了糟糕的几何估计(例如，屋顶) ，并且存在大量的“浮动”。反向图形推理中的错误可能导致强烈的渲染伪影。UrbanIR 使用新的损失来控制这些和其他误差来源。UrbanIR 使用一种新颖的损失来对原始场景中的阴影体积做出很好的估计。由此产生的表示方便可控的编辑，提供重现场景和插入对象的真实感自由视点渲染。定性评价表明，与最先进的技术相比，有了很大的改进。"
    },
    {
        "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
        "url": "http://arxiv.org/abs/2306.09346v1",
        "pub_date": "2023-06-15",
        "summary": "Do different neural networks, trained for various vision tasks, share some\ncommon representations?\n  In this paper, we demonstrate the existence of common features we call\n\"Rosetta Neurons\" across a range of models with different architectures,\ndifferent tasks (generative and discriminative), and different types of\nsupervision (class-supervised, text-supervised, self-supervised). We present an\nalgorithm for mining a dictionary of Rosetta Neurons across several popular\nvision models:\n  Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50,\nBigGAN, StyleGAN-2, StyleGAN-XL.\n  Our findings suggest that certain visual concepts and structures are\ninherently embedded in the natural world and can be learned by different models\nregardless of the specific task or architecture, and without the use of\nsemantic labels. We can visualize shared concepts directly due to generative\nmodels included in our analysis. The Rosetta Neurons facilitate model-to-model\ntranslation enabling various inversion-based manipulations, including\ncross-class alignments, shifting, zooming, and more, without the need for\nspecialized training.",
        "translated": "不同的神经网络，训练不同的视觉任务，共享一些共同的表征？在本文中，我们证明了存在的共同特征，我们称之为“罗塞塔神经元”跨一系列的模型，不同的架构，不同的任务(生成和区分) ，以及不同类型的监督(类监督，文本监督，自我监督)。我们提出了一个算法挖掘罗塞塔神经元字典跨几个流行的视觉模型: 类监督-ResNet50，DINO-ResNet50，DINO-ViT，MAE，CLIP-ResNet50，BigGAN，StyleGAN-2，StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，可以通过不同的模型学习，不管具体的任务或架构，而不使用语义标签。由于分析中包含了生成模型，我们可以直接将共享的概念可视化。Rosetta 神经元促进模型到模型的转换，使各种基于倒置的操作成为可能，包括跨类比对、移位、缩放等等，而不需要专门的训练。"
    },
    {
        "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
        "url": "http://arxiv.org/abs/2306.09347v1",
        "pub_date": "2023-06-15",
        "summary": "Recent advancements in vision foundation models (VFMs) have opened up new\npossibilities for versatile and efficient visual perception. In this work, we\nintroduce Seal, a novel framework that harnesses VFMs for segmenting diverse\nautomotive point cloud sequences. Seal exhibits three appealing properties: i)\nScalability: VFMs are directly distilled into point clouds, eliminating the\nneed for annotations in either 2D or 3D during pretraining. ii) Consistency:\nSpatial and temporal relationships are enforced at both the camera-to-LiDAR and\npoint-to-segment stages, facilitating cross-modal representation learning. iii)\nGeneralizability: Seal enables knowledge transfer in an off-the-shelf manner to\ndownstream tasks involving diverse point clouds, including those from\nreal/synthetic, low/high-resolution, large/small-scale, and clean/corrupted\ndatasets. Extensive experiments conducted on eleven different point cloud\ndatasets showcase the effectiveness and superiority of Seal. Notably, Seal\nachieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing\nrandom initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU.\nMoreover, Seal demonstrates significant performance gains over existing methods\nacross 20 different few-shot fine-tuning tasks on all eleven tested point cloud\ndatasets.",
        "translated": "最近视觉基础模型(vision foundation model，VFM)的进步为多功能和高效的视知觉开辟了新的可能性。在这项工作中，我们介绍了密封，一个新的框架，利用 VFM 分割不同的汽车点云序列。玺展示了三个吸引人的特性: i)可伸缩性: VFM 被直接提炼成点云，在预训练期间不再需要2D 或3D 的注释。一致性: 在相机到激光雷达和点到片段阶段，空间和时间关系都得到了强化，促进了跨模态表示学习。Iii)概括性: 密封使得知识能够以现成的方式转移到涉及不同点云的下游任务，包括来自真实/合成、低/高分辨率、大/小规模和清洁/损坏数据集的任务。在十一个不同的点云数据集上进行的大量实验表明了海豹的有效性和优越性。值得注意的是，希尔在线性探测后在 nuScenes 上达到了显着的45.0% mIoU，比随机初始化高出36.9% mIoU，比现有技术高出6.1% mIoU。此外，在所有11个测试点云数据集上，在20个不同的小镜头微调任务上，Seal 显示了比现有方法显著的性能提高。"
    },
    {
        "title": "Evaluating Data Attribution for Text-to-Image Models",
        "url": "http://arxiv.org/abs/2306.09345v1",
        "pub_date": "2023-06-15",
        "summary": "While large text-to-image models are able to synthesize \"novel\" images, these\nimages are necessarily a reflection of the training data. The problem of data\nattribution in such models -- which of the images in the training set are most\nresponsible for the appearance of a given generated image -- is a difficult yet\nimportant one. As an initial step toward this problem, we evaluate attribution\nthrough \"customization\" methods, which tune an existing large-scale model\ntoward a given exemplar object or style. Our key insight is that this allows us\nto efficiently create synthetic images that are computationally influenced by\nthe exemplar by construction. With our new dataset of such exemplar-influenced\nimages, we are able to evaluate various data attribution algorithms and\ndifferent possible feature spaces. Furthermore, by training on our dataset, we\ncan tune standard models, such as DINO, CLIP, and ViT, toward the attribution\nproblem. Even though the procedure is tuned towards small exemplar sets, we\nshow generalization to larger sets. Finally, by taking into account the\ninherent uncertainty of the problem, we can assign soft attribution scores over\na set of training images.",
        "translated": "虽然大型文本-图像模型能够合成“新颖”图像，但这些图像必然是训练数据的反映。这类模型中的数据归属问题——训练集中的图像中哪一个对生成的图像的外观负有最大责任——是一个困难而又重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估属性，这些方法将现有的大规模模型调优到给定的示例对象或样式。我们的主要见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到构造样本的影响。我们的新数据集，这样的样本影响的图像，我们能够评估各种数据归属算法和不同的可能的特征空间。此外，通过对数据集进行训练，我们可以针对属性问题调优标准模型，如 DINO、 CLIP 和 ViT。尽管这个过程是针对小范例集进行调整的，但是我们展示了对大范例集的泛化。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因得分。"
    },
    {
        "title": "Coaching a Teachable Student",
        "url": "http://arxiv.org/abs/2306.10014v1",
        "pub_date": "2023-06-16",
        "summary": "We propose a novel knowledge distillation framework for effectively teaching\na sensorimotor student agent to drive from the supervision of a privileged\nteacher agent. Current distillation for sensorimotor agents methods tend to\nresult in suboptimal learned driving behavior by the student, which we\nhypothesize is due to inherent differences between the input, modeling\ncapacity, and optimization processes of the two agents. We develop a novel\ndistillation scheme that can address these limitations and close the gap\nbetween the sensorimotor agent and its privileged teacher. Our key insight is\nto design a student which learns to align their input features with the\nteacher's privileged Bird's Eye View (BEV) space. The student then can benefit\nfrom direct supervision by the teacher over the internal representation\nlearning. To scaffold the difficult sensorimotor learning task, the student\nmodel is optimized via a student-paced coaching mechanism with various\nauxiliary supervision. We further propose a high-capacity imitation learned\nprivileged agent that surpasses prior privileged agents in CARLA and ensures\nthe student learns safe driving behavior. Our proposed sensorimotor agent\nresults in a robust image-based behavior cloning agent in CARLA, improving over\ncurrent models by over 20.6% in driving score without requiring LiDAR,\nhistorical observations, ensemble of models, on-policy data aggregation or\nreinforcement learning.",
        "translated": "我们提出了一个新的知识提取框架，有效地教学感觉运动学生代理驱动从特权教师代理的监督。目前对于感觉运动代理方法的精馏往往导致学生学习驾驶行为的次优化，我们假设这是由于两个代理的输入、建模能力和优化过程之间的固有差异。我们开发了一个新颖的蒸馏方案，可以解决这些局限性，并缩小之间的差距，感觉运动代理及其特权教师。我们的主要见解是设计一个学生，学会调整他们的输入功能与教师的特权鸟瞰(BEV)空间。这样，学生就可以从教师对内部表征学习的直接监督中受益。为了构建高难度感觉运动学习任务的框架，通过多种辅助监控的学生节奏辅导机制对学生模型进行优化。我们进一步提出了一个高容量的模仿学习特权代理，超过以往的特权代理在 CARLA，并确保学生学习安全驾驶行为。我们提出的感知运动代理在 CARLA 中产生了一个强大的基于图像的行为克隆代理，在不需要激光雷达、历史观测、模型集成、政策数据聚合或强化学习的情况下，驱动分数比目前的模型提高了20.6% 以上。"
    },
    {
        "title": "PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic\n  Segmentation",
        "url": "http://arxiv.org/abs/2306.10013v1",
        "pub_date": "2023-06-16",
        "summary": "Comprehensive modeling of the surrounding 3D world is key to the success of\nautonomous driving. However, existing perception tasks like object detection,\nroad structure segmentation, depth &amp; elevation estimation, and open-set object\nlocalization each only focus on a small facet of the holistic 3D scene\nunderstanding task. This divide-and-conquer strategy simplifies the algorithm\ndevelopment procedure at the cost of losing an end-to-end unified solution to\nthe problem. In this work, we address this limitation by studying camera-based\n3D panoptic segmentation, aiming to achieve a unified occupancy representation\nfor camera-only 3D scene understanding. To achieve this, we introduce a novel\nmethod called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal\ninformation from multi-frame and multi-view images in a coarse-to-fine scheme,\nintegrating feature learning and scene representation into a unified occupancy\nrepresentation. We have conducted extensive ablation studies to verify the\neffectiveness and efficiency of the proposed method. Our approach achieves new\nstate-of-the-art results for camera-based semantic segmentation and panoptic\nsegmentation on the nuScenes dataset. Furthermore, our method can be easily\nextended to dense occupancy prediction and has shown promising performance on\nthe Occ3D benchmark. The code will be released at\nhttps://github.com/Robertwyq/PanoOcc.",
        "translated": "周围三维世界的综合建模是自主驾驶成功的关键。然而，现有的感知任务，比如目标检测、道路结构分割、深度和高度估计以及开放式对象定位，都只关注于整体三维场景理解任务的一小部分。这种分而治之的策略简化了算法开发过程，但代价是失去了问题的端到端统一解。在这项工作中，我们通过研究基于摄像机的三维全景分割来解决这一局限性，目的是实现一个统一的占用表示，用于只有摄像机的三维场景理解。为了实现这一目标，我们引入了一种新的方法 PanoOcc，该方法利用体素查询从多帧多视图图像中粗到精地聚合时空信息，将特征学习和场景表示融合到一个统一的占用表示中。我们进行了广泛的消融研究，以验证所提出的方法的有效性和效率。我们的方法在 nuScenes 数据集上实现了基于摄像机的语义分割和全景分割的最新结果。此外，我们的方法可以很容易地扩展到密集的占用率预测，并显示了良好的性能在 Occ3D 基准。密码会在 https://github.com/robertwyq/panoocc 公布。"
    },
    {
        "title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image\n  Editing",
        "url": "http://arxiv.org/abs/2306.10012v1",
        "pub_date": "2023-06-16",
        "summary": "Text-guided image editing is widely needed in daily life, ranging from\npersonal use to professional applications such as Photoshop. However, existing\nmethods are either zero-shot or trained on an automatically synthesized\ndataset, which contains a high volume of noise. Thus, they still require lots\nof manual tuning to produce desirable outcomes in practice. To address this\nissue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),\nthe first large-scale, manually annotated dataset for instruction-guided real\nimage editing that covers diverse scenarios: single-turn, multi-turn,\nmask-provided, and mask-free editing. MagicBrush comprises over 10K manually\nannotated triples (source image, instruction, target image), which supports\ntrainining large-scale text-guided image editing models. We fine-tune\nInstructPix2Pix on MagicBrush and show that the new model can produce much\nbetter images according to human evaluation. We further conduct extensive\nexperiments to evaluate current image editing baselines from multiple\ndimensions including quantitative, qualitative, and human evaluations. The\nresults reveal the challenging nature of our dataset and the gap between\ncurrent baselines and real-world editing needs.",
        "translated": "从个人使用到 Photoshop 等专业应用，文本引导图像编辑在日常生活中被广泛需要。然而，现有的方法要么是零拍摄，要么是在一个自动合成的数据集上训练，这个数据集包含了大量的噪声。因此，它们仍然需要大量的手动调优，以便在实践中产生理想的结果。为了解决这个问题，我们引入了 MagicBrush ( https://osu-nlp-group.github.io/MagicBrush/) ，这是第一个用于指令引导的真实图像编辑的大规模手动注释数据集，涵盖了不同的场景: 单转、多转、掩码提供和无掩码编辑。MagicBrush 包含超过10K 的手动注释三元组(源图像、指令、目标图像) ，支持训练大规模的文本引导图像编辑模型。我们在 MagicBrush 上对 DirectPix2Pix 进行了微调，结果表明新的模型可以根据人的评价产生更好的图像。我们进一步进行广泛的实验，以评估目前的图像编辑基线从多个维度，包括定量，定性和人类的评价。结果揭示了我们的数据集的挑战性，以及当前基线和现实世界编辑需求之间的差距。"
    },
    {
        "title": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via\n  Adversarial Latent Search",
        "url": "http://arxiv.org/abs/2306.10008v1",
        "pub_date": "2023-06-16",
        "summary": "The success of deep learning based face recognition systems has given rise to\nserious privacy concerns due to their ability to enable unauthorized tracking\nof users in the digital world. Existing methods for enhancing privacy fail to\ngenerate naturalistic images that can protect facial privacy without\ncompromising user experience. We propose a novel two-step approach for facial\nprivacy protection that relies on finding adversarial latent codes in the\nlow-dimensional manifold of a pretrained generative model. The first step\ninverts the given face image into the latent space and finetunes the generative\nmodel to achieve an accurate reconstruction of the given image from its latent\ncode. This step produces a good initialization, aiding the generation of\nhigh-quality faces that resemble the given identity. Subsequently, user-defined\nmakeup text prompts and identity-preserving regularization are used to guide\nthe search for adversarial codes in the latent space. Extensive experiments\ndemonstrate that faces generated by our approach have stronger black-box\ntransferability with an absolute gain of 12.06% over the state-of-the-art\nfacial privacy protection approach under the face verification task. Finally,\nwe demonstrate the effectiveness of the proposed approach for commercial face\nrecognition systems. Our code is available at\nhttps://github.com/fahadshamshad/Clip2Protect.",
        "translated": "基于深度学习的人脸识别系统的成功引起了严重的隐私问题，因为它们能够在数字世界中对用户进行未经授权的跟踪。现有的增强隐私的方法无法生成自然的图像，从而在不损害用户体验的情况下保护面部隐私。我们提出了一种新颖的两步法来保护面部隐私，这种方法依赖于在预先训练好的生成模型的低维流形中发现对手的潜在代码。第一步将给定的人脸图像反转到潜在空间，并对生成模型进行微调，以便从潜在代码实现给定图像的准确重建。这一步产生了良好的初始化，有助于生成类似于给定身份的高质量面孔。然后，利用自定义化妆文本提示和身份保持正则化来引导在潜在空间中搜索对手代码。大量实验表明，该方法生成的人脸具有更强的黑盒可转移性，在人脸验证任务下比最先进的面部隐私保护方法的绝对增益为12.06% 。最后，我们证明了该方法在商业人脸识别系统中的有效性。我们的代码可以在 https://github.com/fahadshamshad/clip2protect 找到。"
    },
    {
        "title": "Robot Learning with Sensorimotor Pre-training",
        "url": "http://arxiv.org/abs/2306.10007v1",
        "pub_date": "2023-06-16",
        "summary": "We present a self-supervised sensorimotor pre-training approach for robotics.\nOur model, called RPT, is a Transformer that operates on sequences of\nsensorimotor tokens. Given a sequence of camera images, proprioceptive robot\nstates, and past actions, we encode the interleaved sequence into tokens, mask\nout a random subset, and train a model to predict the masked-out content. We\nhypothesize that if the robot can predict the missing content it has acquired a\ngood model of the physical world that can enable it to act. RPT is designed to\noperate on latent visual representations which makes prediction tractable,\nenables scaling to 10x larger models, and 10 Hz inference on a real robot. To\nevaluate our approach, we collect a dataset of 20,000 real-world trajectories\nover 9 months using a combination of motion planning and model-based grasping\nalgorithms. We find that pre-training on this data consistently outperforms\ntraining from scratch, leads to 2x improvements in the block stacking task, and\nhas favorable scaling properties.",
        "translated": "我们提出了一种自我监督的感觉运动机器人预训练方法。我们的模型，称为 RPT，是一个变压器，操作感应运动令牌序列。给定一个摄像机图像序列、本体感知机器人状态和过去的动作，我们将交织序列编码成标记，掩盖掉一个随机子集，并训练一个模型来预测掩盖掉的内容。我们假设，如果机器人能够预测丢失的内容，它已经获得了一个良好的物理世界模型，使它能够采取行动。RPT 被设计用于对潜在的视觉表征进行操作，这使得预测变得易于处理，能够在真实的机器人上扩展到10倍大的模型和10赫兹的推理。为了评估我们的方法，我们使用运动规划和基于模型的抓取算法的组合，在9个月内收集了20,000个真实世界轨迹的数据集。我们发现，对这些数据的预训练始终优于从头开始的训练，导致块堆叠任务的2倍改进，并具有良好的缩放性能。"
    },
    {
        "title": "Learning Profitable NFT Image Diffusions via Multiple Visual-Policy\n  Guided Reinforcement Learning",
        "url": "http://arxiv.org/abs/2306.11731v1",
        "pub_date": "2023-06-20",
        "summary": "We study the task of generating profitable Non-Fungible Token (NFT) images\nfrom user-input texts. Recent advances in diffusion models have shown great\npotential for image generation. However, existing works can fall short in\ngenerating visually-pleasing and highly-profitable NFT images, mainly due to\nthe lack of 1) plentiful and fine-grained visual attribute prompts for an NFT\nimage, and 2) effective optimization metrics for generating high-quality NFT\nimages. To solve these challenges, we propose a Diffusion-based generation\nframework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for\nNFT images. The proposed framework consists of a large language model (LLM), a\ndiffusion-based image generator, and a series of visual rewards by design.\nFirst, the LLM enhances a basic human input (such as \"panda\") by generating\nmore comprehensive NFT-style prompts that include specific visual attributes,\nsuch as \"panda with Ninja style and green background.\" Second, the\ndiffusion-based image generator is fine-tuned using a large-scale NFT dataset\nto capture fine-grained image styles and accessory compositions of popular NFT\nelements. Third, we further propose to utilize multiple visual-policies as\noptimization goals, including visual rarity levels, visual aesthetic scores,\nand CLIP-based text-image relevances. This design ensures that our proposed\nDiffusion-MVP is capable of minting NFT images with high visual quality and\nmarket value. To facilitate this research, we have collected the largest\npublicly available NFT image dataset to date, consisting of 1.5 million\nhigh-quality images with corresponding texts and market values. Extensive\nexperiments including objective evaluations and user studies demonstrate that\nour framework can generate NFT images showing more visually engaging elements\nand higher market value, compared with SOTA approaches.",
        "translated": "我们研究了从用户输入的文本中生成有利可图的非替代令牌(NFT)图像的任务。扩散模型的最新进展显示了图像生成的巨大潜力。然而，现有的作品在生成视觉上令人满意且高利润的非功能性傅里叶变换(NFT)图像方面存在不足，这主要是由于缺乏1)丰富和细粒度的非功能性傅里叶变换图像的视觉属性提示，以及2)生成高质量非功能性傅里叶变换图像的有效优化指标。为了解决这些问题，我们提出了一个基于扩散的生成框架，该框架使用多种视觉策略作为 NFT 图像的奖励(即扩散 MVP)。该框架由一个大型语言模型(LLM)、一个基于扩散的图像生成器和一系列设计的视觉奖励组成。首先，LLM 通过生成更全面的 NFT 风格的提示，包括特定的视觉属性，如“忍者风格和绿色背景的熊猫”，增强了基本的人类输入(如“熊猫”)其次，使用大规模的 NFT 数据集对基于扩散的图像生成器进行微调，以捕获流行的 NFT 元件的细粒度图像样式和附件组成。第三，我们进一步提出利用多个视觉策略作为优化目标，包括视觉稀有度水平，视觉审美评分，以及基于 CLIP 的文本图像相关性。这种设计确保了我们提出的扩散 MVP 能够生成具有高视觉质量和市场价值的 NFT 图像。为了促进这项研究，我们已经收集了迄今为止最大的公开可用的 NFT 图像数据集，包括150万高质量的图像与相应的文本和市场价值。包括客观评价和用户研究在内的大量实验表明，与 SOTA 方法相比，我们的框架可以生成更具视觉吸引力的元素和更高的市场价值的 NFT 图像。"
    },
    {
        "title": "Segment Anything Model (SAM) for Radiation Oncology",
        "url": "http://arxiv.org/abs/2306.11730v1",
        "pub_date": "2023-06-20",
        "summary": "In this study, we evaluate the performance of the Segment Anything Model\n(SAM) model in clinical radiotherapy. We collected real clinical cases from\nfour regions at the Mayo Clinic: prostate, lung, gastrointestinal, and head \\&amp;\nneck, which are typical treatment sites in radiation oncology. For each case,\nwe selected the OARs of concern in radiotherapy planning and compared the Dice\nand Jaccard outcomes between clinical manual delineation, automatic\nsegmentation using SAM's \"segment anything\" mode, and automatic segmentation\nusing SAM with box prompt. Our results indicate that SAM performs better in\nautomatic segmentation for the prostate and lung regions, while its performance\nin the gastrointestinal and head \\&amp; neck regions was relatively inferior. When\nconsidering the size of the organ and the clarity of its boundary, SAM displays\nbetter performance for larger organs with clear boundaries, such as the lung\nand liver, and worse for smaller organs with unclear boundaries, like the\nparotid and cochlea. These findings align with the generally accepted\nvariations in difficulty level associated with manual delineation of different\norgans at different sites in clinical radiotherapy. Given that SAM, a single\ntrained model, could handle the delineation of OARs in four regions, these\nresults also demonstrate SAM's robust generalization capabilities in automatic\nsegmentation for radiotherapy, i.e., achieving delineation of different\nradiotherapy OARs using a generic automatic segmentation model. SAM's\ngeneralization capabilities across different regions make it technically\nfeasible to develop a generic model for automatic segmentation in radiotherapy.",
        "translated": "在这项研究中，我们评估分段任意模型(SAM)在临床放射治疗中的性能。我们从梅奥诊所的四个地区收集了真实的临床病例: 前列腺、肺、胃肠道和头颈部，这是放射肿瘤学的典型治疗部位。对于每种情况，我们选择放疗计划中关注的 OAR，并比较临床手动描绘，使用 SAM 的“分割任何东西”模式的自动分割以及使用 SAM 和框提示的自动分割之间的 Dice 和 Jaccard 结果。我们的研究结果表明，SAM 在前列腺和肺部区域的自动分割方面表现较好，而在胃肠和头颈部区域的表现相对较差。当考虑器官的大小和其边界的清晰度时，SAM 对于边界清晰的较大器官(如肺和肝)表现出更好的性能，对于边界不清楚的较小器官(如腮腺和耳蜗)表现更差。这些发现与临床放射治疗中在不同部位手工描绘不同器官的难度水平的普遍接受的变化相一致。鉴于 SAM 是一个单一的训练模型，可以处理四个区域中 OAR 的描绘，这些结果也证明了 SAM 在放射治疗自动分割方面的强大推广能力，即使用通用的自动分割模型来实现不同放射治疗 OAR 的描绘。SAM 在不同区域的推广能力使得开发放射治疗中自动分割的通用模型在技术上是可行的。"
    },
    {
        "title": "Dense Video Object Captioning from Disjoint Supervision",
        "url": "http://arxiv.org/abs/2306.11729v1",
        "pub_date": "2023-06-20",
        "summary": "We propose a new task and model for dense video object captioning --\ndetecting, tracking, and captioning trajectories of all objects in a video.\nThis task unifies spatial and temporal understanding of the video, and requires\nfine-grained language description. Our model for dense video object captioning\nis trained end-to-end and consists of different modules for spatial\nlocalization, tracking, and captioning. As such, we can train our model with a\nmixture of disjoint tasks, and leverage diverse, large-scale datasets which\nsupervise different parts of our model. This results in noteworthy zero-shot\nperformance. Moreover, by finetuning a model from this initialization, we can\nfurther improve our performance, surpassing strong image-based baselines by a\nsignificant margin. Although we are not aware of other work performing this\ntask, we are able to repurpose existing video grounding datasets for our task,\nnamely VidSTG and VLN. We show our task is more general than grounding, and\nmodels trained on our task can directly be applied to grounding by finding the\nbounding box with the maximum likelihood of generating the query sentence. Our\nmodel outperforms dedicated, state-of-the-art models for spatial grounding on\nboth VidSTG and VLN.",
        "translated": "我们提出了一个新的任务和模型密集视频对象字幕-检测，跟踪和字幕轨迹的所有对象在视频。该任务将视频的时空理解统一起来，要求对视频进行细粒度的语言描述。我们的密集视频对象字幕模型是端到端训练的，由不同的空间定位、跟踪和字幕模块组成。因此，我们可以使用不相关任务的混合来训练模型，并利用不同的、大规模的数据集来监督模型的不同部分。这导致了值得注意的零射击性能。此外，通过从这个初始化微调模型，我们可以进一步提高我们的性能，大大超过强大的基于图像的基线。虽然我们不知道其他工作执行这项任务，我们能够重新利用现有的视频接地数据集为我们的任务，即 VidSTG 和 VLN。我们表明我们的任务是更一般的比接地，模型训练在我们的任务可以直接应用到接地，通过找到最大可能性生成查询句子的边界框。我们的模型在 VidSTG 和 VLN 上的空间接地性能优于专用的、最先进的模型。"
    },
    {
        "title": "How can objects help action recognition?",
        "url": "http://arxiv.org/abs/2306.11726v1",
        "pub_date": "2023-06-20",
        "summary": "Current state-of-the-art video models process a video clip as a long sequence\nof spatio-temporal tokens. However, they do not explicitly model objects, their\ninteractions across the video, and instead process all the tokens in the video.\nIn this paper, we investigate how we can use knowledge of objects to design\nbetter video models, namely to process fewer tokens and to improve recognition\naccuracy. This is in contrast to prior works which either drop tokens at the\ncost of accuracy, or increase accuracy whilst also increasing the computation\nrequired. First, we propose an object-guided token sampling strategy that\nenables us to retain a small fraction of the input tokens with minimal impact\non accuracy. And second, we propose an object-aware attention module that\nenriches our feature representation with object information and improves\noverall accuracy. Our resulting framework achieves better performance when\nusing fewer tokens than strong baselines. In particular, we match our baseline\nwith 30%, 40%, and 60% of the input tokens on SomethingElse,\nSomething-something v2, and Epic-Kitchens, respectively. When we use our model\nto process the same number of tokens as our baseline, we improve by 0.6 to 4.2\npoints on these datasets.",
        "translated": "当前最先进的视频模型将视频剪辑作为一长串时空令牌进行处理。但是，它们不显式地建模对象、它们在视频中的交互，而是处理视频中的所有令牌。本文研究了如何利用对象的知识来设计更好的视频模型，即处理更少的标记和提高识别准确率。这与以前的工作形成了鲜明的对比，以前的工作要么以牺牲精度为代价丢弃令牌，要么在提高精度的同时增加所需的计算量。首先，我们提出了一个对象引导的令牌采样策略，使我们能够保留输入令牌的一小部分，对准确性的影响最小。其次，提出了一种对象感知的注意模块，该模块充实了对象信息的特征表示，提高了整体的准确性。当使用的令牌比强基线少时，我们得到的框架获得了更好的性能。具体来说，我们分别使用30% 、40% 和60% 的输入令牌来匹配我们的基线; 这些令牌分别位于 Something Else、 Something-something v2和 Epic-Kitchens 上。当我们使用模型处理与基线相同数量的令牌时，我们在这些数据集上提高了0.6到4.2个点。"
    },
    {
        "title": "Low-complexity Multidimensional DCT Approximations",
        "url": "http://arxiv.org/abs/2306.11724v1",
        "pub_date": "2023-06-20",
        "summary": "In this paper, we introduce low-complexity multidimensional discrete cosine\ntransform (DCT) approximations. Three dimensional DCT (3D DCT) approximations\nare formalized in terms of high-order tensor theory. The formulation is\nextended to higher dimensions with arbitrary lengths. Several multiplierless\n$8\\times 8\\times 8$ approximate methods are proposed and the computational\ncomplexity is discussed for the general multidimensional case. The proposed\nmethods complexity cost was assessed, presenting considerably lower arithmetic\noperations when compared with the exact 3D DCT. The proposed approximations\nwere embedded into 3D DCT-based video coding scheme and a modified quantization\nstep was introduced. The simulation results showed that the approximate 3D DCT\ncoding methods offer almost identical output visual quality when compared with\nexact 3D DCT scheme. The proposed 3D approximations were also employed as a\ntool for visual tracking. The approximate 3D DCT-based proposed system performs\nsimilarly to the original exact 3D DCT-based method. In general, the suggested\nmethods showed competitive performance at a considerably lower computational\ncost.",
        "translated": "在这篇文章中，我们介绍了低复杂度的多维离散余弦变换(dCT)近似。利用高阶张量理论对三维 DCT (3D DCT)近似进行了形式化处理。将公式推广到具有任意长度的较高维数。提出了几种无乘法 $8乘以8乘以8的近似方法，并讨论了一般多维情况下的计算复杂性。提出的方法复杂度成本进行了评估，提出了相当低的算术运算相比，精确的三维离散余弦变换。在基于三维离散余弦变换(DCT)的视频编码方案中，引入了一种改进的量化步骤。仿真结果表明，与精确的三维 DCT 方案相比，近似的三维 DCT 编码方法提供了几乎相同的输出视觉质量。提出的三维近似也被用作视觉跟踪的工具。基于近似三维离散余弦变换的系统与基于精确三维离散余弦变换的方法性能相似。一般来说，建议的方法显示了竞争性能在一个相当低的计算成本。"
    },
    {
        "title": "Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized\n  Codebase",
        "url": "http://arxiv.org/abs/2306.12423v1",
        "pub_date": "2023-06-21",
        "summary": "Despite the rapid advance of 3D-aware image synthesis, existing studies\nusually adopt a mixture of techniques and tricks, leaving it unclear how each\npart contributes to the final performance in terms of generality. Following the\nmost popular and effective paradigm in this field, which incorporates a neural\nradiance field (NeRF) into the generator of a generative adversarial network\n(GAN), we build a well-structured codebase, dubbed Carver, through modularizing\nthe generation process. Such a design allows researchers to develop and replace\neach module independently, and hence offers an opportunity to fairly compare\nvarious approaches and recognize their contributions from the module\nperspective. The reproduction of a range of cutting-edge algorithms\ndemonstrates the availability of our modularized codebase. We also perform a\nvariety of in-depth analyses, such as the comparison across different types of\npoint feature, the necessity of the tailing upsampler in the generator, the\nreliance on the camera pose prior, etc., which deepen our understanding of\nexisting methods and point out some further directions of the research work. We\nrelease code and models at https://github.com/qiuyu96/Carver to facilitate the\ndevelopment and evaluation of this field.",
        "translated": "尽管3D 感知图像合成技术发展迅速，但现有的研究通常采用技术和技巧相结合的方法，因此不清楚每个部分如何在通用性方面对最终性能做出贡献。遵循这个领域中最流行和有效的范例，将神经辐射场(NeRF)合并到生成对抗网络(GAN)的生成器中，我们通过模块化生成过程建立了一个结构良好的代码库，称为 Carver。这种设计使研究人员能够独立地开发和替换每个模块，从而提供了一个公平地比较各种方法并从模块的角度认识它们的贡献的机会。一系列尖端算法的再现证明了我们的模块化代码库的可用性。我们还进行了各种深入的分析，如不同类型的点特征的比较，在生成器中尾随上采样器的必要性，对摄像机姿态的依赖，等等，这加深了我们对现有方法的理解，并指出了一些进一步的研究工作方向。我们 https://github.com/qiuyu96/carver 发布代码和模型，以促进这一领域的发展和评估。"
    },
    {
        "title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution",
        "url": "http://arxiv.org/abs/2306.12424v1",
        "pub_date": "2023-06-21",
        "summary": "We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related gender biases, inspired\nby Winograd and Winogender schemas, where each image is associated with a\ncaption containing a pronoun relationship of subjects and objects in the scene.\nVisoGender is balanced by gender representation in professional roles,\nsupporting bias evaluation in two ways: i) resolution bias, where we evaluate\nthe difference between gender resolution accuracies for men and women and ii)\nretrieval bias, where we compare ratios of male and female professionals\nretrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they lack the reasoning\nabilities to correctly resolve gender in complex scenes. While the direction\nand magnitude of gender bias depends on the task and the model being evaluated,\ncaptioning models generally are more accurate and less biased than CLIP-like\nmodels. Dataset and code are available at https://github.com/oxai/visogender",
        "translated": "我们介绍了视觉性别，一个新的数据集的基准性别偏见的视觉语言模型。我们重点关注与职业相关的性别偏见，灵感来自 Winograd 和 Winosex 模式，其中每个图像都与一个包含场景中主体和客体的代词关系的标题相关联。Viso 性别通过专业角色中的性别代表性来平衡，支持偏倚评估有两种方式: i)解析偏倚，其中我们评估男性和女性性别解析准确性之间的差异以及 ii)检索偏倚，其中我们比较检索的男性和女性专业人员的比例进行性别中立的搜索查询。我们基准的几个国家的最先进的视觉语言模型，发现他们缺乏推理能力，以正确解决性别在复杂的场景。虽然性别偏见的方向和程度取决于被评估的任务和模型，但字幕模型一般比 CLIP 类模型更准确，偏见更少。数据集和代码可在 https://github.com/oxai/visogender 下载"
    },
    {
        "title": "DreamTime: An Improved Optimization Strategy for Text-to-3D Content\n  Creation",
        "url": "http://arxiv.org/abs/2306.12422v1",
        "pub_date": "2023-06-21",
        "summary": "Text-to-image diffusion models pre-trained on billions of image-text pairs\nhave recently enabled text-to-3D content creation by optimizing a randomly\ninitialized Neural Radiance Fields (NeRF) with score distillation. However, the\nresultant 3D models exhibit two limitations: (a) quality concerns such as\nsaturated color and the Janus problem; (b) extremely low diversity comparing to\ntext-guided image synthesis. In this paper, we show that the conflict between\nNeRF optimization process and uniform timestep sampling in score distillation\nis the main reason for these limitations. To resolve this conflict, we propose\nto prioritize timestep sampling with monotonically non-increasing functions,\nwhich aligns NeRF optimization with the sampling process of diffusion model.\nExtensive experiments show that our simple redesign significantly improves\ntext-to-3D content creation with higher quality and diversity.",
        "translated": "在数十亿图像-文本对上预先训练的文本-图像扩散模型最近通过优化随机初始化的神经辐射场(NeRF)来实现文本到3D 内容的创建。然而，由此产生的三维模型表现出两个局限性: (a)质量问题，如饱和颜色和 Janus 问题; (b)与文本引导的图像合成相比，极低的多样性。本文指出，NERF 优化过程与均匀时间步长抽样在分数精馏中的冲突是造成这些局限性的主要原因。为了解决这一矛盾，我们提出了时间步长采样的优先级与单调非增长函数，调整 NERF 优化与扩散模型的采样过程。大量的实验表明，我们简单的重新设计显著提高了文本到3D 内容的创建，具有更高的质量和多样性。"
    },
    {
        "title": "Multi-Task Consistency for Active Learning",
        "url": "http://arxiv.org/abs/2306.12398v1",
        "pub_date": "2023-06-21",
        "summary": "Learning-based solutions for vision tasks require a large amount of labeled\ntraining data to ensure their performance and reliability. In single-task\nvision-based settings, inconsistency-based active learning has proven to be\neffective in selecting informative samples for annotation. However, there is a\nlack of research exploiting the inconsistency between multiple tasks in\nmulti-task networks. To address this gap, we propose a novel multi-task active\nlearning strategy for two coupled vision tasks: object detection and semantic\nsegmentation. Our approach leverages the inconsistency between them to identify\ninformative samples across both tasks. We propose three constraints that\nspecify how the tasks are coupled and introduce a method for determining the\npixels belonging to the object detected by a bounding box, to later quantify\nthe constraints as inconsistency scores. To evaluate the effectiveness of our\napproach, we establish multiple baselines for multi-task active learning and\nintroduce a new metric, mean Detection Segmentation Quality (mDSQ), tailored\nfor the multi-task active learning comparison that addresses the performance of\nboth tasks. We conduct extensive experiments on the nuImages and A9 datasets,\ndemonstrating that our approach outperforms existing state-of-the-art methods\nby up to 3.4% mDSQ on nuImages. Our approach achieves 95% of the fully-trained\nperformance using only 67% of the available data, corresponding to 20% fewer\nlabels compared to random selection and 5% fewer labels compared to\nstate-of-the-art selection strategy. Our code will be made publicly available\nafter the review process.",
        "translated": "基于学习的视觉任务解决方案需要大量的标记训练数据，以确保其性能和可靠性。在基于单任务视觉的环境中，基于不一致性的主动学习在选择信息样本进行注释时被证明是有效的。然而，针对多任务网络中多任务间不一致性的研究还很少。为了解决这个问题，我们提出了一个新的多任务主动学习策略，用于两个耦合的视觉任务: 目标检测和语义分割。我们的方法利用它们之间的不一致性来识别两个任务之间的信息样本。我们提出了三个约束，指定如何耦合的任务，并引入了一种方法来确定像素属于对象检测的边界框，以后量化的不一致性分数的约束。为了评估我们的方法的有效性，我们建立了多任务主动学习的多个基线，并引入了一个新的度量，平均检测分割质量(mDSQ) ，专为多任务主动学习比较，解决两个任务的性能。我们在 nuImages 和 A9数据集上进行了广泛的实验，证明我们的方法比现有的最先进的方法在 nuImages 上的性能高出3.4% mDSQ。我们的方法仅使用67% 的可用数据就实现了95% 的全训练性能，与随机选择相比，相应的标签减少了20% ，与最先进的选择策略相比，标签减少了5% 。我们的代码将在评审过程之后公开发布。"
    },
    {
        "title": "M-VAAL: Multimodal Variational Adversarial Active Learning for\n  Downstream Medical Image Analysis Tasks",
        "url": "http://arxiv.org/abs/2306.12376v1",
        "pub_date": "2023-06-21",
        "summary": "Acquiring properly annotated data is expensive in the medical field as it\nrequires experts, time-consuming protocols, and rigorous validation. Active\nlearning attempts to minimize the need for large annotated samples by actively\nsampling the most informative examples for annotation. These examples\ncontribute significantly to improving the performance of supervised machine\nlearning models, and thus, active learning can play an essential role in\nselecting the most appropriate information in deep learning-based diagnosis,\nclinical assessments, and treatment planning. Although some existing works have\nproposed methods for sampling the best examples for annotation in medical image\nanalysis, they are not task-agnostic and do not use multimodal auxiliary\ninformation in the sampler, which has the potential to increase robustness.\nTherefore, in this work, we propose a Multimodal Variational Adversarial Active\nLearning (M-VAAL) method that uses auxiliary information from additional\nmodalities to enhance the active sampling. We applied our method to two\ndatasets: i) brain tumor segmentation and multi-label classification using the\nBraTS2018 dataset, and ii) chest X-ray image classification using the\nCOVID-QU-Ex dataset. Our results show a promising direction toward\ndata-efficient learning under limited annotations.",
        "translated": "获取正确注释的数据在医学领域是昂贵的，因为它需要专家、耗时的协议和严格的验证。主动学习试图通过主动抽样最具信息量的注释示例来最小化对大型注释示例的需求。这些例子有助于提高监督式学习模型的性能，因此，在基于深度学习的诊断、临床评估和治疗计划中，主动学习可以在选择最合适的信息方面发挥重要作用。虽然现有的一些工作已经提出了医学图像分析中注释的最佳例子的抽样方法，但是它们不是任务不可知的，并且在采样器中不使用多模态辅助信息，这有可能提高鲁棒性。因此，在本研究中，我们提出一个多模态变分对抗主动学习(M-VAAL)方法，利用附加模态的辅助信息来增强主动抽样。我们将我们的方法应用于两个数据集: i)使用 BraTS2018数据集的脑肿瘤分割和多标记分类，以及 ii)使用 COVID-QU-Ex 数据集的胸部 X 射线图像分类。我们的研究结果为有限注释下的数据高效学习指明了一个有希望的方向。"
    },
    {
        "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale\n  From A New Perspective",
        "url": "http://arxiv.org/abs/2306.13092v1",
        "pub_date": "2023-06-22",
        "summary": "We present a new dataset condensation framework termed Squeeze, Recover and\nRelabel (SRe$^2$L) that decouples the bilevel optimization of model and\nsynthetic data during training, to handle varying scales of datasets, model\narchitectures and image resolutions for effective dataset condensation. The\nproposed method demonstrates flexibility across diverse dataset scales and\nexhibits multiple advantages in terms of arbitrary resolutions of synthesized\nimages, low training cost and memory consumption with high-resolution training,\nand the ability to scale up to arbitrary evaluation network architectures.\nExtensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K\ndatasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%\nvalidation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all\nprevious state-of-the-art methods by margins of 14.5% and 32.9%, respectively.\nOur approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and\n16$\\times$ (ResNet-18) faster in speed with less memory consumption of\n11.6$\\times$ and 6.4$\\times$ during data synthesis. Our code and condensed\ndatasets of 50, 200 IPC with 4K recovery budget are available at\nhttps://zeyuanyin.github.io/projects/SRe2L/.",
        "translated": "我们提出了一个新的数据集压缩框架，称为挤压，恢复和 Relabel (SRe $^ 2 $L) ，解耦模型和合成数据在训练期间的双层优化，以处理不同规模的数据集，模型架构和有效的数据集压缩图像分辨率。该方法在不同的数据集尺度上表现出灵活性，具有合成图像的任意分辨率、高分辨率训练的低训练成本和内存消耗以及扩展到任意评估网络结构的能力等优点。在 Tiny-ImageNet 和完整的 ImageNet-1K 数据集上进行了广泛的实验。在50 IPC 下，我们的方法在 Tiny-ImageNet 和 ImageNet-1K 上实现了最高的42.5% 和60.8% 的验证准确率，分别比以前所有最先进的方法高出14.5% 和32.9% 。我们的方法在速度上也比 MTT 快大约52美元乘以 $(ConverNet-4)和16美元乘以 $(ResNet-18) ，在数据合成期间内存消耗更少，分别为11.6美元乘以 $和6.4美元乘以 $。我们的代码和精简数据集50,200 IPC 与4k 恢复预算可在 https://zeyuanyin.github.io/projects/sre2l/。"
    },
    {
        "title": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial\n  Faces",
        "url": "http://arxiv.org/abs/2306.13091v1",
        "pub_date": "2023-06-22",
        "summary": "The ability of generative models to produce highly realistic synthetic face\nimages has raised security and ethical concerns. As a first line of defense\nagainst such fake faces, deep learning based forensic classifiers have been\ndeveloped. While these forensic models can detect whether a face image is\nsynthetic or real with high accuracy, they are also vulnerable to adversarial\nattacks. Although such attacks can be highly successful in evading detection by\nforensic classifiers, they introduce visible noise patterns that are detectable\nthrough careful human scrutiny. Additionally, these attacks assume access to\nthe target model(s) which may not always be true. Attempts have been made to\ndirectly perturb the latent space of GANs to produce adversarial fake faces\nthat can circumvent forensic classifiers. In this work, we go one step further\nand show that it is possible to successfully generate adversarial fake faces\nwith a specified set of attributes (e.g., hair color, eye size, race, gender,\netc.). To achieve this goal, we leverage the state-of-the-art generative model\nStyleGAN with disentangled representations, which enables a range of\nmodifications without leaving the manifold of natural images. We propose a\nframework to search for adversarial latent codes within the feature space of\nStyleGAN, where the search can be guided either by a text prompt or a reference\nimage. We also propose a meta-learning based optimization strategy to achieve\ntransferable performance on unknown target models. Extensive experiments\ndemonstrate that the proposed approach can produce semantically manipulated\nadversarial fake faces, which are true to the specified attribute set and can\nsuccessfully fool forensic face classifiers, while remaining undetectable by\nhumans. Code: https://github.com/koushiksrivats/face_attribute_attack.",
        "translated": "生成模型产生高度逼真的合成人脸图像的能力引起了安全和伦理方面的关注。作为抵御这种假面的第一道防线，基于深度学习的法医分类器已经被开发出来。虽然这些法医模型能够高精度地检测出人脸图像是合成的还是真实的，但它们也容易受到敌对攻击。虽然这种攻击可以非常成功地躲避法医分类器的检测，但是它们引入了可见的噪音模式，通过仔细的人类检查可以检测到。此外，这些攻击假设对目标模型的访问，这可能并不总是正确的。有人试图直接扰乱 GAN 的潜在空间，制造出可以绕过法医分类器的敌对假面孔。在这项工作中，我们更进一步，表明有可能成功地生成具有特定属性集(例如，头发颜色，眼睛大小，种族，性别等)的敌对假面孔。为了实现这个目标，我们利用了最先进的生成模型 StyleGAN，它可以进行一系列的修改，而不用离开自然图像的流形。提出了一种在 StyleGAN 特征空间内搜索对手潜码的框架，该框架可以通过文本提示或参考图像来指导搜索。我们还提出了一种基于元学习的优化策略，以实现对未知目标模型的可转移性能。大量的实验表明，该方法可以产生语义操纵的敌对假面孔，这是真实的特定属性集，可以成功地欺骗法医面孔分类器，而仍然无法被人类检测到。密码:  https://github.com/koushiksrivats/face_attribute_attack。"
    },
    {
        "title": "PromptIR: Prompting for All-in-One Blind Image Restoration",
        "url": "http://arxiv.org/abs/2306.13090v1",
        "pub_date": "2023-06-22",
        "summary": "Image restoration involves recovering a high-quality clean image from its\ndegraded version. Deep learning-based methods have significantly improved image\nrestoration performance, however, they have limited generalization ability to\ndifferent degradation types and levels. This restricts their real-world\napplication since it requires training individual models for each specific\ndegradation and knowing the input degradation type to apply the relevant model.\nWe present a prompt-based learning approach, PromptIR, for All-In-One image\nrestoration that can effectively restore images from various types and levels\nof degradation. In particular, our method uses prompts to encode\ndegradation-specific information, which is then used to dynamically guide the\nrestoration network. This allows our method to generalize to different\ndegradation types and levels, while still achieving state-of-the-art results on\nimage denoising, deraining, and dehazing. Overall, PromptIR offers a generic\nand efficient plugin module with few lightweight prompts that can be used to\nrestore images of various types and levels of degradation with no prior\ninformation on the corruptions present in the image. Our code and pretrained\nmodels are available here: https://github.com/va1shn9v/PromptIR",
        "translated": "影像复原包括从降级版本中恢复高质量的干净图像。基于深度学习的方法显著改善了影像复原的表现，但是，它们对不同退化类型和水平的推广能力有限。这限制了它们的实际应用程序，因为它需要为每个特定的降级训练单个模型，并了解应用相关模型的输入降级类型。我们提出了一种基于提示的学习方法，PromptIR，用于一体化影像复原，可以有效地从不同类型和不同程度的退化中恢复图像。特别是，我们的方法使用提示来编码特定于退化的信息，然后使用这些信息动态地指导恢复网络。这使得我们的方法可以推广到不同的退化类型和水平，同时仍然实现国家的最先进的图像去噪，去除和去灰度的结果。总的来说，PromptIR 提供了一个通用的、高效的插件模块，只有很少的轻量级提示，可以用来恢复各种类型和级别的退化图像，而不需要事先了解图像中存在的损坏信息。我们的代码和预先训练的模型可以在这里找到:  https://github.com/va1shn9v/promptir"
    },
    {
        "title": "Continuous Layout Editing of Single Images with Diffusion Models",
        "url": "http://arxiv.org/abs/2306.13078v1",
        "pub_date": "2023-06-22",
        "summary": "Recent advancements in large-scale text-to-image diffusion models have\nenabled many applications in image editing. However, none of these methods have\nbeen able to edit the layout of single existing images. To address this gap, we\npropose the first framework for layout editing of a single image while\npreserving its visual properties, thus allowing for continuous editing on a\nsingle image. Our approach is achieved through two key modules. First, to\npreserve the characteristics of multiple objects within an image, we\ndisentangle the concepts of different objects and embed them into separate\ntextual tokens using a novel method called masked textual inversion. Next, we\npropose a training-free optimization method to perform layout control for a\npre-trained diffusion model, which allows us to regenerate images with learned\nconcepts and align them with user-specified layouts. As the first framework to\nedit the layout of existing images, we demonstrate that our method is effective\nand outperforms other baselines that were modified to support this task. Our\ncode will be freely available for public use upon acceptance.",
        "translated": "大规模文本-图像扩散模型的最新进展使图像编辑中的许多应用成为可能。但是，这些方法都不能编辑单个现有图像的布局。为了解决这个问题，我们提出了第一个框架，用于编辑单个图像的布局，同时保留其视觉属性，从而允许对单个图像进行连续编辑。我们的方法是通过两个关键模块实现的。首先，为了保持图像中多个对象的特征，我们将不同对象的概念进行分离，并使用一种新的方法——掩蔽文本反转，将它们嵌入到单独的文本标记中。接下来，我们提出了一种无需训练的优化方法来执行预先训练的扩散模型的布局控制，使我们能够重新生成图像与学习的概念，并与用户指定的布局对齐。作为编辑现有图像布局的第一个框架，我们证明了我们的方法是有效的，并且优于为支持此任务而修改的其他基线。我们的代码将免费供公众使用后接受。"
    },
    {
        "title": "Semi-automated extraction of research topics and trends from NCI funding\n  in radiological sciences from 2000-2020",
        "url": "http://arxiv.org/abs/2306.13075v1",
        "pub_date": "2023-06-22",
        "summary": "Investigators, funders, and the public desire knowledge on topics and trends\nin publicly funded research but current efforts in manual categorization are\nlimited in scale and understanding. We developed a semi-automated approach to\nextract and name research topics, and applied this to \\$1.9B of NCI funding\nover 21 years in the radiological sciences to determine micro- and macro-scale\nresearch topics and funding trends. Our method relies on sequential clustering\nof existing biomedical-based word embeddings, naming using subject matter\nexperts, and visualization to discover trends at a macroscopic scale above\nindividual topics. We present results using 15 and 60 cluster topics, where we\nfound that 2D projection of grant embeddings reveals two dominant axes:\nphysics-biology and therapeutic-diagnostic. For our dataset, we found that\nfunding for therapeutics- and physics-based research have outpaced diagnostics-\nand biology-based research, respectively. We hope these results may (1) give\ninsight to funders on the appropriateness of their funding allocation, (2)\nassist investigators in contextualizing their work and explore neighboring\nresearch domains, and (3) allow the public to review where their tax dollars\nare being allocated.",
        "translated": "研究人员、资助者和公众希望了解公共资助研究的主题和趋势，但目前在人工分类方面的努力在规模和理解方面受到限制。我们开发了一种半自动化的方法来提取和命名研究主题，并将其应用于21年来 NCI 在放射科学领域的19亿美元资金，以确定微观和宏观研究主题和资金趋势。我们的方法依赖于现有的基于生物医学的单词嵌入的顺序聚类，使用主题专家命名，以及可视化来发现在个别主题之上的宏观趋势。我们使用15和60个聚类主题展示结果，其中我们发现赠款嵌入的二维投影揭示了两个主要轴: 物理-生物学和治疗-诊断。对于我们的数据集，我们发现基于治疗学和物理学的研究经费分别超过了基于诊断学和基于生物学的研究。我们希望这些结果可以(1)让资助者了解他们资金分配的适当性，(2)帮助调查人员将他们的工作背景化，并探索相邻的研究领域，(3)允许公众审查他们的税款分配在哪里。"
    },
    {
        "title": "ProRes: Exploring Degradation-aware Visual Prompt for Universal Image\n  Restoration",
        "url": "http://arxiv.org/abs/2306.13653v1",
        "pub_date": "2023-06-23",
        "summary": "Image restoration aims to reconstruct degraded images, e.g., denoising or\ndeblurring. Existing works focus on designing task-specific methods and there\nare inadequate attempts at universal methods. However, simply unifying multiple\ntasks into one universal architecture suffers from uncontrollable and undesired\npredictions. To address those issues, we explore prompt learning in universal\narchitectures for image restoration tasks. In this paper, we present\nDegradation-aware Visual Prompts, which encode various types of image\ndegradation, e.g., noise and blur, into unified visual prompts. These\ndegradation-aware prompts provide control over image processing and allow\nweighted combinations for customized image restoration. We then leverage\ndegradation-aware visual prompts to establish a controllable and universal\nmodel for image restoration, called ProRes, which is applicable to an extensive\nrange of image restoration tasks. ProRes leverages the vanilla Vision\nTransformer (ViT) without any task-specific designs. Furthermore, the\npre-trained ProRes can easily adapt to new tasks through efficient prompt\ntuning with only a few images. Without bells and whistles, ProRes achieves\ncompetitive performance compared to task-specific methods and experiments can\ndemonstrate its ability for controllable restoration and adaptation for new\ntasks. The code and models will be released in\n\\url{https://github.com/leonmakise/ProRes}.",
        "translated": "影像复原的目的是重建退化的图像，例如去噪或去模糊。现有的工作侧重于设计特定任务的方法，对普遍方法的尝试不够。然而，简单地将多个任务统一到一个通用体系结构中，就会遭受无法控制和意想不到的预测。为了解决这些问题，我们探索了在通用架构中快速学习影像复原任务的方法。在本文中，我们提出了退化感知视觉提示，它编码各种类型的图像退化，如噪声和模糊，成为统一的视觉提示。这些降解感知提示提供对图像处理的控制，并允许为定制的影像复原进行加权组合。然后，我们利用降级感知的视觉提示，为影像复原建立一个可控的、通用的模型，称为 proRes，它适用于范围广泛的影像复原任务。ProRes 利用了普通的 Vision Transformer (ViT) ，没有任何特定于任务的设计。此外，预先训练的 ProRes 可以很容易地适应新的任务，通过有效的迅速调整，只有少数图像。没有花哨，ProRes 实现竞争性能相比，任务特定的方法和实验可以证明它的能力，可控恢复和适应新的任务。代码和模型将在 url { https://github.com/leonmakise/prores }中发布。"
    },
    {
        "title": "LightGlue: Local Feature Matching at Light Speed",
        "url": "http://arxiv.org/abs/2306.13643v1",
        "pub_date": "2023-06-23",
        "summary": "We introduce LightGlue, a deep neural network that learns to match local\nfeatures across images. We revisit multiple design decisions of SuperGlue, the\nstate of the art in sparse matching, and derive simple but effective\nimprovements. Cumulatively, they make LightGlue more efficient - in terms of\nboth memory and computation, more accurate, and much easier to train. One key\nproperty is that LightGlue is adaptive to the difficulty of the problem: the\ninference is much faster on image pairs that are intuitively easy to match, for\nexample because of a larger visual overlap or limited appearance change. This\nopens up exciting prospects for deploying deep matchers in latency-sensitive\napplications like 3D reconstruction. The code and trained models are publicly\navailable at https://github.com/cvg/LightGlue.",
        "translated": "我们介绍 LightGlue，一个深度神经网络，学习匹配图像中的局部特征。我们重新审视 SuperGlue 的多重设计决策，稀疏匹配的最新技术，并得出简单而有效的改进。累积起来，它们使 LightGlue 更加高效——在内存和计算方面，更加精确，更容易训练。一个关键特性是 LightGlue 能够适应问题的难度: 对于直观上容易匹配的图像对，比如由于较大的视觉重叠或有限的外观变化，推断速度要快得多。这为在3D 重建等对延迟敏感的应用程序中部署深度匹配器开辟了令人兴奋的前景。代码和经过培训的模型可在 https://github.com/cvg/lightglue 公开查阅。"
    },
    {
        "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
        "url": "http://arxiv.org/abs/2306.13631v1",
        "pub_date": "2023-06-23",
        "summary": "We introduce the task of open-vocabulary 3D instance segmentation.\nTraditional approaches for 3D instance segmentation largely rely on existing 3D\nannotated datasets, which are restricted to a closed-set of object categories.\nThis is an important limitation for real-life applications where one might need\nto perform tasks guided by novel, open-vocabulary queries related to objects\nfrom a wide variety. Recently, open-vocabulary 3D scene understanding methods\nhave emerged to address this problem by learning queryable features per each\npoint in the scene. While such a representation can be directly employed to\nperform semantic segmentation, existing methods have limitations in their\nability to identify object instances. In this work, we address this limitation,\nand propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D\ninstance segmentation. Guided by predicted class-agnostic 3D instance masks,\nour model aggregates per-mask features via multi-view fusion of CLIP-based\nimage embeddings. We conduct experiments and ablation studies on the ScanNet200\ndataset to evaluate the performance of OpenMask3D, and provide insights about\nthe open-vocabulary 3D instance segmentation task. We show that our approach\noutperforms other open-vocabulary counterparts, particularly on the long-tail\ndistribution. Furthermore, OpenMask3D goes beyond the limitations of\nclose-vocabulary approaches, and enables the segmentation of object instances\nbased on free-form queries describing object properties such as semantics,\ngeometry, affordances, and material properties.",
        "translated": "介绍了开放式词汇表三维实例分割的任务。传统的三维实例分割方法主要依赖于现有的三维注释数据集，这些数据集仅限于一组封闭的对象类别。对于实际应用程序来说，这是一个重要的限制，因为在实际应用程序中，人们可能需要执行由与各种对象相关的新颖的开放词汇表查询引导的任务。最近，开放词汇表的3D 场景理解方法已经出现，以解决这个问题，学习每个场景中的点可查询特征。虽然这样的表示可以直接用于执行语义分割，但是现有的方法在识别对象实例的能力方面存在局限性。在这项工作中，我们针对这个限制，并提出了 OpenMask3D，这是一个开放式词汇表3D 实例分割的零镜头方法。在预测的类无关3D 实例掩码的指导下，我们的模型通过基于 CLIP 的图像嵌入的多视图融合来聚集每个掩码的特征。本文对 ScanNet200数据集进行了实验和烧蚀研究，以评价 OpenMask3D 的性能，并对开放词汇表的3D 实例分割任务提供了见解。我们展示了我们的方法优于其他开放词汇表方法，特别是在长尾分布方面。此外，OpenMask3D 超越了封闭词汇表方法的局限性，支持基于自由形式查询的对象实例分割，这些查询描述对象属性，如语义、几何、可视化和材料属性。"
    },
    {
        "title": "Machine Learning methods for simulating particle response in the Zero\n  Degree Calorimeter at the ALICE experiment, CERN",
        "url": "http://arxiv.org/abs/2306.13606v1",
        "pub_date": "2023-06-23",
        "summary": "Currently, over half of the computing power at CERN GRID is used to run High\nEnergy Physics simulations. The recent updates at the Large Hadron Collider\n(LHC) create the need for developing more efficient simulation methods. In\nparticular, there exists a demand for a fast simulation of the neutron Zero\nDegree Calorimeter, where existing Monte Carlo-based methods impose a\nsignificant computational burden. We propose an alternative approach to the\nproblem that leverages machine learning. Our solution utilises neural network\nclassifiers and generative models to directly simulate the response of the\ncalorimeter. In particular, we examine the performance of variational\nautoencoders and generative adversarial networks, expanding the GAN\narchitecture by an additional regularisation network and a simple, yet\neffective postprocessing step. Our approach increases the simulation speed by 2\norders of magnitude while maintaining the high fidelity of the simulation.",
        "translated": "目前，CERN GRID 超过一半的计算能力用于运行高能物理模拟。最近在大型强子对撞机(LHC)的更新创造了开发更有效的模拟方法的需要。特别是对中子零度量热计的快速模拟，现有的基于蒙特卡罗的方法给计算带来了很大的负担。我们提出了一种利用机器学习来解决这个问题的替代方法。我们的解决方案利用神经网络分类器和生成模型来直接模拟热量计的响应。特别是，我们检查的性能变化自动编码器和生成对抗性网络，扩大了 GAN 架构的一个额外的正规化网络和一个简单，但有效的后处理步骤。我们的方法将模拟速度提高了2数量级，同时保持了模拟的高保真度。"
    },
    {
        "title": "A Semi-Paired Approach For Label-to-Image Translation",
        "url": "http://arxiv.org/abs/2306.13585v1",
        "pub_date": "2023-06-23",
        "summary": "Data efficiency, or the ability to generalize from a few labeled data,\nremains a major challenge in deep learning. Semi-supervised learning has\nthrived in traditional recognition tasks alleviating the need for large amounts\nof labeled data, yet it remains understudied in image-to-image translation\n(I2I) tasks. In this work, we introduce the first semi-supervised (semi-paired)\nframework for label-to-image translation, a challenging subtask of I2I which\ngenerates photorealistic images from semantic label maps. In the semi-paired\nsetting, the model has access to a small set of paired data and a larger set of\nunpaired images and labels. Instead of using geometrical transformations as a\npretext task like previous works, we leverage an input reconstruction task by\nexploiting the conditional discriminator on the paired data as a reverse\ngenerator. We propose a training algorithm for this shared network, and we\npresent a rare classes sampling algorithm to focus on under-represented\nclasses. Experiments on 3 standard benchmarks show that the proposed model\noutperforms state-of-the-art unsupervised and semi-supervised approaches, as\nwell as some fully supervised approaches while using a much smaller number of\npaired samples.",
        "translated": "数据效率，或者说从一些标记的数据中归纳出来的能力，仍然是深度学习的一个主要挑战。半监督学习在传统的识别任务中蓬勃发展，减少了对大量标记数据的需求，但在图像到图像的转换(i2I)任务中仍然没有得到充分的研究。在这项工作中，我们介绍了第一个半监督(半配对)框架的标签到图像的转换，一个具有挑战性的子任务 I2I，生成真实感图像的语义标签映射。在半成对设置中，模型可以访问一小组成对的数据和一大组不成对的图像和标签。我们不像以前的工作那样使用几何变换作为借口，而是通过利用配对数据上的条件鉴别器作为反向生成器来利用输入重构任务。我们提出了一个训练算法的共享网络，并提出了一个罕见的类抽样算法，重点是代表性不足的类。在3个标准基准上的实验表明，所提出的模型优于最先进的无监督和半监督方法，以及一些全监督方法，同时使用了少得多的配对样本。"
    },
    {
        "title": "FunQA: Towards Surprising Video Comprehension",
        "url": "http://arxiv.org/abs/2306.14899v1",
        "pub_date": "2023-06-26",
        "summary": "Surprising videos, e.g., funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video, and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Extensive experiments\nwith existing VideoQA models reveal significant performance gaps for the FunQA\nvideos across spatial-temporal reasoning, visual-centered reasoning, and\nfree-text generation.",
        "translated": "令人惊讶的视频，例如，有趣的片段，创造性的表演，或视觉幻觉，吸引了大量的注意力。欣赏这些视频不仅仅是对视觉刺激的反应，而是取决于人类理解(和欣赏)这些视频中描述的违反常识的行为的能力。我们介绍 FunQA，一个具有挑战性的视频问题回答(QA)数据集，专门设计来评估和增强基于反直观和有趣的视频的视频推理深度。与大多数视频 QA 基准不同，FunQA 基准侧重于不那么令人惊讶的背景，例如烹饪或教学视频，FunQA 涵盖了三种以前未探索过的令人惊讶的视频类型: 1) HumorQA，2) CreativeQA，和3) MagicQA。对于每个子集，我们建立严格的 QA 任务，旨在评估模型在反直觉时间戳定位、详细视频描述和反直觉推理方面的能力。我们还提出了更高层次的任务，比如为视频赋予一个合适的、生动的标题，以及对视频创造性进行评分。FunQA 基准测试总共由312K 自由文本 QA 对组成，这些 QA 对来自4.3 K 视频剪辑，跨越总共24个视频小时。对现有 VideoQA 模型的大量实验表明，FunQA 视频在时空推理、以视觉为中心的推理和自由文本生成方面存在显著的性能差距。"
    },
    {
        "title": "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
        "url": "http://arxiv.org/abs/2306.14895v1",
        "pub_date": "2023-06-26",
        "summary": "This tutorial note summarizes the presentation on ``Large Multimodal Models:\nTowards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023\ntutorial on ``Recent Advances in Vision Foundation Models''. The tutorial\nconsists of three parts. We first introduce the background on recent GPT-like\nlarge models for vision-and-language modeling to motivate the research in\ninstruction-tuned large multimodal models (LMMs). As a pre-requisite, we\ndescribe the basics of instruction-tuning in large language models, which is\nfurther extended to the multimodal space. Lastly, we illustrate how to build\nthe minimum prototype of multimodal GPT-4 like models with the open-source\nresource, and review the recently emerged topics.",
        "translated": "本教程说明总结了“大型多模态模型: 构建和超越多模态 GPT-4”的演示文稿，这是 CVPR 2023教程“视觉基础模型的最新进展”的一部分。本教程由三部分组成。我们首先介绍了最近的类 GPT 的视觉和语言建模大模型的背景，以激励教学调整的大型多模态模型(LMM)的研究。作为一个先决条件，我们描述了大型语言模型中指令调优的基础知识，并将其进一步扩展到多模态空间。最后，我们阐述了如何利用开源资源构建类似 GPT-4的多模式模型的最小原型，并回顾了最近出现的一些课题。"
    },
    {
        "title": "RVT: Robotic View Transformer for 3D Object Manipulation",
        "url": "http://arxiv.org/abs/2306.14896v1",
        "pub_date": "2023-06-26",
        "summary": "For 3D object manipulation, methods that build an explicit 3D representation\nperform better than those relying only on camera images. But using explicit 3D\nrepresentations like voxels comes at large computing cost, adversely affecting\nscalability. In this work, we propose RVT, a multi-view transformer for 3D\nmanipulation that is both scalable and accurate. Some key features of RVT are\nan attention mechanism to aggregate information across views and re-rendering\nof the camera input from virtual views around the robot workspace. In\nsimulations, we find that a single RVT model works well across 18 RLBench tasks\nwith 249 task variations, achieving 26% higher relative success than the\nexisting state-of-the-art method (PerAct). It also trains 36X faster than\nPerAct for achieving the same performance and achieves 2.3X the inference speed\nof PerAct. Further, RVT can perform a variety of manipulation tasks in the real\nworld with just a few ($\\sim$10) demonstrations per task. Visual results, code,\nand trained model are provided at https://robotic-view-transformer.github.io/.",
        "translated": "对于3D 对象操作，建立一个明确的3D 表示的方法比那些仅仅依赖于摄像机图像的方法表现得更好。但是使用像体素这样的显式3D 表示会带来巨大的计算成本，对可伸缩性造成不利影响。在这项工作中，我们提出了 RVT，一个多视图转换器的三维操作，既可扩展和准确。RVT 的一些关键特性是一种注意机制，可以聚合视图间的信息，并重新呈现机器人工作空间周围的虚拟视图中的摄像机输入。在模拟中，我们发现一个单一的 RVT 模型在18个 RLBench 任务中运行良好，有249个任务变化，比现有的最先进的方法(PerAct)相对成功率高出26% 。为了达到相同的性能，它还以36倍于 PerAct 的速度进行训练，并达到了2.3倍于 PerAct 的推理速度。此外，RVT 可以在现实世界中执行各种操作任务，每个任务只需几个演示($sim $10)。可视化结果、代码和经过训练的模型在 https://robotic-view-transformer.github.io/中提供。"
    },
    {
        "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied\n  to Facial Image Correction",
        "url": "http://arxiv.org/abs/2306.14891v1",
        "pub_date": "2023-06-26",
        "summary": "Image diffusion has recently shown remarkable performance in image synthesis\nand implicitly as an image prior. Such a prior has been used with conditioning\nto solve the inpainting problem, but only supporting binary user-based\nconditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion\npriors can be exploited with controllable strength. Our fuzzy conditioning can\nbe applied pixel-wise, enabling the modification of different image components\nto varying degrees. Additionally, we propose an application to facial image\ncorrection, where we combine our fuzzy-conditioned diffusion with\ndiffusion-derived attention maps. Our map estimates the degree of anomaly, and\nwe obtain it by projecting on the diffusion space. We show how our approach\nalso leads to interpretable and autonomous facial image correction.",
        "translated": "图像扩散近年来在图像合成方面表现出了显著的性能，并且隐含地作为一种图像先验。这样的先验已被用来解决修补问题的条件，但只支持二进制用户基于条件。我们得到了一个模糊条件扩散，其中隐式扩散先验可以利用可控的强度。我们的模糊调节可以应用像素级，使不同的图像组件修改到不同的程度。此外，我们提出了一个应用于面部图像校正，其中我们结合我们的模糊条件扩散和扩散导出的注意力映射。我们的映射估计了异常的程度，并且通过在扩散空间上的投影得到了它。我们展示了我们的方法如何也导致可解释和自主的面部图像校正。"
    },
    {
        "title": "Domain-Scalable Unpaired Image Translation via Latent Space Anchoring",
        "url": "http://arxiv.org/abs/2306.14879v1",
        "pub_date": "2023-06-26",
        "summary": "Unpaired image-to-image translation (UNIT) aims to map images between two\nvisual domains without paired training data. However, given a UNIT model\ntrained on certain domains, it is difficult for current methods to incorporate\nnew domains because they often need to train the full model on both existing\nand new domains. To address this problem, we propose a new domain-scalable UNIT\nmethod, termed as latent space anchoring, which can be efficiently extended to\nnew visual domains and does not need to fine-tune encoders and decoders of\nexisting domains. Our method anchors images of different domains to the same\nlatent space of frozen GANs by learning lightweight encoder and regressor\nmodels to reconstruct single-domain images. In the inference phase, the learned\nencoders and decoders of different domains can be arbitrarily combined to\ntranslate images between any two domains without fine-tuning. Experiments on\nvarious datasets show that the proposed method achieves superior performance on\nboth standard and domain-scalable UNIT tasks in comparison with the\nstate-of-the-art methods.",
        "translated": "不成对图像到图像转换(UNIT)的目的是映射图像之间的两个视觉领域没有成对的训练数据。然而，鉴于联合国信息和通信技术部的模式是在某些领域进行培训的，目前的方法很难纳入新的领域，因为它们往往需要在现有领域和新的领域对整个模式进行培训。为了解决这个问题，我们提出了一种新的域可伸缩的 UNIT 方法，称为潜在空间锚定，它可以有效地扩展到新的可视域，不需要对现有域的编码器和解码器进行微调。该方法通过学习轻量级编码器和回归模型来重建单域图像，将不同域的图像锚定在冻结 GAN 的相同潜伏空间上。在推理阶段，不同域的学习编码器和解码器可以任意组合，在任意两个域之间进行图像转换，而不需要进行微调。在不同数据集上的实验表明，该方法在标准和领域可扩展的 UNIT 任务上都比现有的方法具有更好的性能。"
    },
    {
        "title": "Symphonize 3D Semantic Scene Completion with Contextual Instance Queries",
        "url": "http://arxiv.org/abs/2306.15670v1",
        "pub_date": "2023-06-27",
        "summary": "3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal task\nfor autonomous driving, as it involves predicting per-voxel occupancy within a\n3D scene from partial LiDAR or image inputs. Existing methods primarily focus\non the voxel-wise feature aggregation, while neglecting the instance-centric\nsemantics and broader context. In this paper, we present a novel paradigm\ntermed Symphonies (Scene-from-Insts) for SSC, which completes the scene volume\nfrom a sparse set of instance queries derived from the input with context\nawareness. By incorporating the queries as the instance feature representations\nwithin the scene, Symphonies dynamically encodes the instance-centric semantics\nto interact with the image and volume features while avoiding the dense\nvoxel-wise modeling. Simultaneously, it orchestrates a more comprehensive\nunderstanding of the scenario by capturing context throughout the entire scene,\ncontributing to alleviating the geometric ambiguity derived from occlusion and\nperspective errors. Symphonies achieves a state-of-the-art result of 13.02 mIoU\non the challenging SemanticKITTI dataset, outperforming existing methods and\nshowcasing the promising advancements of the paradigm. The code is available at\n\\url{https://github.com/hustvl/Symphonies}.",
        "translated": "3D 语义场景完成(SSC)已经成为自主驾驶的一个新兴而关键的任务，因为它涉及到从部分激光雷达或图像输入预测3D 场景中每个体素的占有率。现有的方法主要集中于体素特征聚合，而忽略了以实例为中心的语义和更广泛的上下文。在本文中，我们提出了一个新的范例称为交响乐(场景从实际情况)的 SSC，它完成了场景卷从稀疏的实例查询集的输入与上下文感知。通过将查询合并为场景中的实例特征表示，Symphony 动态地编码以实例为中心的语义，以便与图像和卷特征交互，同时避免密集的体素建模。同时，它通过捕捉整个场景的上下文来协调对场景的更全面的理解，有助于减轻由遮挡和透视错误导致的几何模糊性。在具有挑战性的 SemanticKITTI 数据集上，交响乐达到了13.02 mIoU 的最高水平，超过了现有的方法，并展示了该范式的有希望的进步。该代码可在 url { https://github.com/hustvl/symphonies }获得。"
    },
    {
        "title": "Detector-Free Structure from Motion",
        "url": "http://arxiv.org/abs/2306.15669v1",
        "pub_date": "2023-06-27",
        "summary": "We propose a new structure-from-motion framework to recover accurate camera\nposes and point clouds from unordered images. Traditional SfM systems typically\nrely on the successful detection of repeatable keypoints across multiple views\nas the first step, which is difficult for texture-poor scenes, and poor\nkeypoint detection may break down the whole SfM system. We propose a new\ndetector-free SfM framework to draw benefits from the recent success of\ndetector-free matchers to avoid the early determination of keypoints, while\nsolving the multi-view inconsistency issue of detector-free matchers.\nSpecifically, our framework first reconstructs a coarse SfM model from\nquantized detector-free matches. Then, it refines the model by a novel\niterative refinement pipeline, which iterates between an attention-based\nmulti-view matching module to refine feature tracks and a geometry refinement\nmodule to improve the reconstruction accuracy. Experiments demonstrate that the\nproposed framework outperforms existing detector-based SfM systems on common\nbenchmark datasets. We also collect a texture-poor SfM dataset to demonstrate\nthe capability of our framework to reconstruct texture-poor scenes. Based on\nthis framework, we take $\\textit{first place}$ in Image Matching Challenge\n2023.",
        "translated": "我们提出了一种新的运动结构框架，以恢复准确的相机姿态和点云从无序的图像。传统的 SfM 系统通常依赖于多视图间可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是很困难的，而较差的关键点检测可能会破坏整个 SfM 系统。我们提出了一种新的无检测器 SfM 框架，借鉴无检测器匹配器的最新成功经验，避免了关键点的早期确定，同时解决了无检测器匹配器的多视图不一致性问题。具体来说，我们的框架首先从量化的无检测器匹配重构粗 SfM 模型。然后，采用一种新的迭代细化流水线对模型进行细化，在基于注意的多视点匹配模块和几何细化模块之间进行迭代来细化特征轨迹，以提高重建精度。实验结果表明，该框架的性能优于现有的基于检测器的 SfM 系统的通用基准数据集。我们还收集了一个缺乏纹理的 SfM 数据集，以证明我们的框架重建缺乏纹理的场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得了 $textit { first place } $。"
    },
    {
        "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online\n  Inference of Different Physical Properties",
        "url": "http://arxiv.org/abs/2306.15668v1",
        "pub_date": "2023-06-27",
        "summary": "General physical scene understanding requires more than simply localizing and\nrecognizing objects -- it requires knowledge that objects can have different\nlatent properties (e.g., mass or elasticity), and that those properties affect\nthe outcome of physical events. While there has been great progress in physical\nand video prediction models in recent years, benchmarks to test their\nperformance typically do not require an understanding that objects have\nindividual physical properties, or at best test only those properties that are\ndirectly observable (e.g., size or color). This work proposes a novel dataset\nand benchmark, termed Physion++, that rigorously evaluates visual physical\nprediction in artificial systems under circumstances where those predictions\nrely on accurate estimates of the latent physical properties of objects in the\nscene. Specifically, we test scenarios where accurate prediction relies on\nestimates of properties such as mass, friction, elasticity, and deformability,\nand where the values of those properties can only be inferred by observing how\nobjects move and interact with other objects or fluids. We evaluate the\nperformance of a number of state-of-the-art prediction models that span a\nvariety of levels of learning vs. built-in knowledge, and compare that\nperformance to a set of human predictions. We find that models that have been\ntrained using standard regimes and datasets do not spontaneously learn to make\ninferences about latent properties, but also that models that encode objectness\nand physical states tend to make better predictions. However, there is still a\nhuge gap between all models and human performance, and all models' predictions\ncorrelate poorly with those made by humans, suggesting that no state-of-the-art\nmodel is learning to make physical predictions in a human-like way. Project\npage: https://dingmyu.github.io/physion_v2/",
        "translated": "一般的物理场景理解需要的不仅仅是简单的定位和识别对象——它需要知道对象可以有不同的潜在属性(例如，质量或弹性) ，并且这些属性影响物理事件的结果。虽然近年来在物理和视频预测模型方面取得了很大的进步，但是测试其性能的基准通常并不需要了解对象具有单独的物理属性，或者充其量只测试那些可直接观察到的属性(例如，大小或颜色)。这项工作提出了一个新的数据集和基准，称为 Physion + + ，严格评估视觉物理预测在人工系统的情况下，这些预测依赖于对场景中物体的潜在物理特性的准确估计。具体来说，我们测试的情况下，准确的预测依赖于估计的属性，如质量，摩擦力，弹性和变形性，这些属性的值只能通过观察物体如何移动和与其他物体或流体相互作用来推断。我们评估了一些最先进的预测模型的表现，这些模型跨越了不同的学习水平和内置知识，并将这些表现与一组人类预测进行比较。我们发现，使用标准体系和数据集训练过的模型不会自发地学会对潜在属性做出推断，而且编码客观性和物理状态的模型往往能做出更好的预测。然而，所有模型和人类表现之间仍然存在巨大的差距，所有模型的预测与人类的预测相关性很差，这表明没有一个最先进的模型正在学习以类似人类的方式进行物理预测。项目主页:  https://dingmyu.github.io/physion_v2/"
    },
    {
        "title": "PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle\n  Adjustment",
        "url": "http://arxiv.org/abs/2306.15667v1",
        "pub_date": "2023-06-27",
        "summary": "Camera pose estimation is a long-standing computer vision problem that to\ndate often relies on classical methods, such as handcrafted keypoint matching,\nRANSAC and bundle adjustment. In this paper, we propose to formulate the\nStructure from Motion (SfM) problem inside a probabilistic diffusion framework,\nmodelling the conditional distribution of camera poses given input images. This\nnovel view of an old problem has several advantages. (i) The nature of the\ndiffusion framework mirrors the iterative procedure of bundle adjustment. (ii)\nThe formulation allows a seamless integration of geometric constraints from\nepipolar geometry. (iii) It excels in typically difficult scenarios such as\nsparse views with wide baselines. (iv) The method can predict intrinsics and\nextrinsics for an arbitrary amount of images. We demonstrate that our method\nPoseDiffusion significantly improves over the classic SfM pipelines and the\nlearned approaches on two real-world datasets. Finally, it is observed that our\nmethod can generalize across datasets without further training. Project page:\nhttps://posediffusion.github.io/",
        "translated": "摄像机姿态估计是一个长期存在的计算机视觉问题，迄今为止通常依赖于经典的方法，如手工关键点匹配、 RANSAC 和光束法平差。在本文中，我们提出了一个概率扩散框架下的运动结构(SfM)问题，建立了给定输入图像的摄像机的条件分布模型。这种对旧问题的新观点有几个优点。(i)扩散框架的性质反映了光束法平差的迭代过程。(ii)公式允许对极上几何的几何约束进行无缝集成。(iii)在一些典型的困难情况下，例如视野稀疏、基线宽阔的情况下，它表现出色。(iv)该方法可以对任意数量的图像进行本征值和外征值的预测。实验结果表明，该方法比经典的 SfM 流水线和两个实际数据集上的学习方法有明显的改进。最后，观察到我们的方法不需要进一步的训练就可以在数据集之间进行泛化。项目主页:  https://posediffusion.github.io/"
    },
    {
        "title": "Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation",
        "url": "http://arxiv.org/abs/2306.15662v1",
        "pub_date": "2023-06-27",
        "summary": "Intrinsic image decomposition and inverse rendering are long-standing\nproblems in computer vision. To evaluate albedo recovery, most algorithms\nreport their quantitative performance with a mean Weighted Human Disagreement\nRate (WHDR) metric on the IIW dataset. However, WHDR focuses only on relative\nalbedo values and often fails to capture overall quality of the albedo. In\norder to comprehensively evaluate albedo, we collect a new dataset, Measured\nAlbedo in the Wild (MAW), and propose three new metrics that complement WHDR:\nintensity, chromaticity and texture metrics. We show that existing algorithms\noften improve WHDR metric but perform poorly on other metrics. We then finetune\ndifferent algorithms on our MAW dataset to significantly improve the quality of\nthe reconstructed albedo both quantitatively and qualitatively. Since the\nproposed intensity, chromaticity, and texture metrics and the WHDR are all\ncomplementary we further introduce a relative performance measure that captures\naverage performance. By analysing existing algorithms we show that there is\nsignificant room for improvement. Our dataset and evaluation metrics will\nenable researchers to develop algorithms that improve albedo reconstruction.\nCode and Data available at: https://measuredalbedo.github.io/",
        "translated": "本征图像分解和逆绘制是计算机视觉中一个长期存在的问题。为了评估反照率恢复，大多数算法在 IIW 数据集上使用平均加权人类异议率(WHDR)度量来报告它们的定量性能。然而，WHDR 只关注相对反照率值，往往无法捕捉反照率的整体质量。为了全面评价反照率，我们收集了一个新的数据集——野外测量反照率(MAW) ，并提出了三个补充 WHDR 的新指标: 强度、色度和纹理指标。我们表明，现有的算法往往改善 WHDR 度量，但在其他度量表现不佳。然后，我们微调不同的算法对我们的 MAW 数据集，以显着提高质量的重建反照率的定量和定性。由于提出的强度，色度，纹理度量和 WHDR 都是互补的，我们进一步引入了一个相对的性能度量，捕捉平均性能。通过对现有算法的分析，我们发现还有很大的改进空间。我们的数据集和评估指标将使研究人员能够开发改善反照率重建的算法。代码和数据可在以下 https://measuredalbedo.github.io/查阅:"
    },
    {
        "title": "On Practical Aspects of Aggregation Defenses against Data Poisoning\n  Attacks",
        "url": "http://arxiv.org/abs/2306.16415v1",
        "pub_date": "2023-06-28",
        "summary": "The increasing access to data poses both opportunities and risks in deep\nlearning, as one can manipulate the behaviors of deep learning models with\nmalicious training samples. Such attacks are known as data poisoning. Recent\nadvances in defense strategies against data poisoning have highlighted the\neffectiveness of aggregation schemes in achieving state-of-the-art results in\ncertified poisoning robustness. However, the practical implications of these\napproaches remain unclear. Here we focus on Deep Partition Aggregation, a\nrepresentative aggregation defense, and assess its practical aspects, including\nefficiency, performance, and robustness. For evaluations, we use ImageNet\nresized to a resolution of 64 by 64 to enable evaluations at a larger scale\nthan previous ones. Firstly, we demonstrate a simple yet practical approach to\nscaling base models, which improves the efficiency of training and inference\nfor aggregation defenses. Secondly, we provide empirical evidence supporting\nthe data-to-complexity ratio, i.e. the ratio between the data set size and\nsample complexity, as a practical estimation of the maximum number of base\nmodels that can be deployed while preserving accuracy. Last but not least, we\npoint out how aggregation defenses boost poisoning robustness empirically\nthrough the poisoning overfitting phenomenon, which is the key underlying\nmechanism for the empirical poisoning robustness of aggregations. Overall, our\nfindings provide valuable insights for practical implementations of aggregation\ndefenses to mitigate the threat of data poisoning.",
        "translated": "越来越多的数据访问给深度学习带来了机会和风险，因为人们可以用恶意的训练样本操纵深度学习模型的行为。这种攻击被称为数据中毒。针对数据中毒的防御策略的最新进展强调了聚合方案在获得最先进的证明中毒鲁棒性结果方面的有效性。然而，这些方法的实际意义仍不清楚。在这里，我们重点讨论深度分区聚合(Deep Partition Agging) ，这是一种有代表性的聚合防御，并评估其实用方面，包括效率、性能和健壮性。对于评价，我们使用 ImageNet 调整为64乘64的分辨率，以便能够进行比以前更大规模的评价。首先，我们演示了一种简单而实用的扩展基本模型的方法，它提高了聚合防御的训练和推理效率。其次，我们提供支持数据复杂度比率的经验证明，即数据集大小与样本复杂度之间的比率，作为在保持准确性的同时可以部署的最大基本模型数量的实用估计。最后，我们指出聚集防御是如何通过中毒过拟合现象来提高聚集的经验中毒鲁棒性的，这是聚集的经验中毒鲁棒性的关键机制。总的来说，我们的发现为聚合防御的实际实现提供了有价值的见解，以减轻数据中毒的威胁。"
    },
    {
        "title": "MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep\n  Learning",
        "url": "http://arxiv.org/abs/2306.16413v1",
        "pub_date": "2023-06-28",
        "summary": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. In order to accelerate progress towards\nunderstudied modalities and tasks while ensuring real-world robustness, we\nrelease MultiZoo, a public toolkit consisting of standardized implementations\nof &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark\nspanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.\nTogether, these provide an automated end-to-end machine learning pipeline that\nsimplifies and standardizes data loading, experimental setup, and model\nevaluation. To enable holistic evaluation, we offer a comprehensive methodology\nto assess (1) generalization, (2) time and space complexity, and (3) modality\nrobustness. MultiBench paves the way towards a better understanding of the\ncapabilities and limitations of multimodal models, while ensuring ease of use,\naccessibility, and reproducibility. Our toolkits are publicly available, will\nbe regularly updated, and welcome inputs from the community.",
        "translated": "学习多模态表示涉及到整合来自多个异构数据源的信息。为了在确保现实世界稳健性的同时加快未被研究的模式和任务的进展，我们发布了 MultiZoo，一个公共工具包，由 > 20个核心多模式算法的标准化实现和 MultiBench 组成，MultiBench 是一个跨越15个数据集，10个模式，20个预测任务和6个研究领域的大规模基准。总之，它们提供了一个自动化的端到端机器学习流水线，可以简化和标准化数据加载、实验设置和模型评估。为了实现整体评估，我们提供了一个综合的方法来评估(1)泛化，(2)时间和空间复杂性，和(3)模态鲁棒性。MultiBench 为更好地理解多模式模型的功能和局限性铺平了道路，同时确保了易用性、可访问性和可重复性。我们的工具包是公开可用的，将定期更新，并欢迎来自社区的输入。"
    },
    {
        "title": "Towards Language Models That Can See: Computer Vision Through the LENS\n  of Natural Language",
        "url": "http://arxiv.org/abs/2306.16410v1",
        "pub_date": "2023-06-28",
        "summary": "We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.",
        "translated": "我们提出 LENS，一种通过利用大型语言模型(LLM)解决计算机视觉问题的模块化方法。我们的系统使用一个语言模型来推理来自一组独立且高度描述性的视觉模块的输出，这些模块提供关于图像的详尽信息。我们评估的方法纯计算机视觉设置，如零和少拍摄物体识别，以及在视觉和语言问题。LENS 可以应用于任何现成的 LLM，我们发现 LENS 的 LLM 在更大和更复杂的系统中表现得非常有竞争力，没有任何多模态训练。我们 https://github.com/contextualai/lens 开源代码并提供交互式演示。"
    },
    {
        "title": "Theater Aid System for the Visually Impaired Through Transfer Learning\n  of Spatio-Temporal Graph Convolution Networks",
        "url": "http://arxiv.org/abs/2306.16357v1",
        "pub_date": "2023-06-28",
        "summary": "The aim of this research is to recognize human actions performed on stage to\naid visually impaired and blind individuals. To achieve this, we have created a\ntheatre human action recognition system that uses skeleton data captured by\ndepth image as input. We collected new samples of human actions in a theatre\nenvironment, and then tested the transfer learning technique with three\npre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based human\naction recognition: the spatio-temporal graph convolution network, the\ntwo-stream adaptive graph convolution network, and the multi-scale disentangled\nunified graph convolution network. We selected the NTU-RGBD human action\nbenchmark as the source domain and used our collected dataset as the target\ndomain. We analyzed the transferability of the pre-trained models and proposed\ntwo configurations to apply and adapt the transfer learning technique to the\ndiversity between the source and target domains. The use of transfer learning\nhelped to improve the performance of the human action system within the context\nof theatre. The results indicate that Spatio-Temporal Graph Convolution\nNetworks is positively transferred, and there was an improvement in performance\ncompared to the baseline without transfer learning.",
        "translated": "本研究的目的是识别人类在舞台上的行为，以帮助视障人士和盲人。为了实现这一目标，我们创建了一个剧院人体动作识别系统，该系统使用深度图像获取的骨骼数据作为输入。我们在剧院环境中收集了新的人体动作样本，然后利用三个预先训练好的时空图卷积网络(时空图卷积网络、双流自适应图卷积网络和多尺度分离统一图卷积网络)对传递学习技术进行了测试，以实现基于骨架的人体动作识别。我们选择 NTU-RGBD 人类行为基准作为源域，并使用我们收集的数据集作为目标域。我们分析了预训练模型的可迁移性，并提出了两种配置方案来应用和调整迁移学习技术以适应源域和目标域之间的差异。迁移学习的使用有助于改善戏剧中人类行为系统的表现。结果表明，时空图卷积网络具有正向迁移的特性，在不进行迁移学习的情况下，网络的性能比基线有所提高。"
    },
    {
        "title": "DiffComplete: Diffusion-based Generative 3D Shape Completion",
        "url": "http://arxiv.org/abs/2306.16329v1",
        "pub_date": "2023-06-28",
        "summary": "We introduce a new diffusion-based approach for shape completion on 3D range\nscans. Compared with prior deterministic and probabilistic methods, we strike a\nbalance between realism, multi-modality, and high fidelity. We propose\nDiffComplete by casting shape completion as a generative task conditioned on\nthe incomplete shape. Our key designs are two-fold. First, we devise a\nhierarchical feature aggregation mechanism to inject conditional features in a\nspatially-consistent manner. So, we can capture both local details and broader\ncontexts of the conditional inputs to control the shape completion. Second, we\npropose an occupancy-aware fusion strategy in our model to enable the\ncompletion of multiple partial shapes and introduce higher flexibility on the\ninput conditions. DiffComplete sets a new SOTA performance (e.g., 40% decrease\non l_1 error) on two large-scale 3D shape completion benchmarks. Our completed\nshapes not only have a realistic outlook compared with the deterministic\nmethods but also exhibit high similarity to the ground truths compared with the\nprobabilistic alternatives. Further, DiffComplete has strong generalizability\non objects of entirely unseen classes for both synthetic and real data,\neliminating the need for model re-training in various applications.",
        "translated": "我们介绍了一种新的基于扩散的三维距离扫描形状补全方法。与已有的确定性和概率方法相比，我们在现实主义、多模态和高保真度之间取得了平衡。我们提出铸造形状完成区分完成作为一个生成任务的条件下的不完全形状。我们的关键设计是双重的。首先，我们设计了一个层次化的特征聚合机制，以一种空间一致的方式注入条件特征。因此，我们可以捕获局部细节和更广泛的条件输入上下文，以控制形状完成。其次，我们在模型中提出了一种基于占用感知的融合策略，使得多个部分形状的融合成为可能，并且在输入条件上引入了更高的灵活性。在两个大规模的3D 形状完成基准测试上，DiffComplete 设置了一个新的 SOTA 性能(例如，l _ 1误差降低了40%)。与确定性方法相比，完整形状不仅具有较为真实的外观，而且与概率方法相比，完整形状与地面真相具有较高的相似性。此外，区分完成对合成数据和真实数据的完全看不见类的对象具有很强的泛化能力，消除了在各种应用中对模型重新训练的需要。"
    },
    {
        "title": "An Efficient General-Purpose Modular Vision Model via Multi-Task\n  Heterogeneous Training",
        "url": "http://arxiv.org/abs/2306.17165v1",
        "pub_date": "2023-06-29",
        "summary": "We present a model that can perform multiple vision tasks and can be adapted\nto other downstream tasks efficiently. Despite considerable progress in\nmulti-task learning, most efforts focus on learning from multi-label data: a\nsingle image set with multiple task labels. Such multi-label data sets are\nrare, small, and expensive. We say heterogeneous to refer to image sets with\ndifferent task labels, or to combinations of single-task datasets. Few have\nexplored training on such heterogeneous datasets. General-purpose vision models\nare still dominated by single-task pretraining, and it remains unclear how to\nscale up multi-task models by leveraging mainstream vision datasets designed\nfor different purposes. The challenges lie in managing large intrinsic\ndifferences among vision tasks, including data distribution, architectures,\ntask-specific modules, dataset scales, and sampling strategies. To address\nthese challenges, we propose to modify and scale up mixture-of-experts (MoE)\nvision transformers, so that they can simultaneously learn classification,\ndetection, and segmentation on diverse mainstream vision datasets including\nImageNet, COCO, and ADE20K. Our approach achieves comparable results to\nsingle-task state-of-the-art models and demonstrates strong generalization on\ndownstream tasks. Due to its emergent modularity, this general-purpose model\ndecomposes into high-performing components, efficiently adapting to downstream\ntasks. We can fine-tune it with fewer training parameters, fewer model\nparameters, and less computation. Additionally, its modularity allows for easy\nexpansion in continual-learning-without-forgetting scenarios. Finally, these\nfunctions can be controlled and combined to meet various demands of downstream\ntasks.",
        "translated": "我们提出了一个模型，可以执行多视觉任务，可以适应其他下游任务的效率。尽管在多任务学习方面取得了相当大的进展，但大多数努力都集中在从多标签数据中学习: 一个具有多个任务标签的单个图像集。这样的多标签数据集是罕见的、小的和昂贵的。我们称之为异构，是指具有不同任务标签的图像集，或单任务数据集的组合。很少有人探索过这种异构数据集的训练。通用视觉模型仍然以单任务预训练为主，如何利用为不同目的设计的主流视觉数据集来扩展多任务模型尚不清楚。挑战在于管理视觉任务之间巨大的内在差异，包括数据分布、体系结构、任务特定模块、数据集规模和采样策略。为了应对这些挑战，我们建议修改和扩展混合专家(MoE)视觉变换器，以便它们能够同时学习不同主流视觉数据集(包括 ImageNet，COCO 和 ADE20K)上的分类、检测和分割。我们的方法实现了与最先进的单任务模型相当的结果，并在下游任务上展示了强大的泛化能力。由于它的紧急模块化，这个通用模型分解成高性能的组件，有效地适应下游任务。我们可以用较少的训练参数、较少的模型参数和较少的计算量对它进行微调。此外，它的模块化允许在不遗忘的连续学习场景中轻松扩展。最后，可以对这些功能进行控制和组合，以满足下游任务的各种需求。"
    },
    {
        "title": "Generate Anything Anywhere in Any Scene",
        "url": "http://arxiv.org/abs/2306.17154v1",
        "pub_date": "2023-06-29",
        "summary": "Text-to-image diffusion models have attracted considerable interest due to\ntheir wide applicability across diverse fields. However, challenges persist in\ncreating controllable models for personalized object generation. In this paper,\nwe first identify the entanglement issues in existing personalized generative\nmodels, and then propose a straightforward and efficient data augmentation\ntraining strategy that guides the diffusion model to focus solely on object\nidentity. By inserting the plug-and-play adapter layers from a pre-trained\ncontrollable diffusion model, our model obtains the ability to control the\nlocation and size of each generated personalized object. During inference, we\npropose a regionally-guided sampling technique to maintain the quality and\nfidelity of the generated images. Our method achieves comparable or superior\nfidelity for personalized objects, yielding a robust, versatile, and\ncontrollable text-to-image diffusion model that is capable of generating\nrealistic and personalized images. Our approach demonstrates significant\npotential for various applications, such as those in art, entertainment, and\nadvertising design.",
        "translated": "文本-图像扩散模型因其在不同领域的广泛适用性而引起了人们的极大兴趣。然而，为个性化对象生成创建可控模型仍然是一个挑战。在本文中，我们首先确定了现有的个性化生成模型中存在的纠缠问题，然后提出了一种简单有效的数据增强训练策略，引导扩散模型只关注对象身份。通过从预先训练的可控扩散模型中插入即插即用适配器层，我们的模型获得了控制每个生成的个性化对象的位置和大小的能力。在推理过程中，我们提出了一种区域引导的采样技术，以保持所生成图像的质量和保真度。我们的方法实现了对个性化对象可比或更高的保真度，产生了一个健壮的，通用的，可控的文本到图像扩散模型，能够生成真实的和个性化的图像。我们的方法展示了各种应用的巨大潜力，例如在艺术、娱乐和广告设计中的应用。"
    },
    {
        "title": "Filtered-Guided Diffusion: Fast Filter Guidance for Black-Box Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2306.17141v1",
        "pub_date": "2023-06-29",
        "summary": "Recent advances in diffusion-based generative models have shown incredible\npromise for Image-to-Image translation and editing. Most recent work in this\nspace relies on additional training or architecture-specific adjustments to the\ndiffusion process. In this work, we show that much of this low-level control\ncan be achieved without additional training or any access to features of the\ndiffusion model. Our method simply applies a filter to the input of each\ndiffusion step based on the output of the previous step in an adaptive manner.\nNotably, this approach does not depend on any specific architecture or sampler\nand can be done without access to internal features of the network, making it\neasy to combine with other techniques, samplers, and diffusion architectures.\nFurthermore, it has negligible cost to performance, and allows for more\ncontinuous adjustment of guidance strength than other approaches. We show FGD\noffers a fast and strong baseline that is competitive with recent\narchitecture-dependent approaches. Furthermore, FGD can also be used as a\nsimple add-on to enhance the structural guidance of other state-of-the-art I2I\nmethods. Finally, our derivation of this method helps to understand the impact\nof self attention, a key component of other recent architecture-specific I2I\napproaches, in a more architecture-independent way. Project page:\nhttps://github.com/jaclyngu/FilteredGuidedDiffusion",
        "translated": "基于扩散的生成模型的最新进展表明，图像到图像的翻译和编辑有着令人难以置信的前景。最近在这个领域的工作依赖于对扩散过程的额外培训或特定于架构的调整。在这项工作中，我们表明，大部分这种低水平的控制可以实现没有额外的训练或任何访问的扩散模型的特征。我们的方法只是简单地应用一个滤波器的输入的每一个扩散步骤的基础上输出的前一步骤在一个自适应的方式。值得注意的是，这种方法不依赖于任何特定的体系结构或采样器，并且不需要访问网络的内部特性就可以完成，这使得它很容易与其他技术、采样器和扩散体系结构相结合。此外，它的性能成本可以忽略不计，并允许更多的连续调整制导强度比其他方法。我们显示 FGD 提供了一个快速和强大的基线，与最近的体系结构相关的方法具有竞争力。此外，FGD 还可以作为一个简单的附加组件来增强其他最先进的 I2I 方法的结构指导。最后，我们对这种方法的推导有助于以一种更加独立于体系结构的方式理解自我关注的影响，自我关注是最近其他特定于体系结构的 I2I 方法的一个关键组件。项目主页:  https://github.com/jaclyngu/filteredguideddiffusion"
    },
    {
        "title": "ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2306.17140v1",
        "pub_date": "2023-06-29",
        "summary": "Given sparse views of an object, estimating their camera poses is a\nlong-standing and intractable problem. We harness the pre-trained diffusion\nmodel of novel views conditioned on viewpoints (Zero-1-to-3). We present\nID-Pose which inverses the denoising diffusion process to estimate the relative\npose given two input images. ID-Pose adds a noise on one image, and predicts\nthe noise conditioned on the other image and a decision variable for the pose.\nThe prediction error is used as the objective to find the optimal pose with the\ngradient descent method. ID-Pose can handle more than two images and estimate\neach of the poses with multiple image pairs from triangular relationships.\nID-Pose requires no training and generalizes to real-world images. We conduct\nexperiments using high-quality real-scanned 3D objects, where ID-Pose\nsignificantly outperforms state-of-the-art methods.",
        "translated": "给定一个物体的稀疏视图，估计他们的相机姿态是一个长期和棘手的问题。我们利用预先训练的扩散模型的新观点条件下的观点(零1至3)。我们提出了逆去噪扩散过程的 ID-Pose 算法来估计给定两个输入图像的相对位姿。ID-Pose 在一幅图像上添加一个噪声，并预测噪声条件在另一幅图像和一个决策变量的姿态。以预测误差为目标，采用梯度下降法法寻找最佳姿态。ID-Pose 可以处理两个以上的图像，并通过三角关系估计多个图像对的每个姿势。ID-Pose 不需要任何训练，可以概括为真实世界的图像。我们使用高质量的实际扫描3D 对象进行实验，其中 ID-Pose 显著优于最先进的方法。"
    },
    {
        "title": "PVP: Personalized Video Prior for Editable Dynamic Portraits using\n  StyleGAN",
        "url": "http://arxiv.org/abs/2306.17123v1",
        "pub_date": "2023-06-29",
        "summary": "Portrait synthesis creates realistic digital avatars which enable users to\ninteract with others in a compelling way. Recent advances in StyleGAN and its\nextensions have shown promising results in synthesizing photorealistic and\naccurate reconstruction of human faces. However, previous methods often focus\non frontal face synthesis and most methods are not able to handle large head\nrotations due to the training data distribution of StyleGAN. In this work, our\ngoal is to take as input a monocular video of a face, and create an editable\ndynamic portrait able to handle extreme head poses. The user can create novel\nviewpoints, edit the appearance, and animate the face. Our method utilizes\npivotal tuning inversion (PTI) to learn a personalized video prior from a\nmonocular video sequence. Then we can input pose and expression coefficients to\nMLPs and manipulate the latent vectors to synthesize different viewpoints and\nexpressions of the subject. We also propose novel loss functions to further\ndisentangle pose and expression in the latent space. Our algorithm shows much\nbetter performance over previous approaches on monocular video datasets, and it\nis also capable of running in real-time at 54 FPS on an RTX 3080.",
        "translated": "肖像合成创建真实的数字化身，使用户能够与他人互动，在一个引人注目的方式。StyleGAN 及其扩展技术的最新进展表明，在合成真实感和准确的人脸重建方面取得了有希望的成果。然而，由于 StyleGAN 的训练数据分布特性，以往的人脸合成方法往往侧重于额面合成，大多数方法不能处理大的头部旋转。在这项工作中，我们的目标是采取作为输入一个单目视频的脸，并创建一个可编辑的动态肖像能够处理极端的头部姿势。用户可以创建新的视点，编辑外观，并动画的脸。该方法利用关键调谐反演(PTI)从单目视频序列中学习个性化视频。然后将姿态和表情系数输入 MLP，操纵潜在向量合成不同的主题观点和表情。我们还提出了新的损失函数，以进一步分离潜在空间中的姿态和表达。该算法在单目视频数据集上的性能优于以往的算法，并且能够在 RTX 3080上以54FPS 的速度实时运行。"
    },
    {
        "title": "Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing",
        "url": "http://arxiv.org/abs/2306.17848v1",
        "pub_date": "2023-06-30",
        "summary": "Vision transformers (ViTs) have significantly changed the computer vision\nlandscape and have periodically exhibited superior performance in vision tasks\ncompared to convolutional neural networks (CNNs). Although the jury is still\nout on which model type is superior, each has unique inductive biases that\nshape their learning and generalization performance. For example, ViTs have\ninteresting properties with respect to early layer non-local feature\ndependence, as well as self-attention mechanisms which enhance learning\nflexibility, enabling them to ignore out-of-context image information more\neffectively. We hypothesize that this power to ignore out-of-context\ninformation (which we name $\\textit{patch selectivity}$), while integrating\nin-context information in a non-local manner in early layers, allows ViTs to\nmore easily handle occlusion. In this study, our aim is to see whether we can\nhave CNNs $\\textit{simulate}$ this ability of patch selectivity by effectively\nhardwiring this inductive bias using Patch Mixing data augmentation, which\nconsists of inserting patches from another image onto a training image and\ninterpolating labels between the two image classes. Specifically, we use Patch\nMixing to train state-of-the-art ViTs and CNNs, assessing its impact on their\nability to ignore out-of-context patches and handle natural occlusions. We find\nthat ViTs do not improve nor degrade when trained using Patch Mixing, but CNNs\nacquire new capabilities to ignore out-of-context information and improve on\nocclusion benchmarks, leaving us to conclude that this training method is a way\nof simulating in CNNs the abilities that ViTs already possess. We will release\nour Patch Mixing implementation and proposed datasets for public use. Project\npage: https://arielnlee.github.io/PatchMixing/",
        "translated": ""
    },
    {
        "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D\n  and 3D Diffusion Priors",
        "url": "http://arxiv.org/abs/2306.17843v1",
        "pub_date": "2023-06-30",
        "summary": "We present Magic123, a two-stage coarse-to-fine approach for high-quality,\ntextured 3D meshes generation from a single unposed image in the wild using\nboth2D and 3D priors. In the first stage, we optimize a neural radiance field\nto produce a coarse geometry. In the second stage, we adopt a memory-efficient\ndifferentiable mesh representation to yield a high-resolution mesh with a\nvisually appealing texture. In both stages, the 3D content is learned through\nreference view supervision and novel views guided by a combination of 2D and 3D\ndiffusion priors. We introduce a single trade-off parameter between the 2D and\n3D priors to control exploration (more imaginative) and exploitation (more\nprecise) of the generated geometry. Additionally, we employ textual inversion\nand monocular depth regularization to encourage consistent appearances across\nviews and to prevent degenerate solutions, respectively. Magic123 demonstrates\na significant improvement over previous image-to-3D techniques, as validated\nthrough extensive experiments on synthetic benchmarks and diverse real-world\nimages. Our code, models, and generated 3D assets are available at\nhttps://github.com/guochengqian/Magic123.",
        "translated": ""
    },
    {
        "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen\n  LLMs",
        "url": "http://arxiv.org/abs/2306.17842v1",
        "pub_date": "2023-06-30",
        "summary": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%.",
        "translated": ""
    },
    {
        "title": "Federated Ensemble YOLOv5 - A Better Generalized Object Detection\n  Algorithm",
        "url": "http://arxiv.org/abs/2306.17829v1",
        "pub_date": "2023-06-30",
        "summary": "Federated learning (FL) has gained significant traction as a\nprivacy-preserving algorithm, but the underlying resembles of federated\nlearning algorithm like Federated averaging (FED Avg) or Federated SGD (FED\nSGD) to ensemble learning algorithms has not been fully explored. The purpose\nof this paper is to examine the application of FL to object detection as a\nmethod to enhance generalizability, and to compare its performance against a\ncentralized training approach for an object detection algorithm. Specifically,\nwe investigate the performance of a YOLOv5 model trained using FL across\nmultiple clients and employ a random sampling strategy without replacement, so\neach client holds a portion of the same dataset used for centralized training.\nOur experimental results showcase the superior efficiency of the FL object\ndetector's global model in generating accurate bounding boxes for unseen\nobjects, with the test set being a mixture of objects from two distinct clients\nnot represented in the training dataset. These findings suggest that FL can be\nviewed from an ensemble algorithm perspective, akin to a synergistic blend of\nBagging and Boosting techniques. As a result, FL can be seen not only as a\nmethod to enhance privacy, but also as a method to enhance the performance of a\nmachine learning model.",
        "translated": ""
    },
    {
        "title": "Stay on topic with Classifier-Free Guidance",
        "url": "http://arxiv.org/abs/2306.17806v1",
        "pub_date": "2023-06-30",
        "summary": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&amp;A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.",
        "translated": ""
    },
    {
        "title": "Real-time Monocular Full-body Capture in World Space via Sequential\n  Proxy-to-Motion Learning",
        "url": "http://arxiv.org/abs/2307.01200v1",
        "pub_date": "2023-07-03",
        "summary": "Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we contribute a sequential\nproxy-to-motion learning scheme together with a proxy dataset of 2D skeleton\nsequences and 3D rotational motions in world space. Such proxy data enables us\nto build a learning-based network with accurate full-body supervision while\nalso mitigating the generalization issues. For more accurate and physically\nplausible predictions, a contact-aware neural motion descent module is proposed\nin our network so that it can be aware of foot-ground contact and motion\nmisalignment with the proxy observations. Additionally, we share the body-hand\ncontext information in our network for more compatible wrist poses recovery\nwith the full-body model. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space. More video results can be found\nat our project page: https://liuyebin.com/proxycap.",
        "translated": ""
    },
    {
        "title": "NeuBTF: Neural fields for BTF encoding and transfer",
        "url": "http://arxiv.org/abs/2307.01199v1",
        "pub_date": "2023-07-03",
        "summary": "Neural material representations are becoming a popular way to represent\nmaterials for rendering. They are more expressive than analytic models and\noccupy less memory than tabulated BTFs. However, existing neural materials are\nimmutable, meaning that their output for a certain query of UVs, camera, and\nlight vector is fixed once they are trained. While this is practical when there\nis no need to edit the material, it can become very limiting when the fragment\nof the material used for training is too small or not tileable, which\nfrequently happens when the material has been captured with a\ngonioreflectometer. In this paper, we propose a novel neural material\nrepresentation which jointly tackles the problems of BTF compression, tiling,\nand extrapolation. At test time, our method uses a guidance image as input to\ncondition the neural BTF to the structural features of this input image. Then,\nthe neural BTF can be queried as a regular BTF using UVs, camera, and light\nvectors. Every component in our framework is purposefully designed to maximize\nBTF encoding quality at minimal parameter count and computational complexity,\nachieving competitive compression rates compared with previous work. We\ndemonstrate the results of our method on a variety of synthetic and captured\nmaterials, showing its generality and capacity to learn to represent many\noptical properties.",
        "translated": ""
    },
    {
        "title": "Segment Anything Meets Point Tracking",
        "url": "http://arxiv.org/abs/2307.01197v1",
        "pub_date": "2023-07-03",
        "summary": "The Segment Anything Model (SAM) has established itself as a powerful\nzero-shot image segmentation model, employing interactive prompts such as\npoints to generate masks. This paper presents SAM-PT, a method extending SAM's\ncapability to tracking and segmenting anything in dynamic videos. SAM-PT\nleverages robust and sparse point selection and propagation techniques for mask\ngeneration, demonstrating that a SAM-based segmentation tracker can yield\nstrong zero-shot performance across popular video object segmentation\nbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional\nobject-centric mask propagation strategies, we uniquely use point propagation\nto exploit local structure information that is agnostic to object semantics. We\nhighlight the merits of point-based tracking through direct evaluation on the\nzero-shot open-world Unidentified Video Objects (UVO) benchmark. To further\nenhance our approach, we utilize K-Medoids clustering for point initialization\nand track both positive and negative points to clearly distinguish the target\nobject. We also employ multiple mask decoding passes for mask refinement and\ndevise a point re-initialization strategy to improve tracking accuracy. Our\ncode integrates different point trackers and video segmentation benchmarks and\nwill be released at https://github.com/SysCV/sam-pt.",
        "translated": ""
    },
    {
        "title": "SAMAug: Point Prompt Augmentation for Segment Anything Model",
        "url": "http://arxiv.org/abs/2307.01187v1",
        "pub_date": "2023-07-03",
        "summary": "This paper introduces SAMAug, a novel visual point augmentation method for\nthe Segment Anything Model (SAM) that enhances interactive image segmentation\nperformance. SAMAug generates augmented point prompts to provide more\ninformation to SAM. From the initial point prompt, SAM produces the initial\nmask, which is then fed into our proposed SAMAug to generate augmented point\nprompts. By incorporating these extra points, SAM can generate augmented\nsegmentation masks based on the augmented point prompts and the initial prompt,\nresulting in improved segmentation performance. We evaluate four point\naugmentation techniques: random selection, maximum difference entropy, maximum\ndistance, and a saliency model. Experiments on the COCO, Fundus, and Chest\nX-ray datasets demonstrate that SAMAug can boost SAM's segmentation results,\nespecially using the maximum distance and saliency model methods. SAMAug\nunderscores the potential of visual prompt engineering to advance interactive\ncomputer vision models.",
        "translated": ""
    },
    {
        "title": "Investigating Data Memorization in 3D Latent Diffusion Models for\n  Medical Image Synthesis",
        "url": "http://arxiv.org/abs/2307.01148v1",
        "pub_date": "2023-07-03",
        "summary": "Generative latent diffusion models have been established as state-of-the-art\nin data generation. One promising application is generation of realistic\nsynthetic medical imaging data for open data sharing without compromising\npatient privacy. Despite the promise, the capacity of such models to memorize\nsensitive patient training data and synthesize samples showing high resemblance\nto training data samples is relatively unexplored. Here, we assess the\nmemorization capacity of 3D latent diffusion models on photon-counting coronary\ncomputed tomography angiography and knee magnetic resonance imaging datasets.\nTo detect potential memorization of training samples, we utilize\nself-supervised models based on contrastive learning. Our results suggest that\nsuch latent diffusion models indeed memorize training data, and there is a dire\nneed for devising strategies to mitigate memorization.",
        "translated": ""
    },
    {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.02485v1",
        "pub_date": "2023-07-05",
        "summary": "Large Language Models (LLMs) have demonstrated impressive planning abilities\nin single-agent embodied tasks across various domains. However, their capacity\nfor planning and communication in multi-agent cooperation remains unclear, even\nthough these are crucial skills for intelligent embodied agents. In this paper,\nwe present a novel framework that utilizes LLMs for multi-agent cooperation and\ntests it in various embodied environments. Our framework enables embodied\nagents to plan, communicate, and cooperate with other embodied agents or humans\nto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,\nsuch as GPT-4, can surpass strong planning-based methods and exhibit emergent\neffective communication using our framework without requiring fine-tuning or\nfew-shot prompting. We also discover that LLM-based agents that communicate in\nnatural language can earn more trust and cooperate more effectively with\nhumans. Our research underscores the potential of LLMs for embodied AI and lays\nthe foundation for future research in multi-agent cooperation. Videos can be\nfound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "translated": ""
    },
    {
        "title": "A Dataset of Inertial Measurement Units for Handwritten English\n  Alphabets",
        "url": "http://arxiv.org/abs/2307.02480v1",
        "pub_date": "2023-07-05",
        "summary": "This paper presents an end-to-end methodology for collecting datasets to\nrecognize handwritten English alphabets by utilizing Inertial Measurement Units\n(IMUs) and leveraging the diversity present in the Indian writing style. The\nIMUs are utilized to capture the dynamic movement patterns associated with\nhandwriting, enabling more accurate recognition of alphabets. The Indian\ncontext introduces various challenges due to the heterogeneity in writing\nstyles across different regions and languages. By leveraging this diversity,\nthe collected dataset and the collection system aim to achieve higher\nrecognition accuracy. Some preliminary experimental results demonstrate the\neffectiveness of the dataset in accurately recognizing handwritten English\nalphabet in the Indian context. This research can be extended and contributes\nto the field of pattern recognition and offers valuable insights for developing\nimproved systems for handwriting recognition, particularly in diverse\nlinguistic and cultural contexts.",
        "translated": ""
    },
    {
        "title": "What Matters in Training a GPT4-Style Language Model with Multimodal\n  Inputs?",
        "url": "http://arxiv.org/abs/2307.02469v1",
        "pub_date": "2023-07-05",
        "summary": "Recent advancements in Large Language Models (LLMs) such as GPT4 have\ndisplayed exceptional multi-modal capabilities in following open-ended\ninstructions given images. However, the performance of these models heavily\nrelies on design choices such as network structures, training data, and\ntraining strategies, and these choices have not been extensively discussed in\nthe literature, making it difficult to quantify progress in this field. To\naddress this issue, this paper presents a systematic and comprehensive study,\nquantitatively and qualitatively, on training such models. We implement over 20\nvariants with controlled settings. Concretely, for network structures, we\ncompare different LLM backbones and model designs. For training data, we\ninvestigate the impact of data and sampling strategies. For instructions, we\nexplore the influence of diversified prompts on the instruction-following\nability of the trained models. For benchmarks, we contribute the first, to our\nbest knowledge, comprehensive evaluation set including both image and video\ntasks through crowd-sourcing. Based on our findings, we present Lynx, which\nperforms the most accurate multi-modal understanding while keeping the best\nmulti-modal generation ability compared to existing open-sourced GPT4-style\nmodels.",
        "translated": ""
    },
    {
        "title": "Large-scale Detection of Marine Debris in Coastal Areas with Sentinel-2",
        "url": "http://arxiv.org/abs/2307.02465v1",
        "pub_date": "2023-07-05",
        "summary": "Detecting and quantifying marine pollution and macro-plastics is an\nincreasingly pressing ecological issue that directly impacts ecology and human\nhealth. Efforts to quantify marine pollution are often conducted with sparse\nand expensive beach surveys, which are difficult to conduct on a large scale.\nHere, remote sensing can provide reliable estimates of plastic pollution by\nregularly monitoring and detecting marine debris in coastal areas.\nMedium-resolution satellite data of coastal areas is readily available and can\nbe leveraged to detect aggregations of marine debris containing plastic litter.\nIn this work, we present a detector for marine debris built on a deep\nsegmentation model that outputs a probability for marine debris at the pixel\nlevel. We train this detector with a combination of annotated datasets of\nmarine debris and evaluate it on specifically selected test sites where it is\nhighly probable that plastic pollution is present in the detected marine\ndebris. We demonstrate quantitatively and qualitatively that a deep learning\nmodel trained on this dataset issued from multiple sources outperforms existing\ndetection models trained on previous datasets by a large margin. Our\nexperiments show, consistent with the principles of data-centric AI, that this\nperformance is due to our particular dataset design with extensive sampling of\nnegative examples and label refinements rather than depending on the particular\ndeep learning model. We hope to accelerate advances in the large-scale\nautomated detection of marine debris, which is a step towards quantifying and\nmonitoring marine litter with remote sensing at global scales, and release the\nmodel weights and training source code under\nhttps://github.com/marccoru/marinedebrisdetector",
        "translated": ""
    },
    {
        "title": "AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus\n  Callosum cross section from EM Images",
        "url": "http://arxiv.org/abs/2307.02464v1",
        "pub_date": "2023-07-05",
        "summary": "The electron microscope (EM) remains the predominant technique for\nelucidating intricate details of the animal nervous system at the nanometer\nscale. However, accurately reconstructing the complex morphology of axons and\nmyelin sheaths poses a significant challenge. Furthermore, the absence of\npublicly available, large-scale EM datasets encompassing complete cross\nsections of the corpus callosum, with dense ground truth segmentation for axons\nand myelin sheaths, hinders the advancement and evaluation of holistic corpus\ncallosum reconstructions. To surmount these obstacles, we introduce the\nAxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured from\nthe corpus callosum of the Rett Syndrome (RTT) mouse model, which entail\nextensive axon bundles. We meticulously proofread over 600,000 patches at a\nresolution of 1024 times 1024, thus providing a comprehensive ground truth for\nmyelinated axons and myelin sheaths. Additionally, we extensively annotated\nthree distinct regions within the dataset for the purposes of training,\ntesting, and validation. Utilizing this dataset, we develop a fine-tuning\nmethodology that adapts Segment Anything Model (SAM) to EM images segmentation\ntasks, called EM-SAM, enabling outperforms other state-of-the-art methods.\nFurthermore, we present the evaluation results of EM-SAM as a baseline.",
        "translated": ""
    },
    {
        "title": "Synthesizing Artistic Cinemagraphs from Text",
        "url": "http://arxiv.org/abs/2307.03190v1",
        "pub_date": "2023-07-06",
        "summary": "We introduce Artistic Cinemagraph, a fully automated method for creating\ncinemagraphs from text descriptions - an especially challenging task when\nprompts feature imaginary elements and artistic styles, given the complexity of\ninterpreting the semantics and motions of these images. Existing single-image\nanimation methods fall short on artistic inputs, and recent text-based video\nmethods frequently introduce temporal inconsistencies, struggling to keep\ncertain regions static. To address these challenges, we propose an idea of\nsynthesizing image twins from a single text prompt - a pair of an artistic\nimage and its pixel-aligned corresponding natural-looking twin. While the\nartistic image depicts the style and appearance detailed in our text prompt,\nthe realistic counterpart greatly simplifies layout and motion analysis.\nLeveraging existing natural image and video datasets, we can accurately segment\nthe realistic image and predict plausible motion given the semantic\ninformation. The predicted motion can then be transferred to the artistic image\nto create the final cinemagraph. Our method outperforms existing approaches in\ncreating cinemagraphs for natural landscapes as well as artistic and\nother-worldly scenes, as validated by automated metrics and user studies.\nFinally, we demonstrate two extensions: animating existing paintings and\ncontrolling motion directions using text.",
        "translated": ""
    },
    {
        "title": "IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via\n  Latent Diffusion Model",
        "url": "http://arxiv.org/abs/2307.03177v1",
        "pub_date": "2023-07-06",
        "summary": "Generating complete 360-degree panoramas from narrow field of view images is\nongoing research as omnidirectional RGB data is not readily available. Existing\nGAN-based approaches face some barriers to achieving higher quality output, and\nhave poor generalization performance over different mask types. In this paper,\nwe present our 360-degree indoor RGB panorama outpainting model using latent\ndiffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latent\ndiffusion structure that utilizes both RGB and depth panoramic data during\ntraining, but works surprisingly well to outpaint normal depth-free RGB images\nduring inference. We further propose a novel technique of introducing\nprogressive camera rotations during each diffusion denoising step, which leads\nto substantial improvement in achieving panorama wraparound consistency.\nResults show that our IPO-LDM not only significantly outperforms\nstate-of-the-art methods on RGB panorama outpainting, but can also produce\nmultiple and diverse well-structured results for different types of masks.",
        "translated": ""
    },
    {
        "title": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It",
        "url": "http://arxiv.org/abs/2307.03175v1",
        "pub_date": "2023-07-06",
        "summary": "Autonomous agriculture applications (e.g., inspection, phenotyping, plucking\nfruits) require manipulating the plant foliage to look behind the leaves and\nthe branches. Partial visibility, extreme clutter, thin structures, and unknown\ngeometry and dynamics for plants make such manipulation challenging. We tackle\nthese challenges through data-driven methods. We use self-supervision to train\nSRPNet, a neural network that predicts what space is revealed on execution of a\ncandidate action on a given plant. We use SRPNet with the cross-entropy method\nto predict actions that are effective at revealing space beneath plant foliage.\nFurthermore, as SRPNet does not just predict how much space is revealed but\nalso where it is revealed, we can execute a sequence of actions that\nincrementally reveal more and more space beneath the plant foliage. We\nexperiment with a synthetic (vines) and a real plant (Dracaena) on a physical\ntest-bed across 5 settings including 2 settings that test generalization to\nnovel plant configurations. Our experiments reveal the effectiveness of our\noverall method, PPG, over a competitive hand-crafted exploration method, and\nthe effectiveness of SRPNet over a hand-crafted dynamics model and relevant\nablations.",
        "translated": ""
    },
    {
        "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
        "url": "http://arxiv.org/abs/2307.03166v1",
        "pub_date": "2023-07-06",
        "summary": "We evaluate existing foundation models video understanding capabilities using\na carefully designed experiment protocol consisting of three hallmark tasks\n(action recognition, temporal localization, and spatiotemporal localization),\neight datasets well received by the community, and four adaptation methods\ntailoring a foundation model (FM) for a downstream task. Moreover, we propose a\nscalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency when\nadapting to general video understanding tasks. Our main findings are as\nfollows. First, task-specialized models significantly outperform the six FMs\nstudied in this work, in sharp contrast to what FMs have achieved in natural\nlanguage and image understanding. Second,video-native FMs, whose pretraining\ndata contains the video modality, are generally better than image-native FMs in\nclassifying motion-rich videos, localizing actions in time, and understanding a\nvideo of more than one action. Third, the video-native FMs can perform well on\nvideo tasks under light adaptations to downstream tasks(e.g., freezing the FM\nbackbones), while image-native FMs win in full end-to-end finetuning. The first\ntwo observations reveal the need and tremendous opportunities to conduct\nresearch on video-focused FMs, and the last confirms that both tasks and\nadaptation methods matter when it comes to the evaluation of FMs.",
        "translated": ""
    },
    {
        "title": "Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion\n  Classification?",
        "url": "http://arxiv.org/abs/2307.03157v1",
        "pub_date": "2023-07-06",
        "summary": "Deep learning-based diagnostic system has demonstrated potential in\nclassifying skin cancer conditions when labeled training example are abundant.\nHowever, skin lesion analysis often suffers from a scarcity of labeled data,\nhindering the development of an accurate and reliable diagnostic system. In\nthis work, we leverage multiple skin lesion datasets and investigate the\nfeasibility of various unsupervised domain adaptation (UDA) methods in binary\nand multi-class skin lesion classification. In particular, we assess three UDA\ntraining schemes: single-, combined-, and multi-source. Our experiment results\nshow that UDA is effective in binary classification, with further improvement\nbeing observed when imbalance is mitigated. In multi-class task, its\nperformance is less prominent, and imbalance problem again needs to be\naddressed to achieve above-baseline accuracy. Through our quantitative\nanalysis, we find that the test error of multi-class tasks is strongly\ncorrelated with label shift, and feature-level UDA methods have limitations\nwhen handling imbalanced datasets. Finally, our study reveals that UDA can\neffectively reduce bias against minority groups and promote fairness, even\nwithout the explicit use of fairness-focused techniques.",
        "translated": ""
    },
    {
        "title": "Training Ensembles with Inliers and Outliers for Semi-supervised Active\n  Learning",
        "url": "http://arxiv.org/abs/2307.03741v1",
        "pub_date": "2023-07-07",
        "summary": "Deep active learning in the presence of outlier examples poses a realistic\nyet challenging scenario. Acquiring unlabeled data for annotation requires a\ndelicate balance between avoiding outliers to conserve the annotation budget\nand prioritizing useful inlier examples for effective training. In this work,\nwe present an approach that leverages three highly synergistic components,\nwhich are identified as key ingredients: joint classifier training with inliers\nand outliers, semi-supervised learning through pseudo-labeling, and model\nensembling. Our work demonstrates that ensembling significantly enhances the\naccuracy of pseudo-labeling and improves the quality of data acquisition. By\nenabling semi-supervision through the joint training process, where outliers\nare properly handled, we observe a substantial boost in classifier accuracy\nthrough the use of all available unlabeled examples. Notably, we reveal that\nthe integration of joint training renders explicit outlier detection\nunnecessary; a conventional component for acquisition in prior work. The three\nkey components align seamlessly with numerous existing approaches. Through\nempirical evaluations, we showcase that their combined use leads to a\nperformance increase. Remarkably, despite its simplicity, our proposed approach\noutperforms all other methods in terms of performance. Code:\nhttps://github.com/vladan-stojnic/active-outliers",
        "translated": ""
    },
    {
        "title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and\n  Vision Transformers",
        "url": "http://arxiv.org/abs/2307.03712v1",
        "pub_date": "2023-07-07",
        "summary": "The recent rise of large language models (LLMs) has resulted in increased\nefforts towards running LLMs at reduced precision. Running LLMs at lower\nprecision supports resource constraints and furthers their democratization,\nenabling users to run billion-parameter LLMs on their personal devices. To\nsupplement this ongoing effort, we propose INT-FP-QSim: an open-source\nsimulator that enables flexible evaluation of LLMs and vision transformers at\nvarious numerical precisions and formats. INT-FP-QSim leverages existing\nopen-source repositories such as TensorRT, QPytorch and AIMET for a combined\nsimulator that supports various floating point and integer formats. With the\nhelp of our simulator, we survey the impact of different numerical formats on\nthe performance of LLMs and vision transformers at 4-bit weights and 4-bit or\n8-bit activations. We also compare recently proposed methods like Adaptive\nBlock Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We\nhope INT-FP-QSim will enable researchers to flexibly simulate models at various\nprecisions to support further research in quantization of LLMs and vision\ntransformers.",
        "translated": ""
    },
    {
        "title": "Equivariant Single View Pose Prediction Via Induced and Restricted\n  Representations",
        "url": "http://arxiv.org/abs/2307.03704v1",
        "pub_date": "2023-07-07",
        "summary": "Learning about the three-dimensional world from two-dimensional images is a\nfundamental problem in computer vision. An ideal neural network architecture\nfor such tasks would leverage the fact that objects can be rotated and\ntranslated in three dimensions to make predictions about novel images. However,\nimposing SO(3)-equivariance on two-dimensional inputs is difficult because the\ngroup of three-dimensional rotations does not have a natural action on the\ntwo-dimensional plane. Specifically, it is possible that an element of SO(3)\nwill rotate an image out of plane. We show that an algorithm that learns a\nthree-dimensional representation of the world from two dimensional images must\nsatisfy certain geometric consistency properties which we formulate as\nSO(2)-equivariance constraints. We use the induced and restricted\nrepresentations of SO(2) on SO(3) to construct and classify architectures which\nsatisfy these geometric consistency constraints. We prove that any architecture\nwhich respects said consistency constraints can be realized as an instance of\nour construction. We show that three previously proposed neural architectures\nfor 3D pose prediction are special cases of our construction. We propose a new\nalgorithm that is a learnable generalization of previously considered methods.\nWe test our architecture on three pose predictions task and achieve SOTA\nresults on both the PASCAL3D+ and SYMSOL pose estimation tasks.",
        "translated": ""
    },
    {
        "title": "Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware\n  Artery Segmentation",
        "url": "http://arxiv.org/abs/2307.03698v1",
        "pub_date": "2023-07-07",
        "summary": "Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial\ndiseases, mainly due to the advantages of being non-invasive, radiation-free,\nand real-time. In order to provide additional information to assist clinicians\nin diagnosis, the tubular structures are often segmented from US images. To\nimprove the artery segmentation accuracy and stability during scans, this work\npresents a novel pulsation-assisted segmentation neural network (PAS-NN) by\nexplicitly taking advantage of the cardiac-induced motions. Motion\nmagnification techniques are employed to amplify the subtle motion within the\nfrequency band of interest to extract the pulsation signals from sequential US\nimages. The extracted real-time pulsation information can help to locate the\narteries on cross-section US images; therefore, we explicitly integrated the\npulsation into the proposed PAS-NN as attention guidance. Notably, a robotic\narm is necessary to provide stable movement during US imaging since magnifying\nthe target motions from the US images captured along a scan path is not\nmanually feasible due to the hand tremor. To validate the proposed robotic US\nsystem for imaging arteries, experiments are carried out on volunteers' carotid\nand radial arteries. The results demonstrated that the PAS-NN could achieve\ncomparable results as state-of-the-art on carotid and can effectively improve\nthe segmentation performance for small vessels (radial artery).",
        "translated": ""
    },
    {
        "title": "Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive\n  Cancer Surgery",
        "url": "http://arxiv.org/abs/2307.03662v1",
        "pub_date": "2023-07-07",
        "summary": "In surgical oncology, it is challenging for surgeons to identify lymph nodes\nand completely resect cancer even with pre-operative imaging systems like PET\nand CT, because of the lack of reliable intraoperative visualization tools.\nEndoscopic radio-guided cancer detection and resection has recently been\nevaluated whereby a novel tethered laparoscopic gamma detector is used to\nlocalize a preoperatively injected radiotracer. This can both enhance the\nendoscopic imaging and complement preoperative nuclear imaging data. However,\ngamma activity visualization is challenging to present to the operator because\nthe probe is non-imaging and it does not visibly indicate the activity\norigination on the tissue surface. Initial failed attempts used segmentation or\ngeometric methods, but led to the discovery that it could be resolved by\nleveraging high-dimensional image features and probe position information. To\ndemonstrate the effectiveness of this solution, we designed and implemented a\nsimple regression network that successfully addressed the problem. To further\nvalidate the proposed solution, we acquired and publicly released two datasets\ncaptured using a custom-designed, portable stereo laparoscope system. Through\nintensive experimentation, we demonstrated that our method can successfully and\neffectively detect the sensing area, establishing a new performance benchmark.\nCode and data are available at\nhttps://github.com/br0202/Sensing_area_detection.git",
        "translated": ""
    },
    {
        "title": "Differentiable Blocks World: Qualitative 3D Decomposition by Rendering\n  Primitives",
        "url": "http://arxiv.org/abs/2307.05473v1",
        "pub_date": "2023-07-11",
        "summary": "Given a set of calibrated images of a scene, we present an approach that\nproduces a simple, compact, and actionable 3D world representation by means of\n3D primitives. While many approaches focus on recovering high-fidelity 3D\nscenes, we focus on parsing a scene into mid-level 3D representations made of a\nsmall set of textured primitives. Such representations are interpretable, easy\nto manipulate and suited for physics-based simulations. Moreover, unlike\nexisting primitive decomposition methods that rely on 3D input data, our\napproach operates directly on images through differentiable rendering.\nSpecifically, we model primitives as textured superquadric meshes and optimize\ntheir parameters from scratch with an image rendering loss. We highlight the\nimportance of modeling transparency for each primitive, which is critical for\noptimization and also enables handling varying numbers of primitives. We show\nthat the resulting textured primitives faithfully reconstruct the input images\nand accurately model the visible 3D points, while providing amodal shape\ncompletions of unseen object regions. We compare our approach to the state of\nthe art on diverse scenes from DTU, and demonstrate its robustness on real-life\ncaptures from BlendedMVS and Nerfstudio. We also showcase how our results can\nbe used to effortlessly edit a scene or perform physical simulations. Code and\nvideo results are available at https://www.tmonnier.com/DBW .",
        "translated": ""
    },
    {
        "title": "Scale Alone Does not Improve Mechanistic Interpretability in Vision\n  Models",
        "url": "http://arxiv.org/abs/2307.05471v1",
        "pub_date": "2023-07-11",
        "summary": "In light of the recent widespread adoption of AI systems, understanding the\ninternal information processing of neural networks has become increasingly\ncritical. Most recently, machine vision has seen remarkable progress by scaling\nneural networks to unprecedented levels in dataset and model size. We here ask\nwhether this extraordinary increase in scale also positively impacts the field\nof mechanistic interpretability. In other words, has our understanding of the\ninner workings of scaled neural networks improved as well? We here use a\npsychophysical paradigm to quantify mechanistic interpretability for a diverse\nsuite of models and find no scaling effect for interpretability - neither for\nmodel nor dataset size. Specifically, none of the nine investigated\nstate-of-the-art models are easier to interpret than the GoogLeNet model from\nalmost a decade ago. Latest-generation vision models appear even less\ninterpretable than older architectures, hinting at a regression rather than\nimprovement, with modern models sacrificing interpretability for accuracy.\nThese results highlight the need for models explicitly designed to be\nmechanistically interpretable and the need for more helpful interpretability\nmethods to increase our understanding of networks at an atomic level. We\nrelease a dataset containing more than 120'000 human responses from our\npsychophysical evaluation of 767 units across nine models. This dataset is\nmeant to facilitate research on automated instead of human-based\ninterpretability evaluations that can ultimately be leveraged to directly\noptimize the mechanistic interpretability of models.",
        "translated": ""
    },
    {
        "title": "My3DGen: Building Lightweight Personalized 3D Generative Model",
        "url": "http://arxiv.org/abs/2307.05468v1",
        "pub_date": "2023-07-11",
        "summary": "Our paper presents My3DGen, a practical system for creating a personalized\nand lightweight 3D generative prior using as few as 10 images. My3DGen can\nreconstruct multi-view consistent images from an input test image, and generate\nnovel appearances by interpolating between any two images of the same\nindividual. While recent studies have demonstrated the effectiveness of\npersonalized generative priors in producing high-quality 2D portrait\nreconstructions and syntheses, to the best of our knowledge, we are the first\nto develop a personalized 3D generative prior. Instead of fine-tuning a large\npre-trained generative model with millions of parameters to achieve\npersonalization, we propose a parameter-efficient approach. Our method involves\nutilizing a pre-trained model with fixed weights as a generic prior, while\ntraining a separate personalized prior through low-rank decomposition of the\nweights in each convolution and fully connected layer. However,\nparameter-efficient few-shot fine-tuning on its own often leads to overfitting.\nTo address this, we introduce a regularization technique based on symmetry of\nhuman faces. This regularization enforces that novel view renderings of a\ntraining sample, rendered from symmetric poses, exhibit the same identity. By\nincorporating this symmetry prior, we enhance the quality of reconstruction and\nsynthesis, particularly for non-frontal (profile) faces. Our final system\ncombines low-rank fine-tuning with symmetry regularization and significantly\nsurpasses the performance of pre-trained models, e.g. EG3D. It introduces only\napproximately 0.6 million additional parameters per identity compared to 31\nmillion for full finetuning of the original model. As a result, our system\nachieves a 50-fold reduction in model size without sacrificing the quality of\nthe generated 3D faces. Code will be available at our project page:\nhttps://luchaoqi.github.io/my3dgen.",
        "translated": ""
    },
    {
        "title": "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the\n  Backbone",
        "url": "http://arxiv.org/abs/2307.05463v1",
        "pub_date": "2023-07-11",
        "summary": "Video-language pre-training (VLP) has become increasingly important due to\nits ability to generalize to various vision and language tasks. However,\nexisting egocentric VLP frameworks utilize separate video and language encoders\nand learn task-specific cross-modal information only during fine-tuning,\nlimiting the development of a unified system. In this work, we introduce the\nsecond generation of egocentric video-language pre-training (EgoVLPv2), a\nsignificant improvement from the previous generation, by incorporating\ncross-modal fusion directly into the video and language backbones. EgoVLPv2\nlearns strong video-text representation during pre-training and reuses the\ncross-modal attention modules to support different downstream tasks in a\nflexible and efficient manner, reducing fine-tuning costs. Moreover, our\nproposed fusion in the backbone strategy is more lightweight and\ncompute-efficient than stacking additional fusion-specific layers. Extensive\nexperiments on a wide range of VL tasks demonstrate the effectiveness of\nEgoVLPv2 by achieving consistent state-of-the-art performance over strong\nbaselines across all downstream. Our project page can be found at\nhttps://shramanpramanick.github.io/EgoVLPv2/.",
        "translated": ""
    },
    {
        "title": "Efficient 3D Articulated Human Generation with Layered Surface Volumes",
        "url": "http://arxiv.org/abs/2307.05462v1",
        "pub_date": "2023-07-11",
        "summary": "Access to high-quality and diverse 3D articulated digital human assets is\ncrucial in various applications, ranging from virtual reality to social\nplatforms. Generative approaches, such as 3D generative adversarial networks\n(GANs), are rapidly replacing laborious manual content creation tools. However,\nexisting 3D GAN frameworks typically rely on scene representations that\nleverage either template meshes, which are fast but offer limited quality, or\nvolumes, which offer high capacity but are slow to render, thereby limiting the\n3D fidelity in GAN settings. In this work, we introduce layered surface volumes\n(LSVs) as a new 3D object representation for articulated digital humans. LSVs\nrepresent a human body using multiple textured mesh layers around a\nconventional template. These layers are rendered using alpha compositing with\nfast differentiable rasterization, and they can be interpreted as a volumetric\nrepresentation that allocates its capacity to a manifold of finite thickness\naround the template. Unlike conventional single-layer templates that struggle\nwith representing fine off-surface details like hair or accessories, our\nsurface volumes naturally capture such details. LSVs can be articulated, and\nthey exhibit exceptional efficiency in GAN settings, where a 2D generator\nlearns to synthesize the RGBA textures for the individual layers. Trained on\nunstructured, single-view 2D image datasets, our LSV-GAN generates high-quality\nand view-consistent 3D articulated digital humans without the need for\nview-inconsistent 2D upsampling networks.",
        "translated": ""
    },
    {
        "title": "Neural Free-Viewpoint Relighting for Glossy Indirect Illumination",
        "url": "http://arxiv.org/abs/2307.06335v1",
        "pub_date": "2023-07-12",
        "summary": "Precomputed Radiance Transfer (PRT) remains an attractive solution for\nreal-time rendering of complex light transport effects such as glossy global\nillumination. After precomputation, we can relight the scene with new\nenvironment maps while changing viewpoint in real-time. However, practical PRT\nmethods are usually limited to low-frequency spherical harmonic lighting.\nAll-frequency techniques using wavelets are promising but have so far had\nlittle practical impact. The curse of dimensionality and much higher data\nrequirements have typically limited them to relighting with fixed view or only\ndirect lighting with triple product integrals. In this paper, we demonstrate a\nhybrid neural-wavelet PRT solution to high-frequency indirect illumination,\nincluding glossy reflection, for relighting with changing view. Specifically,\nwe seek to represent the light transport function in the Haar wavelet basis.\nFor global illumination, we learn the wavelet transport using a small\nmulti-layer perceptron (MLP) applied to a feature field as a function of\nspatial location and wavelet index, with reflected direction and material\nparameters being other MLP inputs. We optimize/learn the feature field\n(compactly represented by a tensor decomposition) and MLP parameters from\nmultiple images of the scene under different lighting and viewing conditions.\nWe demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed\nrendering of challenging scenes involving view-dependent reflections and even\ncaustics.",
        "translated": ""
    },
    {
        "title": "Deep Learning of Crystalline Defects from TEM images: A Solution for the\n  Problem of \"Never Enough Training Data\"",
        "url": "http://arxiv.org/abs/2307.06322v1",
        "pub_date": "2023-07-12",
        "summary": "Crystalline defects, such as line-like dislocations, play an important role\nfor the performance and reliability of many metallic devices. Their interaction\nand evolution still poses a multitude of open questions to materials science\nand materials physics. In-situ TEM experiments can provide important insights\ninto how dislocations behave and move. During such experiments, the dislocation\nmicrostructure is captured in form of videos. The analysis of individual video\nframes can provide useful insights but is limited by the capabilities of\nautomated identification, digitization, and quantitative extraction of the\ndislocations as curved objects. The vast amount of data also makes manual\nannotation very time consuming, thereby limiting the use of Deep\nLearning-based, automated image analysis and segmentation of the dislocation\nmicrostructure. In this work, a parametric model for generating synthetic\ntraining data for segmentation of dislocations is developed. Even though domain\nscientists might dismiss synthetic training images sometimes as too artificial,\nour findings show that they can result in superior performance, particularly\nregarding the generalizing of the Deep Learning models with respect to\ndifferent microstructures and imaging conditions. Additionally, we propose an\nenhanced deep learning method optimized for segmenting overlapping or\nintersecting dislocation lines. Upon testing this framework on four distinct\nreal datasets, we find that our synthetic training data are able to yield\nhigh-quality results also on real images-even more so if fine-tune on a few\nreal images was done.",
        "translated": ""
    },
    {
        "title": "Correlation-Aware Mutual Learning for Semi-supervised Medical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2307.06312v1",
        "pub_date": "2023-07-12",
        "summary": "Semi-supervised learning has become increasingly popular in medical image\nsegmentation due to its ability to leverage large amounts of unlabeled data to\nextract additional information. However, most existing semi-supervised\nsegmentation methods only focus on extracting information from unlabeled data,\ndisregarding the potential of labeled data to further improve the performance\nof the model. In this paper, we propose a novel Correlation Aware Mutual\nLearning (CAML) framework that leverages labeled data to guide the extraction\nof information from unlabeled data. Our approach is based on a mutual learning\nstrategy that incorporates two modules: the Cross-sample Mutual Attention\nModule (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module\nestablishes dense cross-sample correlations among a group of samples, enabling\nthe transfer of label prior knowledge to unlabeled data. The OCC module\nconstructs omni-correlations between the unlabeled and labeled datasets and\nregularizes dual models by constraining the omni-correlation matrix of each\nsub-model to be consistent. Experiments on the Atrial Segmentation Challenge\ndataset demonstrate that our proposed approach outperforms state-of-the-art\nmethods, highlighting the effectiveness of our framework in medical image\nsegmentation tasks. The codes, pre-trained weights, and data are publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Facial Reenactment Through a Personalized Generator",
        "url": "http://arxiv.org/abs/2307.06307v1",
        "pub_date": "2023-07-12",
        "summary": "In recent years, the role of image generative models in facial reenactment\nhas been steadily increasing. Such models are usually subject-agnostic and\ntrained on domain-wide datasets. The appearance of the reenacted individual is\nlearned from a single image, and hence, the entire breadth of the individual's\nappearance is not entirely captured, leading these methods to resort to\nunfaithful hallucination. Thanks to recent advancements, it is now possible to\ntrain a personalized generative model tailored specifically to a given\nindividual. In this paper, we propose a novel method for facial reenactment\nusing a personalized generator. We train the generator using frames from a\nshort, yet varied, self-scan video captured using a simple commodity camera.\nImages synthesized by the personalized generator are guaranteed to preserve\nidentity. The premise of our work is that the task of reenactment is thus\nreduced to accurately mimicking head poses and expressions. To this end, we\nlocate the desired frames in the latent space of the personalized generator\nusing carefully designed latent optimization. Through extensive evaluation, we\ndemonstrate state-of-the-art performance for facial reenactment. Furthermore,\nwe show that since our reenactment takes place in a semantic latent space, it\ncan be semantically edited and stylized in post-processing.",
        "translated": ""
    },
    {
        "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and\n  Resolution",
        "url": "http://arxiv.org/abs/2307.06304v1",
        "pub_date": "2023-07-12",
        "summary": "The ubiquitous and demonstrably suboptimal choice of resizing images to a\nfixed resolution before processing them with computer vision models has not yet\nbeen successfully challenged. However, models such as the Vision Transformer\n(ViT) offer flexible sequence-based modeling, and hence varying input sequence\nlengths. We take advantage of this with NaViT (Native Resolution ViT) which\nuses sequence packing during training to process inputs of arbitrary\nresolutions and aspect ratios. Alongside flexible model usage, we demonstrate\nimproved training efficiency for large-scale supervised and contrastive\nimage-text pretraining. NaViT can be efficiently transferred to standard tasks\nsuch as image and video classification, object detection, and semantic\nsegmentation and leads to improved results on robustness and fairness\nbenchmarks. At inference time, the input resolution flexibility can be used to\nsmoothly navigate the test-time cost-performance trade-off. We believe that\nNaViT marks a departure from the standard, CNN-designed, input and modelling\npipeline used by most computer vision models, and represents a promising\ndirection for ViTs.",
        "translated": ""
    },
    {
        "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image\n  Models",
        "url": "http://arxiv.org/abs/2307.06949v1",
        "pub_date": "2023-07-13",
        "summary": "Personalization has emerged as a prominent aspect within the field of\ngenerative AI, enabling the synthesis of individuals in diverse contexts and\nstyles, while retaining high-fidelity to their identities. However, the process\nof personalization presents inherent challenges in terms of time and memory\nrequirements. Fine-tuning each personalized model needs considerable GPU time\ninvestment, and storing a personalized model per subject can be demanding in\nterms of storage capacity. To overcome these challenges, we propose\nHyperDreamBooth-a hypernetwork capable of efficiently generating a small set of\npersonalized weights from a single image of a person. By composing these\nweights into the diffusion model, coupled with fast finetuning, HyperDreamBooth\ncan generate a person's face in various contexts and styles, with high subject\ndetails while also preserving the model's crucial knowledge of diverse styles\nand semantic modifications. Our method achieves personalization on faces in\nroughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual\nInversion, using as few as one reference image, with the same quality and style\ndiversity as DreamBooth. Also our method yields a model that is 10000x smaller\nthan a normal DreamBooth model. Project page: https://hyperdreambooth.github.io",
        "translated": ""
    },
    {
        "title": "Self-regulating Prompts: Foundational Model Adaptation without\n  Forgetting",
        "url": "http://arxiv.org/abs/2307.06948v1",
        "pub_date": "2023-07-13",
        "summary": "Prompt learning has emerged as an efficient alternative for fine-tuning\nfoundational models, such as CLIP, for various downstream tasks. Conventionally\ntrained using the task-specific objective, i.e., cross-entropy loss, prompts\ntend to overfit downstream data distributions and find it challenging to\ncapture task-agnostic general features from the frozen CLIP. This leads to the\nloss of the model's original generalization capability. To address this issue,\nour work introduces a self-regularization framework for prompting called\nPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the\nprompts to optimize for both task-specific and task-agnostic general\nrepresentations using a three-pronged approach by: (a) regulating {prompted}\nrepresentations via mutual agreement maximization with the frozen model, (b)\nregulating with self-ensemble of prompts over the training trajectory to encode\ntheir complementary strengths, and (c) regulating with textual diversity to\nmitigate sample diversity imbalance with the visual branch. To the best of our\nknowledge, this is the first regularization framework for prompt learning that\navoids overfitting by jointly attending to pre-trained model features, the\ntraining trajectory during prompting, and the textual diversity. PromptSRC\nexplicitly steers the prompts to learn a representation space that maximizes\nperformance on downstream tasks without compromising CLIP generalization. We\nperform extensive experiments on 4 benchmarks where PromptSRC overall performs\nfavorably well compared to the existing methods. Our code and pre-trained\nmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.",
        "translated": ""
    },
    {
        "title": "Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action\n  Recognition",
        "url": "http://arxiv.org/abs/2307.06947v1",
        "pub_date": "2023-07-13",
        "summary": "Recent video recognition models utilize Transformer models for long-range\nspatio-temporal context modeling. Video transformer designs are based on\nself-attention that can model global context at a high computational cost. In\ncomparison, convolutional designs for videos offer an efficient alternative but\nlack long-range dependency modeling. Towards achieving the best of both\ndesigns, this work proposes Video-FocalNet, an effective and efficient\narchitecture for video recognition that models both local and global contexts.\nVideo-FocalNet is based on a spatio-temporal focal modulation architecture that\nreverses the interaction and aggregation steps of self-attention for better\nefficiency. Further, the aggregation step and the interaction step are both\nimplemented using efficient convolution and element-wise multiplication\noperations that are computationally less expensive than their self-attention\ncounterparts on video representations. We extensively explore the design space\nof focal modulation-based spatio-temporal context modeling and demonstrate our\nparallel spatial and temporal encoding design to be the optimal choice.\nVideo-FocalNets perform favorably well against the state-of-the-art\ntransformer-based models for video recognition on three large-scale datasets\n(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our\ncode/models are released at https://github.com/TalalWasim/Video-FocalNets.",
        "translated": ""
    },
    {
        "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding\n  and Generation",
        "url": "http://arxiv.org/abs/2307.06942v1",
        "pub_date": "2023-07-13",
        "summary": "This paper introduces InternVid, a large-scale video-centric multimodal\ndataset that enables learning powerful and transferable video-text\nrepresentations for multimodal understanding and generation. The InternVid\ndataset contains over 7 million videos lasting nearly 760K hours, yielding 234M\nvideo clips accompanied by detailed descriptions of total 4.1B words. Our core\ncontribution is to develop a scalable approach to autonomously build a\nhigh-quality video-text dataset with large language models (LLM), thereby\nshowcasing its efficacy in learning video-language representation at scale.\nSpecifically, we utilize a multi-scale approach to generate video-related\ndescriptions. Furthermore, we introduce ViCLIP, a video-text representation\nlearning model based on ViT-L. Learned on InternVid via contrastive learning,\nthis model demonstrates leading zero-shot action recognition and competitive\nvideo retrieval performance. Beyond basic video understanding tasks like\nrecognition and retrieval, our dataset and model have broad applications. They\nare particularly beneficial for generating interleaved video-text data for\nlearning a video-centric dialogue system, advancing video-to-text and\ntext-to-video generation research. These proposed resources provide a tool for\nresearchers and practitioners interested in multimodal video understanding and\ngeneration.",
        "translated": ""
    },
    {
        "title": "On the Connection between Game-Theoretic Feature Attributions and\n  Counterfactual Explanations",
        "url": "http://arxiv.org/abs/2307.06941v1",
        "pub_date": "2023-07-13",
        "summary": "Explainable Artificial Intelligence (XAI) has received widespread interest in\nrecent years, and two of the most popular types of explanations are feature\nattributions, and counterfactual explanations. These classes of approaches have\nbeen largely studied independently and the few attempts at reconciling them\nhave been primarily empirical. This work establishes a clear theoretical\nconnection between game-theoretic feature attributions, focusing on but not\nlimited to SHAP, and counterfactuals explanations. After motivating operative\nchanges to Shapley values based feature attributions and counterfactual\nexplanations, we prove that, under conditions, they are in fact equivalent. We\nthen extend the equivalency result to game-theoretic solution concepts beyond\nShapley values. Moreover, through the analysis of the conditions of such\nequivalence, we shed light on the limitations of naively using counterfactual\nexplanations to provide feature importances. Experiments on three datasets\nquantitatively show the difference in explanations at every stage of the\nconnection between the two approaches and corroborate the theoretical findings.",
        "translated": ""
    },
    {
        "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion\n  Synthesis",
        "url": "http://arxiv.org/abs/2307.07511v1",
        "pub_date": "2023-07-14",
        "summary": "We address the problem of generating realistic 3D motions of humans\ninteracting with objects in a scene. Our key idea is to create a neural\ninteraction field attached to a specific object, which outputs the distance to\nthe valid interaction manifold given a human pose as input. This interaction\nfield guides the sampling of an object-conditioned human motion diffusion\nmodel, so as to encourage plausible contacts and affordance semantics. To\nsupport interactions with scarcely available data, we propose an automated\nsynthetic data pipeline. For this, we seed a pre-trained motion model, which\nhas priors for the basics of human movement, with interaction-specific anchor\nposes extracted from limited motion capture data. Using our guided diffusion\nmodel trained on generated synthetic data, we synthesize realistic motions for\nsitting and lifting with several objects, outperforming alternative approaches\nin terms of motion quality and successful action completion. We call our\nframework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.",
        "translated": ""
    },
    {
        "title": "Brain Tumor Detection using Convolutional Neural Networks with Skip\n  Connections",
        "url": "http://arxiv.org/abs/2307.07503v1",
        "pub_date": "2023-07-14",
        "summary": "In this paper, we present different architectures of Convolutional Neural\nNetworks (CNN) to analyze and classify the brain tumors into benign and\nmalignant types using the Magnetic Resonance Imaging (MRI) technique. Different\nCNN architecture optimization techniques such as widening and deepening of the\nnetwork and adding skip connections are applied to improve the accuracy of the\nnetwork. Results show that a subset of these techniques can judiciously be used\nto outperform a baseline CNN model used for the same purpose.",
        "translated": ""
    },
    {
        "title": "TALL: Thumbnail Layout for Deepfake Video Detection",
        "url": "http://arxiv.org/abs/2307.07494v1",
        "pub_date": "2023-07-14",
        "summary": "The growing threats of deepfakes to society and cybersecurity have raised\nenormous public concerns, and increasing efforts have been devoted to this\ncritical topic of deepfake video detection. Existing video methods achieve good\nperformance but are computationally intensive. This paper introduces a simple\nyet effective strategy named Thumbnail Layout (TALL), which transforms a video\nclip into a pre-defined layout to realize the preservation of spatial and\ntemporal dependencies. Specifically, consecutive frames are masked in a fixed\nposition in each frame to improve generalization, then resized to sub-images\nand rearranged into a pre-defined layout as the thumbnail. TALL is\nmodel-agnostic and extremely simple by only modifying a few lines of code.\nInspired by the success of vision transformers, we incorporate TALL into Swin\nTransformer, forming an efficient and effective method TALL-Swin. Extensive\nexperiments on intra-dataset and cross-dataset validate the validity and\nsuperiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\\%$ AUC on the\nchallenging cross-dataset task, FaceForensics++ $\\to$ Celeb-DF. The code is\navailable at https://github.com/rainy-xu/TALL4Deepfake.",
        "translated": ""
    },
    {
        "title": "PseudoCal: A Source-Free Approach to Unsupervised Uncertainty\n  Calibration in Domain Adaptation",
        "url": "http://arxiv.org/abs/2307.07489v1",
        "pub_date": "2023-07-14",
        "summary": "Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in\nimproving the accuracy of models for unlabeled target domains. However, the\ncalibration of predictive uncertainty in the target domain, a crucial aspect of\nthe safe deployment of UDA models, has received limited attention. The\nconventional in-domain calibration method, \\textit{temperature scaling}\n(TempScal), encounters challenges due to domain distribution shifts and the\nabsence of labeled target domain data. Recent approaches have employed\nimportance-weighting techniques to estimate the target-optimal temperature\nbased on re-weighted labeled source data. Nonetheless, these methods require\nsource data and suffer from unreliable density estimates under severe domain\nshifts, rendering them unsuitable for source-free UDA settings. To overcome\nthese limitations, we propose PseudoCal, a source-free calibration method that\nexclusively relies on unlabeled target data. Unlike previous approaches that\ntreat UDA calibration as a \\textit{covariate shift} problem, we consider it as\nan unsupervised calibration problem specific to the target domain. Motivated by\nthe factorization of the negative log-likelihood (NLL) objective in TempScal,\nwe generate a labeled pseudo-target set that captures the structure of the real\ntarget. By doing so, we transform the unsupervised calibration problem into a\nsupervised one, enabling us to effectively address it using widely-used\nin-domain methods like TempScal. Finally, we thoroughly evaluate the\ncalibration performance of PseudoCal by conducting extensive experiments on 10\nUDA methods, considering both traditional UDA settings and recent source-free\nUDA scenarios. The experimental results consistently demonstrate the superior\nperformance of PseudoCal, exhibiting significantly reduced calibration error\ncompared to existing calibration methods.",
        "translated": ""
    },
    {
        "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
        "url": "http://arxiv.org/abs/2307.07487v1",
        "pub_date": "2023-07-14",
        "summary": "In this work, we introduce a self-supervised feature representation learning\nframework DreamTeacher that utilizes generative networks for pre-training\ndownstream image backbones. We propose to distill knowledge from a trained\ngenerative model into standard image backbones that have been well engineered\nfor specific perception tasks. We investigate two types of knowledge\ndistillation: 1) distilling learned generative features onto target image\nbackbones as an alternative to pretraining these backbones on large labeled\ndatasets such as ImageNet, and 2) distilling labels obtained from generative\nnetworks with task heads onto logits of target backbones. We perform extensive\nanalyses on multiple generative models, dense prediction benchmarks, and\nseveral pre-training regimes. We empirically find that our DreamTeacher\nsignificantly outperforms existing self-supervised representation learning\napproaches across the board. Unsupervised ImageNet pre-training with\nDreamTeacher leads to significant improvements over ImageNet classification\npre-training on downstream datasets, showcasing generative models, and\ndiffusion generative models specifically, as a promising approach to\nrepresentation learning on large, diverse datasets without requiring manual\nannotation.",
        "translated": ""
    },
    {
        "title": "Diffusion Models Beat GANs on Image Classification",
        "url": "http://arxiv.org/abs/2307.08702v1",
        "pub_date": "2023-07-17",
        "summary": "While many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified\nrepresentation learner: a model which uses a single pre-training stage to\naddress both families of tasks simultaneously. We identify diffusion models as\na prime candidate. Diffusion models have risen to prominence as a\nstate-of-the-art method for image generation, denoising, inpainting,\nsuper-resolution, manipulation, etc. Such models involve training a U-Net to\niteratively predict and remove noise, and the resulting model can synthesize\nhigh fidelity, diverse, novel images. The U-Net architecture, as a\nconvolution-based architecture, generates a diverse set of feature\nrepresentations in the form of intermediate feature maps. We present our\nfindings that these embeddings are useful beyond the noise prediction task, as\nthey contain discriminative information and can also be leveraged for\nclassification. We explore optimal methods for extracting and using these\nembeddings for classification tasks, demonstrating promising results on the\nImageNet classification task. We find that with careful feature selection and\npooling, diffusion models outperform comparable generative-discriminative\nmethods such as BigBiGAN for classification tasks. We investigate diffusion\nmodels in the transfer learning regime, examining their performance on several\nfine-grained visual classification datasets. We compare these embeddings to\nthose generated by competing architectures and pre-trainings for classification\ntasks.",
        "translated": ""
    },
    {
        "title": "Fast model inference and training on-board of Satellites",
        "url": "http://arxiv.org/abs/2307.08700v1",
        "pub_date": "2023-07-17",
        "summary": "Artificial intelligence onboard satellites has the potential to reduce data\ntransmission requirements, enable real-time decision-making and collaboration\nwithin constellations. This study deploys a lightweight foundational model\ncalled RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational\nauto-encoder (VAE) that generates compressed latent vectors from small image\ntiles, enabling several downstream tasks. In this work we demonstrate the\nreliable use of RaVAEn onboard a satellite, achieving an encoding time of\n0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast\nfew-shot training onboard a satellite using the latent representation of data.\nWe compare the deployment of the model on the on-board CPU and on the available\nMyriad vision processing unit (VPU) accelerator. To our knowledge, this work\nshows for the first time the deployment of a multi-task model on-board a\nCubeSat and the on-board training of a machine learning model.",
        "translated": ""
    },
    {
        "title": "Pair then Relation: Pair-Net for Panoptic Scene Graph Generation",
        "url": "http://arxiv.org/abs/2307.08699v1",
        "pub_date": "2023-07-17",
        "summary": "Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation\n(SGG) that aims to create a more comprehensive scene graph representation using\npanoptic segmentation instead of boxes. However, current PSG methods have\nlimited performance, which can hinder downstream task development. To improve\nPSG methods, we conducted an in-depth analysis to identify the bottleneck of\nthe current PSG models, finding that inter-object pair-wise recall is a crucial\nfactor which was ignored by previous PSG methods. Based on this, we present a\nnovel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal\nNetwork (PPN) to learn and filter sparse pair-wise relationships between\nsubjects and objects. We also observed the sparse nature of object pairs and\nused this insight to design a lightweight Matrix Learner within the PPN.\nThrough extensive ablation and analysis, our approach significantly improves\nupon leveraging the strong segmenter baseline. Notably, our approach achieves\nnew state-of-the-art results on the PSG benchmark, with over 10% absolute gains\ncompared to PSGFormer. The code of this paper is publicly available at\nhttps://github.com/king159/Pair-Net.",
        "translated": ""
    },
    {
        "title": "Flow Matching in Latent Space",
        "url": "http://arxiv.org/abs/2307.08698v1",
        "pub_date": "2023-07-17",
        "summary": "Flow matching is a recent framework to train generative models that exhibits\nimpressive empirical performance while being relatively easier to train\ncompared with diffusion-based models. Despite its advantageous properties,\nprior methods still face the challenges of expensive computing and a large\nnumber of function evaluations of off-the-shelf solvers in the pixel space.\nFurthermore, although latent-based generative methods have shown great success\nin recent years, this particular model type remains underexplored in this area.\nIn this work, we propose to apply flow matching in the latent spaces of\npretrained autoencoders, which offers improved computational efficiency and\nscalability for high-resolution image synthesis. This enables flow-matching\ntraining on constrained computational resources while maintaining their quality\nand flexibility. Additionally, our work stands as a pioneering contribution in\nthe integration of various conditions into flow matching for conditional\ngeneration tasks, including label-conditioned image generation, image\ninpainting, and semantic-to-image generation. Through extensive experiments,\nour approach demonstrates its effectiveness in both quantitative and\nqualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church &amp;\nBedroom, and ImageNet. We also provide a theoretical control of the\nWasserstein-2 distance between the reconstructed latent flow distribution and\ntrue data distribution, showing it is upper-bounded by the latent flow matching\nobjective. Our code will be available at\nhttps://github.com/VinAIResearch/LFM.git.",
        "translated": ""
    },
    {
        "title": "Neural Video Depth Stabilizer",
        "url": "http://arxiv.org/abs/2307.08695v1",
        "pub_date": "2023-07-17",
        "summary": "Video depth estimation aims to infer temporally consistent depth. Some\nmethods achieve temporal consistency by finetuning a single-image depth model\nduring test time using geometry and re-projection constraints, which is\ninefficient and not robust. An alternative approach is to learn how to enforce\ntemporal consistency from data, but this requires well-designed models and\nsufficient video depth data. To address these challenges, we propose a\nplug-and-play framework called Neural Video Depth Stabilizer (NVDS) that\nstabilizes inconsistent depth estimations and can be applied to different\nsingle-image depth models without extra effort. We also introduce a large-scale\ndataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with\nover two million frames, making it the largest natural-scene video depth\ndataset to our knowledge. We evaluate our method on the VDW dataset as well as\ntwo public benchmarks and demonstrate significant improvements in consistency,\naccuracy, and efficiency compared to previous approaches. Our work serves as a\nsolid baseline and provides a data foundation for learning-based video depth\nmodels. We will release our dataset and code for future research.",
        "translated": ""
    },
    {
        "title": "AnyDoor: Zero-shot Object-level Image Customization",
        "url": "http://arxiv.org/abs/2307.09481v1",
        "pub_date": "2023-07-18",
        "summary": "This work presents AnyDoor, a diffusion-based image generator with the power\nto teleport target objects to new scenes at user-specified locations in a\nharmonious way. Instead of tuning parameters for each object, our model is\ntrained only once and effortlessly generalizes to diverse object-scene\ncombinations at the inference stage. Such a challenging zero-shot setting\nrequires an adequate characterization of a certain object. To this end, we\ncomplement the commonly used identity feature with detail features, which are\ncarefully designed to maintain texture details yet allow versatile local\nvariations (e.g., lighting, orientation, posture, etc.), supporting the object\nin favorably blending with different surroundings. We further propose to borrow\nknowledge from video datasets, where we can observe various forms (i.e., along\nthe time axis) of a single object, leading to stronger model generalizability\nand robustness. Extensive experiments demonstrate the superiority of our\napproach over existing alternatives as well as its great potential in\nreal-world applications, such as virtual try-on and object moving. Project page\nis https://damo-vilab.github.io/AnyDoor-Page/.",
        "translated": ""
    },
    {
        "title": "FACTS: Facial Animation Creation using the Transfer of Styles",
        "url": "http://arxiv.org/abs/2307.09480v1",
        "pub_date": "2023-07-18",
        "summary": "The ability to accurately capture and express emotions is a critical aspect\nof creating believable characters in video games and other forms of\nentertainment. Traditionally, this animation has been achieved with artistic\neffort or performance capture, both requiring costs in time and labor. More\nrecently, audio-driven models have seen success, however, these often lack\nexpressiveness in areas not correlated to the audio signal. In this paper, we\npresent a novel approach to facial animation by taking existing animations and\nallowing for the modification of style characteristics. Specifically, we\nexplore the use of a StarGAN to enable the conversion of 3D facial animations\ninto different emotions and person-specific styles. We are able to maintain the\nlip-sync of the animations with this method thanks to the use of a novel\nviseme-preserving loss.",
        "translated": ""
    },
    {
        "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring\n  Instruction Tuning",
        "url": "http://arxiv.org/abs/2307.09474v1",
        "pub_date": "2023-07-18",
        "summary": "Human-AI interactivity is a critical aspect that reflects the usability of\nmultimodal large language models (MLLMs). However, existing end-to-end MLLMs\nonly allow users to interact with them through language instructions, leading\nto the limitation of the interactive accuracy and efficiency. In this study, we\npresent precise referring instructions that utilize diverse reference\nrepresentations such as points and boxes as referring prompts to refer to the\nspecial region. This enables MLLMs to focus on the region of interest and\nachieve finer-grained interaction. Based on precise referring instruction, we\npropose ChatSpot, a unified end-to-end multimodal large language model that\nsupports diverse forms of interactivity including mouse clicks, drag-and-drop,\nand drawing boxes, which provides a more flexible and seamless interactive\nexperience. We also construct a multi-grained vision-language\ninstruction-following dataset based on existing datasets and GPT-4 generating.\nFurthermore, we design a series of evaluation tasks to assess the effectiveness\nof region recognition and interaction. Experimental results showcase ChatSpot's\npromising performance.",
        "translated": ""
    },
    {
        "title": "GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping",
        "url": "http://arxiv.org/abs/2307.09472v1",
        "pub_date": "2023-07-18",
        "summary": "Efficiency is quite important for 3D lane detection due to practical\ndeployment demand. In this work, we propose a simple, fast, and end-to-end\ndetector that still maintains high detection precision. Specifically, we devise\na set of fully convolutional heads based on row-wise classification. In\ncontrast to previous counterparts, ours supports recognizing both vertical and\nhorizontal lanes. Besides, our method is the first one to perform row-wise\nclassification in bird-eye-view. In the heads, we split feature into multiple\ngroups and every group of feature corresponds to a lane instance. During\ntraining, the predictions are associated with lane labels using the proposed\nsingle-win one-to-one matching to compute loss, and no post-processing\noperation is demanded for inference. In this way, our proposed fully\nconvolutional detector, GroupLane, realizes end-to-end detection like DETR.\nEvaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and\nOpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms\nthe published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane\nvalidation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by\n4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is\nonly 13.3% of it.",
        "translated": ""
    },
    {
        "title": "Occlusion Aware Student Emotion Recognition based on Facial Action Unit\n  Detection",
        "url": "http://arxiv.org/abs/2307.09465v1",
        "pub_date": "2023-07-18",
        "summary": "Given that approximately half of science, technology, engineering, and\nmathematics (STEM) undergraduate students in U.S. colleges and universities\nleave by the end of the first year [15], it is crucial to improve the quality\nof classroom environments. This study focuses on monitoring students' emotions\nin the classroom as an indicator of their engagement and proposes an approach\nto address this issue. The impact of different facial parts on the performance\nof an emotional recognition model is evaluated through experimentation. To test\nthe proposed model under partial occlusion, an artificially occluded dataset is\nintroduced. The novelty of this work lies in the proposal of an occlusion-aware\narchitecture for facial action units (AUs) extraction, which employs attention\nmechanism and adaptive feature learning. The AUs can be used later to classify\nfacial expressions in classroom settings.\n  This research paper's findings provide valuable insights into handling\nocclusion in analyzing facial images for emotional engagement analysis. The\nproposed experiments demonstrate the significance of considering occlusion and\nenhancing the reliability of facial analysis models in classroom environments.\nThese findings can also be extended to other settings where occlusions are\nprevalent.",
        "translated": ""
    },
    {
        "title": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity\n  Human-centric Rendering",
        "url": "http://arxiv.org/abs/2307.10173v1",
        "pub_date": "2023-07-19",
        "summary": "Realistic human-centric rendering plays a key role in both computer vision\nand computer graphics. Rapid progress has been made in the algorithm aspect\nover the years, yet existing human-centric rendering datasets and benchmarks\nare rather impoverished in terms of diversity, which are crucial for rendering\neffect. Researchers are usually constrained to explore and evaluate a small set\nof rendering problems on current datasets, while real-world applications\nrequire methods to be robust across different scenarios. In this work, we\npresent DNA-Rendering, a large-scale, high-fidelity repository of human\nperformance data for neural actor rendering. DNA-Rendering presents several\nalluring attributes. First, our dataset contains over 1500 human subjects, 5000\nmotion sequences, and 67.5M frames' data volume. Second, we provide rich assets\nfor each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models,\ncloth/accessory materials, multi-view images, and videos. These assets boost\nthe current method's accuracy on downstream rendering tasks. Third, we\nconstruct a professional multi-view system to capture data, which contains 60\nsynchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern\ncamera calibration steps, ensuring high-quality resources for task training and\nevaluation. Along with the dataset, we provide a large-scale and quantitative\nbenchmark in full-scale, with multiple tasks to evaluate the existing progress\nof novel view synthesis, novel pose animation synthesis, and novel identity\nrendering methods. In this manuscript, we describe our DNA-Rendering effort as\na revealing of new observations, challenges, and future directions to\nhuman-centric rendering. The dataset, code, and benchmarks will be publicly\navailable at https://dna-rendering.github.io/",
        "translated": ""
    },
    {
        "title": "Adversarial Latent Autoencoder with Self-Attention for Structural Image\n  Synthesis",
        "url": "http://arxiv.org/abs/2307.10166v1",
        "pub_date": "2023-07-19",
        "summary": "Generative Engineering Design approaches driven by Deep Generative Models\n(DGM) have been proposed to facilitate industrial engineering processes. In\nsuch processes, designs often come in the form of images, such as blueprints,\nengineering drawings, and CAD models depending on the level of detail. DGMs\nhave been successfully employed for synthesis of natural images, e.g.,\ndisplaying animals, human faces and landscapes. However, industrial design\nimages are fundamentally different from natural scenes in that they contain\nrich structural patterns and long-range dependencies, which are challenging for\nconvolution-based DGMs to generate. Moreover, DGM-driven generation process is\ntypically triggered based on random noisy inputs, which outputs unpredictable\nsamples and thus cannot perform an efficient industrial design exploration. We\ntackle these challenges by proposing a novel model Self-Attention Adversarial\nLatent Autoencoder (SA-ALAE), which allows generating feasible design images of\ncomplex engineering parts. With SA-ALAE, users can not only explore novel\nvariants of an existing design, but also control the generation process by\noperating in latent space. The potential of SA-ALAE is shown by generating\nengineering blueprints in a real automotive design task.",
        "translated": ""
    },
    {
        "title": "Drone navigation and license place detection for vehicle location in\n  indoor spaces",
        "url": "http://arxiv.org/abs/2307.10165v1",
        "pub_date": "2023-07-19",
        "summary": "Millions of vehicles are transported every year, tightly parked in vessels or\nboats. To reduce the risks of associated safety issues like fires, knowing the\nlocation of vehicles is essential, since different vehicles may need different\nmitigation measures, e.g. electric cars. This work is aimed at creating a\nsolution based on a nano-drone that navigates across rows of parked vehicles\nand detects their license plates. We do so via a wall-following algorithm, and\na CNN trained to detect license plates. All computations are done in real-time\non the drone, which just sends position and detected images that allow the\ncreation of a 2D map with the position of the plates. Our solution is capable\nof reading all plates across eight test cases (with several rows of plates,\ndifferent drone speeds, or low light) by aggregation of measurements across\nseveral drone journeys.",
        "translated": ""
    },
    {
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.10160v1",
        "pub_date": "2023-07-19",
        "summary": "Although deep reinforcement learning (DRL) has shown promising results for\nautonomous navigation in interactive traffic scenarios, existing work typically\nadopts a fixed behavior policy to control social vehicles in the training\nenvironment. This may cause the learned driving policy to overfit the\nenvironment, making it difficult to interact well with vehicles with different,\nunseen behaviors. In this work, we introduce an efficient method to train\ndiverse driving policies for social vehicles as a single meta-policy. By\nrandomizing the interaction-based reward functions of social vehicles, we can\ngenerate diverse objectives and efficiently train the meta-policy through\nguiding policies that achieve specific objectives. We further propose a\ntraining strategy to enhance the robustness of the ego vehicle's driving policy\nusing the environment where social vehicles are controlled by the learned\nmeta-policy. Our method successfully learns an ego driving policy that\ngeneralizes well to unseen situations with out-of-distribution (OOD) social\nagents' behaviors in a challenging uncontrolled T-intersection scenario.",
        "translated": ""
    },
    {
        "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback",
        "url": "http://arxiv.org/abs/2307.10159v1",
        "pub_date": "2023-07-19",
        "summary": "In an era where visual content generation is increasingly driven by machine\nlearning, the integration of human feedback into generative models presents\nsignificant opportunities for enhancing user experience and output quality.\nThis study explores strategies for incorporating iterative human feedback into\nthe generative process of diffusion-based text-to-image models. We propose\nFABRIC, a training-free approach applicable to a wide range of popular\ndiffusion models, which exploits the self-attention layer present in the most\nwidely used architectures to condition the diffusion process on a set of\nfeedback images. To ensure a rigorous assessment of our approach, we introduce\na comprehensive evaluation methodology, offering a robust mechanism to quantify\nthe performance of generative visual models that integrate human feedback. We\nshow that generation results improve over multiple rounds of iterative feedback\nthrough exhaustive analysis, implicitly optimizing arbitrary user preferences.\nThe potential applications of these findings extend to fields such as\npersonalized content creation and customization.",
        "translated": ""
    },
    {
        "title": "PAPR: Proximity Attention Point Rendering",
        "url": "http://arxiv.org/abs/2307.11086v1",
        "pub_date": "2023-07-20",
        "summary": "Learning accurate and parsimonious point cloud representations of scene\nsurfaces from scratch remains a challenge in 3D representation learning.\nExisting point-based methods often suffer from the vanishing gradient problem\nor require a large number of points to accurately model scene geometry and\ntexture. To address these limitations, we propose Proximity Attention Point\nRendering (PAPR), a novel method that consists of a point-based scene\nrepresentation and a differentiable renderer. Our scene representation uses a\npoint cloud where each point is characterized by its spatial position,\nforeground score, and view-independent feature vector. The renderer selects the\nrelevant points for each ray and produces accurate colours using their\nassociated features. PAPR effectively learns point cloud positions to represent\nthe correct scene geometry, even when the initialization drastically differs\nfrom the target geometry. Notably, our method captures fine texture details\nwhile using only a parsimonious set of points. We also demonstrate four\npractical applications of our method: geometry editing, object manipulation,\ntexture transfer, and exposure control. More results and code are available on\nour project website at https://zvict.github.io/papr/.",
        "translated": ""
    },
    {
        "title": "Representation Learning in Anomaly Detection: Successes, Limits and a\n  Grand Challenge",
        "url": "http://arxiv.org/abs/2307.11085v1",
        "pub_date": "2023-07-20",
        "summary": "In this perspective paper, we argue that the dominant paradigm in anomaly\ndetection cannot scale indefinitely and will eventually hit fundamental limits.\nThis is due to the a no free lunch principle for anomaly detection. These\nlimitations can be overcome when there are strong tasks priors, as is the case\nfor many industrial tasks. When such priors do not exists, the task is much\nharder for anomaly detection. We pose two such tasks as grand challenges for\nanomaly detection: i) scientific discovery by anomaly detection ii) a\n\"mini-grand\" challenge of detecting the most anomalous image in the ImageNet\ndataset. We believe new anomaly detection tools and ideas would need to be\ndeveloped to overcome these challenges.",
        "translated": ""
    },
    {
        "title": "GLSFormer : Gated - Long, Short Sequence Transformer for Step\n  Recognition in Surgical Videos",
        "url": "http://arxiv.org/abs/2307.11081v1",
        "pub_date": "2023-07-20",
        "summary": "Automated surgical step recognition is an important task that can\nsignificantly improve patient safety and decision-making during surgeries.\nExisting state-of-the-art methods for surgical step recognition either rely on\nseparate, multi-stage modeling of spatial and temporal information or operate\non short-range temporal resolution when learned jointly. However, the benefits\nof joint modeling of spatio-temporal features and long-range information are\nnot taken in account. In this paper, we propose a vision transformer-based\napproach to jointly learn spatio-temporal features directly from sequence of\nframe-level patches. Our method incorporates a gated-temporal attention\nmechanism that intelligently combines short-term and long-term spatio-temporal\nfeature representations. We extensively evaluate our approach on two cataract\nsurgery video datasets, namely Cataract-101 and D99, and demonstrate superior\nperformance compared to various state-of-the-art methods. These results\nvalidate the suitability of our proposed approach for automated surgical step\nrecognition. Our code is released at:\nhttps://github.com/nisargshah1999/GLSFormer",
        "translated": ""
    },
    {
        "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection",
        "url": "http://arxiv.org/abs/2307.11077v1",
        "pub_date": "2023-07-20",
        "summary": "The paradigm of large-scale pre-training followed by downstream fine-tuning\nhas been widely employed in various object detection algorithms. In this paper,\nwe reveal discrepancies in data, model, and task between the pre-training and\nfine-tuning procedure in existing practices, which implicitly limit the\ndetector's performance, generalization ability, and convergence speed. To this\nend, we propose AlignDet, a unified pre-training framework that can be adapted\nto various existing detectors to alleviate the discrepancies. AlignDet\ndecouples the pre-training process into two stages, i.e., image-domain and\nbox-domain pre-training. The image-domain pre-training optimizes the detection\nbackbone to capture holistic visual abstraction, and box-domain pre-training\nlearns instance-level semantics and task-aware concepts to initialize the parts\nout of the backbone. By incorporating the self-supervised pre-trained\nbackbones, we can pre-train all modules for various detectors in an\nunsupervised paradigm. As depicted in Figure 1, extensive experiments\ndemonstrate that AlignDet can achieve significant improvements across diverse\nprotocols, such as detection algorithm, model backbone, data setting, and\ntraining schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by\n2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.",
        "translated": ""
    },
    {
        "title": "Learning Dense UV Completion for Human Mesh Recovery",
        "url": "http://arxiv.org/abs/2307.11074v1",
        "pub_date": "2023-07-20",
        "summary": "Human mesh reconstruction from a single image is challenging in the presence\nof occlusion, which can be caused by self, objects, or other humans. Existing\nmethods either fail to separate human features accurately or lack proper\nsupervision for feature completion. In this paper, we propose Dense Inpainting\nHuman Mesh Recovery (DIMR), a two-stage method that leverages dense\ncorrespondence maps to handle occlusion. Our method utilizes a dense\ncorrespondence map to separate visible human features and completes human\nfeatures on a structured UV map dense human with an attention-based feature\ncompletion module. We also design a feature inpainting training procedure that\nguides the network to learn from unoccluded features. We evaluate our method on\nseveral datasets and demonstrate its superior performance under heavily\noccluded scenarios compared to other methods. Extensive experiments show that\nour method obviously outperforms prior SOTA methods on heavily occluded images\nand achieves comparable results on the standard benchmarks (3DPW).",
        "translated": ""
    },
    {
        "title": "GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition\n  in Surgical Videos",
        "url": "http://arxiv.org/abs/2307.11081v1",
        "pub_date": "2023-07-20",
        "summary": "Automated surgical step recognition is an important task that can\nsignificantly improve patient safety and decision-making during surgeries.\nExisting state-of-the-art methods for surgical step recognition either rely on\nseparate, multi-stage modeling of spatial and temporal information or operate\non short-range temporal resolution when learned jointly. However, the benefits\nof joint modeling of spatio-temporal features and long-range information are\nnot taken in account. In this paper, we propose a vision transformer-based\napproach to jointly learn spatio-temporal features directly from sequence of\nframe-level patches. Our method incorporates a gated-temporal attention\nmechanism that intelligently combines short-term and long-term spatio-temporal\nfeature representations. We extensively evaluate our approach on two cataract\nsurgery video datasets, namely Cataract-101 and D99, and demonstrate superior\nperformance compared to various state-of-the-art methods. These results\nvalidate the suitability of our proposed approach for automated surgical step\nrecognition. Our code is released at:\nhttps://github.com/nisargshah1999/GLSFormer",
        "translated": ""
    },
    {
        "title": "BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection\n  Evaluation",
        "url": "http://arxiv.org/abs/2307.11748v1",
        "pub_date": "2023-07-21",
        "summary": "Scale-wise evaluation of object detectors is important for real-world\napplications. However, existing metrics are either coarse or not sufficiently\nreliable. In this paper, we propose novel scale-wise metrics that strike a\nbalance between fineness and reliability, using a filter bank consisting of\ntriangular and trapezoidal band-pass filters. We conduct experiments with two\nmethods on two datasets and show that the proposed metrics can highlight the\ndifferences between the methods and between the datasets. Code is available at\nhttps://github.com/shinya7y/UniverseNet .",
        "translated": ""
    },
    {
        "title": "3D Skeletonization of Complex Grapevines for Robotic Pruning",
        "url": "http://arxiv.org/abs/2307.11706v1",
        "pub_date": "2023-07-21",
        "summary": "Robotic pruning of dormant grapevines is an area of active research in order\nto promote vine balance and grape quality, but so far robotic efforts have\nlargely focused on planar, simplified vines not representative of commercial\nvineyards. This paper aims to advance the robotic perception capabilities\nnecessary for pruning in denser and more complex vine structures by extending\nplant skeletonization techniques. The proposed pipeline generates skeletal\ngrapevine models that have lower reprojection error and higher connectivity\nthan baseline algorithms. We also show how 3D and skeletal information enables\nprediction accuracy of pruning weight for dense vines surpassing prior work,\nwhere pruning weight is an important vine metric influencing pruning site\nselection.",
        "translated": ""
    },
    {
        "title": "SACReg: Scene-Agnostic Coordinate Regression for Visual Localization",
        "url": "http://arxiv.org/abs/2307.11702v1",
        "pub_date": "2023-07-21",
        "summary": "Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every\npixel of a given image, has recently shown promising potential. However,\nexisting methods remain mostly scene-specific or limited to small scenes and\nthus hardly scale to realistic datasets. In this paper, we propose a new\nparadigm where a single generic SCR model is trained once to be then deployed\nto new test scenes, regardless of their scale and without further finetuning.\nFor a given query image, it collects inputs from off-the-shelf image retrieval\ntechniques and Structure-from-Motion databases: a list of relevant database\nimages with sparse pointwise 2D-3D annotations. The model is based on the\ntransformer architecture and can take a variable number of images and sparse\n2D-3D annotations as input. It is trained on a few diverse datasets and\nsignificantly outperforms other scene regression approaches on several\nbenchmarks, including scene-specific models, for visual localization. In\nparticular, we set a new state of the art on the Cambridge localization\nbenchmark, even outperforming feature-matching-based approaches.",
        "translated": ""
    },
    {
        "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
        "url": "http://arxiv.org/abs/2307.11661v1",
        "pub_date": "2023-07-21",
        "summary": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have\nrevolutionized visual representation learning by providing good performance on\ndownstream datasets. VLMs are 0-shot adapted to a downstream dataset by\ndesigning prompts that are relevant to the dataset. Such prompt engineering\nmakes use of domain expertise and a validation dataset. Meanwhile, recent\ndevelopments in generative pretrained models like GPT-4 mean they can be used\nas advanced internet search tools. They can also be manipulated to provide\nvisual information in any structure. In this work, we show that GPT-4 can be\nused to generate text that is visually descriptive and how this can be used to\nadapt CLIP to downstream tasks. We show considerable improvements in 0-shot\ntransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD\n(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.\nWe also design a simple few-shot adapter that learns to choose the best\npossible sentences to construct generalizable classifiers that outperform the\nrecently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized\nfine-grained datasets. We will release the code, prompts, and auxiliary text\ndataset upon acceptance.",
        "translated": ""
    },
    {
        "title": "FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation\n  and Malignancy Classification",
        "url": "http://arxiv.org/abs/2307.11654v1",
        "pub_date": "2023-07-21",
        "summary": "Skin diseases affect millions of people worldwide, across all ethnicities.\nIncreasing diagnosis accessibility requires fair and accurate segmentation and\nclassification of dermatology images. However, the scarcity of annotated\nmedical images, especially for rare diseases and underrepresented skin tones,\nposes a challenge to the development of fair and accurate models. In this\nstudy, we introduce a Fair, Efficient, and Diverse Diffusion-based framework\nfor skin lesion segmentation and malignancy classification. FEDD leverages\nsemantically meaningful feature embeddings learned through a denoising\ndiffusion probabilistic backbone and processes them via linear probes to\nachieve state-of-the-art performance on Diverse Dermatology Images (DDI). We\nachieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07\nwhile using only 5%, 10%, 15%, and 20% labeled samples, respectively.\nAdditionally, FEDD trained on 10% of DDI demonstrates malignancy classification\naccuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high\nefficiency in data-constrained scenarios while providing fair performance for\ndiverse skin tones and rare malignancy conditions. Our newly annotated DDI\nsegmentation masks and training code can be found on\nhttps://github.com/hectorcarrion/fedd.",
        "translated": ""
    },
    {
        "title": "3D-LLM: Injecting the 3D World into Large Language Models",
        "url": "http://arxiv.org/abs/2307.12981v1",
        "pub_date": "2023-07-24",
        "summary": "Large language models (LLMs) and Vision-Language Models (VLMs) have been\nproven to excel at multiple tasks, such as commonsense reasoning. Powerful as\nthese models can be, they are not grounded in the 3D physical world, which\ninvolves richer concepts such as spatial relationships, affordances, physics,\nlayout, and so on. In this work, we propose to inject the 3D world into large\nlanguage models and introduce a whole new family of 3D-LLMs. Specifically,\n3D-LLMs can take 3D point clouds and their features as input and perform a\ndiverse set of 3D-related tasks, including captioning, dense captioning, 3D\nquestion answering, task decomposition, 3D grounding, 3D-assisted dialog,\nnavigation, and so on. Using three types of prompting mechanisms that we\ndesign, we are able to collect over 300k 3D-language data covering these tasks.\nTo efficiently train 3D-LLMs, we first utilize a 3D feature extractor that\nobtains 3D features from rendered multi- view images. Then, we use 2D VLMs as\nour backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,\n3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show\nthat our model outperforms state-of-the-art baselines by a large margin (e.g.,\nthe BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore,\nexperiments on our held-in datasets for 3D captioning, task composition, and\n3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative\nexamples also show that our model could perform more tasks beyond the scope of\nexisting LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.",
        "translated": ""
    },
    {
        "title": "A Systematic Survey of Prompt Engineering on Vision-Language Foundation\n  Models",
        "url": "http://arxiv.org/abs/2307.12980v1",
        "pub_date": "2023-07-24",
        "summary": "Prompt engineering is a technique that involves augmenting a large\npre-trained model with task-specific hints, known as prompts, to adapt the\nmodel to new tasks. Prompts can be created manually as natural language\ninstructions or generated automatically as either natural language instructions\nor vector representations. Prompt engineering enables the ability to perform\npredictions based solely on prompts without updating model parameters, and the\neasier application of large pre-trained models in real-world tasks. In past\nyears, Prompt engineering has been well-studied in natural language processing.\nRecently, it has also been intensively studied in vision-language modeling.\nHowever, there is currently a lack of a systematic overview of prompt\nengineering on pre-trained vision-language models. This paper aims to provide a\ncomprehensive survey of cutting-edge research in prompt engineering on three\ntypes of vision-language models: multimodal-to-text generation models (e.g.\nFlamingo), image-text matching models (e.g. CLIP), and text-to-image generation\nmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,\nprompting methods, prompting-based applications, and the corresponding\nresponsibility and integrity issues are summarized and discussed. Furthermore,\nthe commonalities and differences between prompting on vision-language models,\nlanguage models, and vision models are also discussed. The challenges, future\ndirections, and research opportunities are summarized to foster future research\non this topic.",
        "translated": ""
    },
    {
        "title": "DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting",
        "url": "http://arxiv.org/abs/2307.12972v1",
        "pub_date": "2023-07-24",
        "summary": "In this paper, we propose a new operator, called 3D DeFormable Attention\n(DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D image\nfeatures into a unified 3D space for 3D object detection. Existing feature\nlifting approaches, such as Lift-Splat-based and 2D attention-based, either use\nestimated depth to get pseudo LiDAR features and then splat them to a 3D space,\nwhich is a one-pass operation without feature refinement, or ignore depth and\nlift features by 2D attention mechanisms, which achieve finer semantics while\nsuffering from a depth ambiguity problem. In contrast, our DFA3D-based method\nfirst leverages the estimated depth to expand each view's 2D feature map to 3D\nand then utilizes DFA3D to aggregate features from the expanded 3D feature\nmaps. With the help of DFA3D, the depth ambiguity problem can be effectively\nalleviated from the root, and the lifted features can be progressively refined\nlayer by layer, thanks to the Transformer-like architecture. In addition, we\npropose a mathematically equivalent implementation of DFA3D which can\nsignificantly improve its memory efficiency and computational speed. We\nintegrate DFA3D into several methods that use 2D attention-based feature\nlifting with only a few modifications in code and evaluate on the nuScenes\ndataset. The experiment results show a consistent improvement of +1.41\\% mAP on\naverage, and up to +15.1\\% mAP improvement when high-quality depth information\nis available, demonstrating the superiority, applicability, and huge potential\nof DFA3D. The code is available at\nhttps://github.com/IDEA-Research/3D-deformable-attention.git.",
        "translated": ""
    },
    {
        "title": "Volcanic ash delimitation using Artificial Intelligence based on Pix2Pix",
        "url": "http://arxiv.org/abs/2307.12970v1",
        "pub_date": "2023-07-24",
        "summary": "Volcanic eruptions emit ash that can be harmful to human health and cause\ndamage to infrastructure, economic activities and the environment. The\ndelimitation of ash clouds allows to know their behavior and dispersion, which\nhelps in the prevention and mitigation of this phenomenon. Traditional methods\ntake advantage of specialized software programs to process the bands or\nchannels that compose the satellite images. However, their use is limited to\nexperts and demands a lot of time and significant computational resources. In\nrecent years, Artificial Intelligence has been a milestone in the computational\ntreatment of complex problems in different areas. In particular, Deep Learning\ntechniques allow automatic, fast and accurate processing of digital images. The\npresent work proposes the use of the Pix2Pix model, a type of generative\nadversarial network that, once trained, learns the mapping of input images to\noutput images. The architecture of such a network consisting of a generator and\na discriminator provides the versatility needed to produce black and white ash\ncloud images from multispectral satellite images. The evaluation of the model,\nbased on loss and accuracy plots, a confusion matrix, and visual inspection,\nindicates a satisfactory solution for accurate ash cloud delineation,\napplicable in any area of the world and becomes a useful tool in risk\nmanagement.",
        "translated": ""
    },
    {
        "title": "Learning Dense Correspondences between Photos and Sketches",
        "url": "http://arxiv.org/abs/2307.12967v1",
        "pub_date": "2023-07-24",
        "summary": "Humans effortlessly grasp the connection between sketches and real-world\nobjects, even when these sketches are far from realistic. Moreover, human\nsketch understanding goes beyond categorization -- critically, it also entails\nunderstanding how individual elements within a sketch correspond to parts of\nthe physical world it represents. What are the computational ingredients needed\nto support this ability? Towards answering this question, we make two\ncontributions: first, we introduce a new sketch-photo correspondence benchmark,\n$\\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across\n125 object categories, augmenting the existing Sketchy dataset with\nfine-grained correspondence metadata. Second, we propose a self-supervised\nmethod for learning dense correspondences between sketch-photo pairs, building\nupon recent advances in correspondence learning for pairs of photos. Our model\nuses a spatial transformer network to estimate the warp flow between latent\nrepresentations of a sketch and photo extracted by a contrastive learning-based\nConvNet backbone. We found that this approach outperformed several strong\nbaselines and produced predictions that were quantitatively consistent with\nother warp-based methods. However, our benchmark also revealed systematic\ndifferences between predictions of the suite of models we tested and those of\nhumans. Taken together, our work suggests a promising path towards developing\nartificial systems that achieve more human-like understanding of visual images\nat different levels of abstraction. Project page:\nhttps://photo-sketch-correspondence.github.io",
        "translated": ""
    },
    {
        "title": "Benchmarking and Analyzing Generative Data for Visual Recognition",
        "url": "http://arxiv.org/abs/2307.13697v1",
        "pub_date": "2023-07-25",
        "summary": "Advancements in large pre-trained generative models have expanded their\npotential as effective data generators in visual recognition. This work delves\ninto the impact of generative images, primarily comparing paradigms that\nharness external data (\\ie generative \\vs retrieval \\vs original).\n  Our key contributions are: \\textbf{1) GenBench Construction:} We devise\n\\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548\ncategories, to appraise generative data across various visual recognition\ntasks. \\textbf{2) CLER Score:} To address the insufficient correlation of\nexisting metrics (\\eg, FID, CLIP score) with downstream recognition\nperformance, we propose \\textbf{CLER}, a training-free metric indicating\ngenerative data's efficiency for recognition tasks prior to training.\n\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data\nfrom the same external pool help to elucidate the unique traits of generative\ndata. \\textbf{4) External Knowledge Injection:} By fine-tuning special token\nembeddings for each category via Textual Inversion, performance improves across\n17 datasets, except when dealing with low-resolution reference images.\n  Our exhaustive benchmark and analysis spotlight generative data's promise in\nvisual recognition, while identifying key challenges for future investigation.",
        "translated": ""
    },
    {
        "title": "The Visual Language of Fabrics",
        "url": "http://arxiv.org/abs/2307.13681v1",
        "pub_date": "2023-07-25",
        "summary": "We introduce text2fabric, a novel dataset that links free-text descriptions\nto various fabric materials. The dataset comprises 15,000 natural language\ndescriptions associated to 3,000 corresponding images of fabric materials.\nTraditionally, material descriptions come in the form of tags/keywords, which\nlimits their expressivity, induces pre-existing knowledge of the appropriate\nvocabulary, and ultimately leads to a chopped description system. Therefore, we\nstudy the use of free-text as a more appropriate way to describe material\nappearance, taking the use case of fabrics as a common item that non-experts\nmay often deal with. Based on the analysis of the dataset, we identify a\ncompact lexicon, set of attributes and key structure that emerge from the\ndescriptions. This allows us to accurately understand how people describe\nfabrics and draw directions for generalization to other types of materials. We\nalso show that our dataset enables specializing large vision-language models\nsuch as CLIP, creating a meaningful latent space for fabric appearance, and\nsignificantly improving applications such as fine-grained material retrieval\nand automatic captioning.",
        "translated": ""
    },
    {
        "title": "Personal Protective Equipment Detection in Extreme Construction\n  Conditions",
        "url": "http://arxiv.org/abs/2307.13654v1",
        "pub_date": "2023-07-25",
        "summary": "Object detection has been widely applied for construction safety management,\nespecially personal protective equipment (PPE) detection. Though the existing\nPPE detection models trained on conventional datasets have achieved excellent\nresults, their performance dramatically declines in extreme construction\nconditions. A robust detection model NST-YOLOv5 is developed by combining the\nneural style transfer (NST) and YOLOv5 technologies. Five extreme conditions\nare considered and simulated via the NST module to endow the detection model\nwith excellent robustness, including low light, intense light, sand dust, fog,\nand rain. Experiments show that the NST has great potential as a tool for\nextreme data synthesis since it is better at simulating extreme conditions than\nother traditional image processing algorithms and helps the NST-YOLOv5 achieve\n0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme\ndata. This study provides a new feasible way to obtain a more robust detection\nmodel for extreme construction conditions.",
        "translated": ""
    },
    {
        "title": "QuickQual: Lightweight, convenient retinal image quality scoring with\n  off-the-shelf pretrained models",
        "url": "http://arxiv.org/abs/2307.13646v1",
        "pub_date": "2023-07-25",
        "summary": "Image quality remains a key problem for both traditional and deep learning\n(DL)-based approaches to retinal image analysis, but identifying poor quality\nimages can be time consuming and subjective. Thus, automated methods for\nretinal image quality scoring (RIQS) are needed. The current state-of-the-art\nis MCFNet, composed of three Densenet121 backbones each operating in a\ndifferent colour space. MCFNet, and the EyeQ dataset released by the same\nauthors, was a huge step forward for RIQS. We present QuickQual, a simple\napproach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained\nDensenet121 backbone plus a Support Vector Machine (SVM). QuickQual performs\nvery well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00%\nfor MCFNet; AUC: 0.9687 vs 0.9588). This suggests that RIQS can be solved with\ngeneric perceptual features learned on natural images, as opposed to requiring\nDL models trained on large amounts of fundus images. Additionally, we propose a\nFixed Prior linearisation scheme, that converts EyeQ from a 3-way\nclassification to a continuous logistic regression task. For this task, we\npresent a second model, QuickQual MEga Minified Estimator (QuickQual-MEME),\nthat consists of only 10 parameters on top of an off-the-shelf Densenet121 and\ncan distinguish between gradable and ungradable images with an accuracy of\n89.18% (AUC: 0.9537). Code and model are available on GitHub:\nhttps://github.com/justinengelmann/QuickQual . QuickQual is so lightweight,\nthat the entire inference code (and even the parameters for QuickQual-MEME) is\nalready contained in this paper.",
        "translated": ""
    },
    {
        "title": "Learning Transferable Object-Centric Diffeomorphic Transformations for\n  Data Augmentation in Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2307.13645v1",
        "pub_date": "2023-07-25",
        "summary": "Obtaining labelled data in medical image segmentation is challenging due to\nthe need for pixel-level annotations by experts. Recent works have shown that\naugmenting the object of interest with deformable transformations can help\nmitigate this challenge. However, these transformations have been learned\nglobally for the image, limiting their transferability across datasets or\napplicability in problems where image alignment is difficult. While\nobject-centric augmentations provide a great opportunity to overcome these\nissues, existing works are only focused on position and random transformations\nwithout considering shape variations of the objects. To this end, we propose a\nnovel object-centric data augmentation model that is able to learn the shape\nvariations for the objects of interest and augment the object in place without\nmodifying the rest of the image. We demonstrated its effectiveness in improving\nkidney tumour segmentation when leveraging shape variations learned both from\nwithin the same dataset and transferred from external datasets.",
        "translated": ""
    },
    {
        "title": "Virtual Mirrors: Non-Line-of-Sight Imaging Beyond the Third Bounce",
        "url": "http://arxiv.org/abs/2307.14341v1",
        "pub_date": "2023-07-26",
        "summary": "Non-line-of-sight (NLOS) imaging methods are capable of reconstructing\ncomplex scenes that are not visible to an observer using indirect illumination.\nHowever, they assume only third-bounce illumination, so they are currently\nlimited to single-corner configurations, and present limited visibility when\nimaging surfaces at certain orientations. To reason about and tackle these\nlimitations, we make the key observation that planar diffuse surfaces behave\nspecularly at wavelengths used in the computational wave-based NLOS imaging\ndomain. We call such surfaces virtual mirrors. We leverage this observation to\nexpand the capabilities of NLOS imaging using illumination beyond the third\nbounce, addressing two problems: imaging single-corner objects at limited\nvisibility angles, and imaging objects hidden behind two corners. To image\nobjects at limited visibility angles, we first analyze the reflections of the\nknown illuminated point on surfaces of the scene as an estimator of the\nposition and orientation of objects with limited visibility. We then image\nthose limited visibility objects by computationally building secondary\napertures at other surfaces that observe the target object from a direct\nvisibility perspective. Beyond single-corner NLOS imaging, we exploit the\nspecular behavior of virtual mirrors to image objects hidden behind a second\ncorner by imaging the space behind such virtual mirrors, where the mirror image\nof objects hidden around two corners is formed. No specular surfaces were\ninvolved in the making of this paper.",
        "translated": ""
    },
    {
        "title": "MAMo: Leveraging Memory and Attention for Monocular Video Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2307.14336v1",
        "pub_date": "2023-07-26",
        "summary": "We propose MAMo, a novel memory and attention frame-work for monocular video\ndepth estimation. MAMo can augment and improve any single-image depth\nestimation networks into video depth estimation models, enabling them to take\nadvantage of the temporal information to predict more accurate depth. In MAMo,\nwe augment model with memory which aids the depth prediction as the model\nstreams through the video. Specifically, the memory stores learned visual and\ndisplacement tokens of the previous time instances. This allows the depth\nnetwork to cross-reference relevant features from the past when predicting\ndepth on the current frame. We introduce a novel scheme to continuously update\nthe memory, optimizing it to keep tokens that correspond with both the past and\nthe present visual information. We adopt attention-based approach to process\nmemory features where we first learn the spatio-temporal relation among the\nresultant visual and displacement memory tokens using self-attention module.\nFurther, the output features of self-attention are aggregated with the current\nvisual features through cross-attention. The cross-attended features are\nfinally given to a decoder to predict depth on the current frame. Through\nextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and\nDDAD, we show that MAMo consistently improves monocular depth estimation\nnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video\ndepth estimation provides higher accuracy with lower latency, when omparing to\nSOTA cost-volume-based video depth models.",
        "translated": ""
    },
    {
        "title": "Towards Generalist Biomedical AI",
        "url": "http://arxiv.org/abs/2307.14334v1",
        "pub_date": "2023-07-26",
        "summary": "Medicine is inherently multimodal, with rich data modalities spanning text,\nimaging, genomics, and more. Generalist biomedical artificial intelligence (AI)\nsystems that flexibly encode, integrate, and interpret this data at scale can\npotentially enable impactful applications ranging from scientific discovery to\ncare delivery. To enable the development of these models, we first curate\nMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses\n14 diverse tasks such as medical question answering, mammography and\ndermatology image interpretation, radiology report generation and\nsummarization, and genomic variant calling. We then introduce Med-PaLM\nMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI\nsystem. Med-PaLM M is a large multimodal generative model that flexibly encodes\nand interprets biomedical data including clinical language, imaging, and\ngenomics with the same set of model weights. Med-PaLM M reaches performance\ncompetitive with or exceeding the state of the art on all MultiMedBench tasks,\noften surpassing specialist models by a wide margin. We also report examples of\nzero-shot generalization to novel medical concepts and tasks, positive transfer\nlearning across tasks, and emergent zero-shot medical reasoning. To further\nprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologist\nevaluation of model-generated (and human) chest X-ray reports and observe\nencouraging performance across model scales. In a side-by-side ranking on 246\nretrospective chest X-rays, clinicians express a pairwise preference for\nMed-PaLM M reports over those produced by radiologists in up to 40.50% of\ncases, suggesting potential clinical utility. While considerable work is needed\nto validate these models in real-world use cases, our results represent a\nmilestone towards the development of generalist biomedical AI systems.",
        "translated": ""
    },
    {
        "title": "Event-based Vision for Early Prediction of Manipulation Actions",
        "url": "http://arxiv.org/abs/2307.14332v1",
        "pub_date": "2023-07-26",
        "summary": "Neuromorphic visual sensors are artificial retinas that output sequences of\nasynchronous events when brightness changes occur in the scene. These sensors\noffer many advantages including very high temporal resolution, no motion blur\nand smart data compression ideal for real-time processing. In this study, we\nintroduce an event-based dataset on fine-grained manipulation actions and\nperform an experimental study on the use of transformers for action prediction\nwith events. There is enormous interest in the fields of cognitive robotics and\nhuman-robot interaction on understanding and predicting human actions as early\nas possible. Early prediction allows anticipating complex stages for planning,\nenabling effective and real-time interaction. Our Transformer network uses\nevents to predict manipulation actions as they occur, using online inference.\nThe model succeeds at predicting actions early on, building up confidence over\ntime and achieving state-of-the-art classification. Moreover, the\nattention-based transformer architecture allows us to study the role of the\nspatio-temporal patterns selected by the model. Our experiments show that the\nTransformer network captures action dynamic features outperforming video-based\napproaches and succeeding with scenarios where the differences between actions\nlie in very subtle cues. Finally, we release the new event dataset, which is\nthe first in the literature for manipulation action recognition. Code will be\navailable at https://github.com/DaniDeniz/EventVisionTransformer.",
        "translated": ""
    },
    {
        "title": "Visual Instruction Inversion: Image Editing via Visual Prompting",
        "url": "http://arxiv.org/abs/2307.14331v1",
        "pub_date": "2023-07-26",
        "summary": "Text-conditioned image editing has emerged as a powerful tool for editing\nimages. However, in many situations, language can be ambiguous and ineffective\nin describing specific image edits. When faced with such challenges, visual\nprompts can be a more informative and intuitive way to convey ideas. We present\na method for image editing via visual prompting. Given pairs of example that\nrepresent the \"before\" and \"after\" images of an edit, our goal is to learn a\ntext-based editing direction that can be used to perform the same edit on new\nimages. We leverage the rich, pretrained editing capabilities of text-to-image\ndiffusion models by inverting visual prompts into editing instructions. Our\nresults show that with just one example pair, we can achieve competitive\nresults compared to state-of-the-art text-conditioned image editing frameworks.",
        "translated": ""
    },
    {
        "title": "To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.15063v1",
        "pub_date": "2023-07-27",
        "summary": "The goal of Online Domain Adaptation for semantic segmentation is to handle\nunforeseeable domain changes that occur during deployment, like sudden weather\nevents. However, the high computational costs associated with brute-force\nadaptation make this paradigm unfeasible for real-world applications. In this\npaper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training\nframework for real-time domain adaptation. Our approach includes a\nhardware-aware back-propagation orchestration agent (HAMT) and a dedicated\ndomain-shift detector that enables active control over when and how the model\nis adapted (LT). Thanks to these advancements, our approach is capable of\nperforming semantic segmentation while simultaneously adapting at more than\n29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and\nspeed trade-off is demonstrated on OnDA and SHIFT benchmarks through\nexperimental results.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Visual Acoustic Matching",
        "url": "http://arxiv.org/abs/2307.15064v1",
        "pub_date": "2023-07-27",
        "summary": "Acoustic matching aims to re-synthesize an audio clip to sound as if it were\nrecorded in a target acoustic environment. Existing methods assume access to\npaired training data, where the audio is observed in both source and target\nenvironments, but this limits the diversity of training data or requires the\nuse of simulated data or heuristics to create paired samples. We propose a\nself-supervised approach to visual acoustic matching where training samples\ninclude only the target scene image and audio -- without acoustically\nmismatched source audio for reference. Our approach jointly learns to\ndisentangle room acoustics and re-synthesize audio into the target environment,\nvia a conditional GAN framework and a novel metric that quantifies the level of\nresidual acoustic information in the de-biased audio. Training with either\nin-the-wild web data or simulated data, we demonstrate it outperforms the\nstate-of-the-art on multiple challenging datasets and a wide variety of\nreal-world audio and environments.",
        "translated": ""
    },
    {
        "title": "The RoboDepth Challenge: Methods and Advancements Towards Robust Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2307.15061v1",
        "pub_date": "2023-07-27",
        "summary": "Accurate depth estimation under out-of-distribution (OoD) scenarios, such as\nadverse weather conditions, sensor failure, and noise contamination, is\ndesirable for safety-critical applications. Existing depth estimation systems,\nhowever, suffer inevitably from real-world corruptions and perturbations and\nare struggled to provide reliable depth predictions under such cases. In this\npaper, we summarize the winning solutions from the RoboDepth Challenge -- an\nacademic competition designed to facilitate and advance robust OoD depth\nestimation. This challenge was developed based on the newly established KITTI-C\nand NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis\non robust self-supervised and robust fully-supervised depth estimation,\nrespectively. Out of more than two hundred participants, nine unique and\ntop-performing solutions have appeared, with novel designs ranging from the\nfollowing aspects: spatial- and frequency-domain augmentations, masked image\nmodeling, image restoration and super-resolution, adversarial training,\ndiffusion-based noise suppression, vision-language pre-training, learned model\nensembling, and hierarchical feature enhancement. Extensive experimental\nanalyses along with insightful observations are drawn to better understand the\nrationale behind each design. We hope this challenge could lay a solid\nfoundation for future research on robust and reliable depth estimation and\nbeyond. The datasets, competition toolkit, workshop recordings, and source code\nfrom the winning teams are publicly available on the challenge website.",
        "translated": ""
    },
    {
        "title": "MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2307.15058v1",
        "pub_date": "2023-07-27",
        "summary": "Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is\nwidely recognized that realistic sensor simulation will play a critical role in\nsolving remaining corner cases by simulating them. To this end, we propose an\nautonomous driving simulator based upon neural radiance fields (NeRFs).\nCompared with existing works, ours has three notable features: (1)\nInstance-aware. Our simulator models the foreground instances and background\nenvironments separately with independent networks so that the static (e.g.,\nsize and appearance) and dynamic (e.g., trajectory) properties of instances can\nbe controlled separately. (2) Modular. Our simulator allows flexible switching\nbetween different modern NeRF-related backbones, sampling strategies, input\nmodalities, etc. We expect this modular design to boost academic progress and\nindustrial deployment of NeRF-based autonomous driving simulation. (3)\nRealistic. Our simulator set new state-of-the-art photo-realism results given\nthe best module selection. Our simulator will be open-sourced while most of our\ncounterparts are not. Project page: https://open-air-sun.github.io/mars/.",
        "translated": ""
    },
    {
        "title": "PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point\n  Tracking",
        "url": "http://arxiv.org/abs/2307.15055v1",
        "pub_date": "2023-07-27",
        "summary": "We introduce PointOdyssey, a large-scale synthetic dataset, and data\ngeneration framework, for the training and evaluation of long-term fine-grained\ntracking algorithms. Our goal is to advance the state-of-the-art by placing\nemphasis on long videos with naturalistic motion. Toward the goal of\nnaturalism, we animate deformable characters using real-world motion capture\ndata, we build 3D scenes to match the motion capture environments, and we\nrender camera viewpoints using trajectories mined via structure-from-motion on\nreal videos. We create combinatorial diversity by randomizing character\nappearance, motion profiles, materials, lighting, 3D assets, and atmospheric\neffects. Our dataset currently includes 104 videos, averaging 2,000 frames\nlong, with orders of magnitude more correspondence annotations than prior work.\nWe show that existing methods can be trained from scratch in our dataset and\noutperform the published variants. Finally, we introduce modifications to the\nPIPs point tracking method, greatly widening its temporal receptive field,\nwhich improves its performance on PointOdyssey as well as on two real-world\nbenchmarks. Our data and code are publicly available at:\nhttps://pointodyssey.com",
        "translated": ""
    },
    {
        "title": "Semi-Supervised Object Detection in the Open World",
        "url": "http://arxiv.org/abs/2307.15710v1",
        "pub_date": "2023-07-28",
        "summary": "Existing approaches for semi-supervised object detection assume a fixed set\nof classes present in training and unlabeled datasets, i.e., in-distribution\n(ID) data. The performance of these techniques significantly degrades when\nthese techniques are deployed in the open-world, due to the fact that the\nunlabeled and test data may contain objects that were not seen during training,\ni.e., out-of-distribution (OOD) data. The two key questions that we explore in\nthis paper are: can we detect these OOD samples and if so, can we learn from\nthem? With these considerations in mind, we propose the Open World\nSemi-supervised Detection framework (OWSSD) that effectively detects OOD data\nalong with a semi-supervised learning pipeline that learns from both ID and OOD\ndata. We introduce an ensemble based OOD detector consisting of lightweight\nauto-encoder networks trained only on ID data. Through extensive evalulation,\nwe demonstrate that our method performs competitively against state-of-the-art\nOOD detection algorithms and also significantly boosts the semi-supervised\nlearning performance in open-world scenarios.",
        "translated": ""
    },
    {
        "title": "MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2307.15700v1",
        "pub_date": "2023-07-28",
        "summary": "As a video task, Multi-Object Tracking (MOT) is expected to capture temporal\ninformation of targets effectively. Unfortunately, most existing methods only\nexplicitly exploit the object features between adjacent frames, while lacking\nthe capacity to model long-term temporal information. In this paper, we propose\nMeMOTR, a long-term memory-augmented Transformer for multi-object tracking. Our\nmethod is able to make the same object's track embedding more stable and\ndistinguishable by leveraging long-term memory injection with a customized\nmemory-attention layer. This significantly improves the target association\nability of our model. Experimental results on DanceTrack show that MeMOTR\nimpressively surpasses the state-of-the-art method by 7.9\\% and 13.0\\% on HOTA\nand AssA metrics, respectively. Furthermore, our model also outperforms other\nTransformer-based methods on association performance on MOT17 and generalizes\nwell on BDD100K. Code is available at\n\\href{https://github.com/MCG-NJU/MeMOTR}{https://github.com/MCG-NJU/MeMOTR}.",
        "translated": ""
    },
    {
        "title": "SimDETR: Simplifying self-supervised pretraining for DETR",
        "url": "http://arxiv.org/abs/2307.15697v1",
        "pub_date": "2023-07-28",
        "summary": "DETR-based object detectors have achieved remarkable performance but are\nsample-inefficient and exhibit slow convergence. Unsupervised pretraining has\nbeen found to be helpful to alleviate these impediments, allowing training with\nlarge amounts of unlabeled data to improve the detector's performance. However,\nexisting methods have their own limitations, like keeping the detector's\nbackbone frozen in order to avoid performance degradation and utilizing\npretraining objectives misaligned with the downstream task. To overcome these\nlimitations, we propose a simple pretraining framework for DETR-based detectors\nthat consists of three simple yet key ingredients: (i) richer, semantics-based\ninitial proposals derived from high-level feature maps, (ii) discriminative\ntraining using object pseudo-labels produced via clustering, (iii)\nself-training to take advantage of the improved object proposals learned by the\ndetector. We report two main findings: (1) Our pretraining outperforms prior\nDETR pretraining works on both the full and low data regimes by significant\nmargins. (2) We show we can pretrain DETR from scratch (including the backbone)\ndirectly on complex image datasets like COCO, paving the path for unsupervised\nrepresentation learning directly using DETR.",
        "translated": ""
    },
    {
        "title": "PatchMixer: Rethinking network design to boost generalization for 3D\n  point cloud understanding",
        "url": "http://arxiv.org/abs/2307.15692v1",
        "pub_date": "2023-07-28",
        "summary": "The recent trend in deep learning methods for 3D point cloud understanding is\nto propose increasingly sophisticated architectures either to better capture 3D\ngeometries or by introducing possibly undesired inductive biases. Moreover,\nprior works introducing novel architectures compared their performance on the\nsame domain, devoting less attention to their generalization to other domains.\nWe argue that the ability of a model to transfer the learnt knowledge to\ndifferent domains is an important feature that should be evaluated to\nexhaustively assess the quality of a deep network architecture. In this work we\npropose PatchMixer, a simple yet effective architecture that extends the ideas\nbehind the recent MLP-Mixer paper to 3D point clouds. The novelties of our\napproach are the processing of local patches instead of the whole shape to\npromote robustness to partial point clouds, and the aggregation of patch-wise\nfeatures using an MLP as a simpler alternative to the graph convolutions or the\nattention mechanisms that are used in prior works. We evaluated our method on\nthe shape classification and part segmentation tasks, achieving superior\ngeneralization performance compared to a selection of the most relevant deep\narchitectures.",
        "translated": ""
    },
    {
        "title": "TrackAgent: 6D Object Tracking via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.15671v1",
        "pub_date": "2023-07-28",
        "summary": "Tracking an object's 6D pose, while either the object itself or the observing\ncamera is moving, is important for many robotics and augmented reality\napplications. While exploiting temporal priors eases this problem,\nobject-specific knowledge is required to recover when tracking is lost. Under\nthe tight time constraints of the tracking task, RGB(D)-based methods are often\nconceptionally complex or rely on heuristic motion models. In comparison, we\npropose to simplify object tracking to a reinforced point cloud (depth only)\nalignment task. This allows us to train a streamlined approach from scratch\nwith limited amounts of sparse 3D point clouds, compared to the large datasets\nof diverse RGBD sequences required in previous works. We incorporate temporal\nframe-to-frame registration with object-based recovery by frame-to-model\nrefinement using a reinforcement learning (RL) agent that jointly solves for\nboth objectives. We also show that the RL agent's uncertainty and a\nrendering-based mask propagation are effective reinitialization triggers.",
        "translated": ""
    },
    {
        "title": "DiVA-360: The Dynamic Visuo-Audio Dataset for Immersive Neural Fields",
        "url": "http://arxiv.org/abs/2307.16897v1",
        "pub_date": "2023-07-31",
        "summary": "Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of static and dynamic scenes. However, their capabilities lag behind\nthose offered by representations such as pixels or meshes due to algorithmic\nchallenges and the lack of large-scale real-world datasets. We address the\ndataset limitation with DiVA-360, a real-world 360 dynamic visual-audio dataset\nwith synchronized multimodal visual, audio, and textual information about\ntable-scale scenes. It contains 46 dynamic scenes, 30 static scenes, and 95\nstatic objects spanning 11 categories captured using a new hardware system\nusing 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M image\nframes and 1360 s of dynamic data. We provide detailed text descriptions for\nall scenes, foreground-background segmentation masks, category-specific 3D pose\nalignment for static objects, as well as metrics for comparison. Our data,\nhardware and software, and code are available at https://diva360.github.io/.",
        "translated": ""
    },
    {
        "title": "Disruptive Autoencoders: Leveraging Low-level features for 3D Medical\n  Image Pre-training",
        "url": "http://arxiv.org/abs/2307.16896v1",
        "pub_date": "2023-07-31",
        "summary": "Harnessing the power of pre-training on large-scale datasets like ImageNet\nforms a fundamental building block for the progress of representation\nlearning-driven solutions in computer vision. Medical images are inherently\ndifferent from natural images as they are acquired in the form of many\nmodalities (CT, MR, PET, Ultrasound etc.) and contain granulated information\nlike tissue, lesion, organs etc. These characteristics of medical images\nrequire special attention towards learning features representative of local\ncontext. In this work, we focus on designing an effective pre-training\nframework for 3D radiology images. First, we propose a new masking strategy\ncalled local masking where the masking is performed across channel embeddings\ninstead of tokens to improve the learning of local feature representations. We\ncombine this with classical low-level perturbations like adding noise and\ndownsampling to further enable low-level representation learning. To this end,\nwe introduce Disruptive Autoencoders, a pre-training framework that attempts to\nreconstruct the original image from disruptions created by a combination of\nlocal masking and low-level perturbations. Additionally, we also devise a\ncross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple\nmodalities in a single framework. We curate a large-scale dataset to enable\npre-training of 3D medical radiology images (MRI and CT). The proposed\npre-training framework is tested across multiple downstream tasks and achieves\nstate-of-the-art performance. Notably, our proposed method tops the public test\nleaderboard of BTCV multi-organ segmentation challenge.",
        "translated": ""
    },
    {
        "title": "Image Synthesis under Limited Data: A Survey and Taxonomy",
        "url": "http://arxiv.org/abs/2307.16879v1",
        "pub_date": "2023-07-31",
        "summary": "Deep generative models, which target reproducing the given data distribution\nto produce novel samples, have made unprecedented advancements in recent years.\nTheir technical breakthroughs have enabled unparalleled quality in the\nsynthesis of visual content. However, one critical prerequisite for their\ntremendous success is the availability of a sufficient number of training\nsamples, which requires massive computation resources. When trained on limited\ndata, generative models tend to suffer from severe performance deterioration\ndue to overfitting and memorization. Accordingly, researchers have devoted\nconsiderable attention to develop novel models that are capable of generating\nplausible and diverse images from limited training data recently. Despite\nnumerous efforts to enhance training stability and synthesis quality in the\nlimited data scenarios, there is a lack of a systematic survey that provides 1)\na clear problem definition, critical challenges, and taxonomy of various tasks;\n2) an in-depth analysis on the pros, cons, and remain limitations of existing\nliterature; as well as 3) a thorough discussion on the potential applications\nand future directions in the field of image synthesis under limited data. In\norder to fill this gap and provide a informative introduction to researchers\nwho are new to this topic, this survey offers a comprehensive review and a\nnovel taxonomy on the development of image synthesis under limited data. In\nparticular, it covers the problem definition, requirements, main solutions,\npopular benchmarks, and remain challenges in a comprehensive and all-around\nmanner.",
        "translated": ""
    },
    {
        "title": "Revisiting the Parameter Efficiency of Adapters from the Perspective of\n  Precision Redundancy",
        "url": "http://arxiv.org/abs/2307.16867v1",
        "pub_date": "2023-07-31",
        "summary": "Current state-of-the-art results in computer vision depend in part on\nfine-tuning large pre-trained vision models. However, with the exponential\ngrowth of model sizes, the conventional full fine-tuning, which needs to store\na individual network copy for each tasks, leads to increasingly huge storage\nand transmission overhead. Adapter-based Parameter-Efficient Tuning (PET)\nmethods address this challenge by tuning lightweight adapters inserted into the\nfrozen pre-trained models. In this paper, we investigate how to make adapters\neven more efficient, reaching a new minimum size required to store a\ntask-specific fine-tuned network. Inspired by the observation that the\nparameters of adapters converge at flat local minima, we find that adapters are\nresistant to noise in parameter space, which means they are also resistant to\nlow numerical precision. To train low-precision adapters, we propose a\ncomputational-efficient quantization method which minimizes the quantization\nerror. Through extensive experiments, we find that low-precision adapters\nexhibit minimal performance degradation, and even 1-bit precision is sufficient\nfor adapters. The experimental results demonstrate that 1-bit adapters\noutperform all other PET methods on both the VTAB-1K benchmark and few-shot\nFGVC tasks, while requiring the smallest storage size. Our findings show, for\nthe first time, the significant potential of quantization techniques in PET,\nproviding a general solution to enhance the parameter efficiency of\nadapter-based PET methods. Code: https://github.com/JieShibo/PETL-ViT",
        "translated": ""
    },
    {
        "title": "Universal Adversarial Defense in Remote Sensing Based on Pre-trained\n  Denoising Diffusion Models",
        "url": "http://arxiv.org/abs/2307.16865v1",
        "pub_date": "2023-07-31",
        "summary": "Deep neural networks (DNNs) have achieved tremendous success in many remote\nsensing (RS) applications. However, their vulnerability to the threat of\nadversarial perturbations should not be neglected. Unfortunately, current\nadversarial defense approaches in RS studies usually suffer from performance\nfluctuation and unnecessary re-training costs due to the need for prior\nknowledge of the adversarial perturbations among RS data. To circumvent these\nchallenges, we propose a universal adversarial defense approach in RS imagery\n(UAD-RS) using pre-trained diffusion models to defend the common DNNs against\nmultiple unknown adversarial attacks. Specifically, the generative diffusion\nmodels are first pre-trained on different RS datasets to learn generalized\nrepresentations in various data domains. After that, a universal adversarial\npurification framework is developed using the forward and reverse process of\nthe pre-trained diffusion models to purify the perturbations from adversarial\nsamples. Furthermore, an adaptive noise level selection (ANLS) mechanism is\nbuilt to capture the optimal noise level of the diffusion model that can\nachieve the best purification results closest to the clean samples according to\ntheir Frechet Inception Distance (FID) in deep feature space. As a result, only\na single pre-trained diffusion model is needed for the universal purification\nof adversarial samples on each dataset, which significantly alleviates the\nre-training efforts for each attack setting and maintains high performance\nwithout the prior knowledge of adversarial perturbations. Experiments on four\nheterogeneous RS datasets regarding scene classification and semantic\nsegmentation verify that UAD-RS outperforms state-of-the-art adversarial\npurification approaches with a universal defense against seven commonly\nexisting adversarial perturbations.",
        "translated": ""
    },
    {
        "title": "LISA: Reasoning Segmentation via Large Language Model",
        "url": "http://arxiv.org/abs/2308.00692v1",
        "pub_date": "2023-08-01",
        "summary": "Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction to identify the target\nobjects or categories before executing visual recognition tasks. Such systems\nlack the ability to actively reason and comprehend implicit user intentions. In\nthis work, we propose a new segmentation task -- reasoning segmentation. The\ntask is designed to output a segmentation mask given a complex and implicit\nquery text. Furthermore, we establish a benchmark comprising over one thousand\nimage-instruction pairs, incorporating intricate reasoning and world knowledge\nfor evaluation purposes. Finally, we present LISA: large Language Instructed\nSegmentation Assistant, which inherits the language generation capabilities of\nthe multi-modal Large Language Model (LLM) while also possessing the ability to\nproduce segmentation masks. We expand the original vocabulary with a &lt;SEG&gt;\ntoken and propose the embedding-as-mask paradigm to unlock the segmentation\ncapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;\n2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,\nit demonstrates robust zero-shot capability when trained exclusively on\nreasoning-free datasets. In addition, fine-tuning the model with merely 239\nreasoning segmentation image-instruction pairs results in further performance\nenhancement. Experiments show our method not only unlocks new reasoning\nsegmentation capabilities but also proves effective in both complex reasoning\nsegmentation and standard referring segmentation tasks. Code, models, and demo\nare at https://github.com/dvlab-research/LISA.",
        "translated": ""
    },
    {
        "title": "AnyLoc: Towards Universal Visual Place Recognition",
        "url": "http://arxiv.org/abs/2308.00688v1",
        "pub_date": "2023-08-01",
        "summary": "Visual Place Recognition (VPR) is vital for robot localization. To date, the\nmost performant VPR approaches are environment- and task-specific: while they\nexhibit strong performance in structured environments (predominantly urban\ndriving), their performance degrades severely in unstructured environments,\nrendering most approaches brittle to robust real-world deployment. In this\nwork, we develop a universal solution to VPR -- a technique that works across a\nbroad range of structured and unstructured environments (urban, outdoors,\nindoors, aerial, underwater, and subterranean environments) without any\nre-training or fine-tuning. We demonstrate that general-purpose feature\nrepresentations derived from off-the-shelf self-supervised models with no\nVPR-specific training are the right substrate upon which to build such a\nuniversal VPR solution. Combining these derived features with unsupervised\nfeature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X\nsignificantly higher performance than existing approaches. We further obtain a\n6% improvement in performance by characterizing the semantic properties of\nthese features, uncovering unique domains which encapsulate datasets from\nsimilar environments. Our detailed experiments and analysis lay a foundation\nfor building VPR solutions that may be deployed anywhere, anytime, and across\nanyview. We encourage the readers to explore our project page and interactive\ndemos: https://anyloc.github.io/.",
        "translated": ""
    },
    {
        "title": "Applicability of scaling laws to vision encoding models",
        "url": "http://arxiv.org/abs/2308.00678v1",
        "pub_date": "2023-08-01",
        "summary": "In this paper, we investigated how to build a high-performance vision\nencoding model to predict brain activity as part of our participation in the\nAlgonauts Project 2023 Challenge. The challenge provided brain activity\nrecorded by functional MRI (fMRI) while participants viewed images. Several\nvision models with parameter sizes ranging from 86M to 4.3B were used to build\npredictive models. To build highly accurate models, we focused our analysis on\ntwo main aspects: (1) How does the sample size of the fMRI training set change\nthe prediction accuracy? (2) How does the prediction accuracy across the visual\ncortex vary with the parameter size of the vision models? The results show that\nas the sample size used during training increases, the prediction accuracy\nimproves according to the scaling law. Similarly, we found that as the\nparameter size of the vision models increases, the prediction accuracy improves\naccording to the scaling law. These results suggest that increasing the sample\nsize of the fMRI training set and the parameter size of visual models may\ncontribute to more accurate visual models of the brain and lead to a better\nunderstanding of visual neuroscience.",
        "translated": ""
    },
    {
        "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.00675v1",
        "pub_date": "2023-08-01",
        "summary": "Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.",
        "translated": ""
    },
    {
        "title": "Toward Zero-shot Character Recognition: A Gold Standard Dataset with\n  Radical-level Annotations",
        "url": "http://arxiv.org/abs/2308.00655v1",
        "pub_date": "2023-08-01",
        "summary": "Optical character recognition (OCR) methods have been applied to diverse\ntasks, e.g., street view text recognition and document analysis. Recently,\nzero-shot OCR has piqued the interest of the research community because it\nconsiders a practical OCR scenario with unbalanced data distribution. However,\nthere is a lack of benchmarks for evaluating such zero-shot methods that apply\na divide-and-conquer recognition strategy by decomposing characters into\nradicals. Meanwhile, radical recognition, as another important OCR task, also\nlacks radical-level annotation for model training. In this paper, we construct\nan ancient Chinese character image dataset that contains both radical-level and\ncharacter-level annotations to satisfy the requirements of the above-mentioned\nmethods, namely, ACCID, where radical-level annotations include radical\ncategories, radical locations, and structural relations. To increase the\nadaptability of ACCID, we propose a splicing-based synthetic character\nalgorithm to augment the training samples and apply an image denoising method\nto improve the image quality. By introducing character decomposition and\nrecombination, we propose a baseline method for zero-shot OCR. The experimental\nresults demonstrate the validity of ACCID and the baseline model quantitatively\nand qualitatively.",
        "translated": ""
    },
    {
        "title": "ELIXR: Towards a general purpose X-ray artificial intelligence system\n  through alignment of large language models and radiology vision encoders",
        "url": "http://arxiv.org/abs/2308.01317v1",
        "pub_date": "2023-08-02",
        "summary": "Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or\nELIXR, leverages a language-aligned image encoder combined or grafted onto a\nfixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight\nadapter architecture using images paired with corresponding free-text radiology\nreports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance\non zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13\nfindings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898\nacross five findings (atelectasis, cardiomegaly, consolidation, pleural\neffusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images)\ntraining data), and semantic search (0.76 normalized discounted cumulative gain\n(NDCG) across nineteen queries, including perfect retrieval on twelve of them).\nCompared to existing data-efficient methods including supervised contrastive\nlearning (SupCon), ELIXR required two orders of magnitude less data to reach\nsimilar performance. ELIXR also showed promise on CXR vision-language tasks,\ndemonstrating overall accuracies of 58.7% and 62.5% on visual question\nanswering and report quality assurance tasks, respectively. These results\nsuggest that ELIXR is a robust and versatile approach to CXR AI.",
        "translated": ""
    },
    {
        "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis",
        "url": "http://arxiv.org/abs/2308.01316v1",
        "pub_date": "2023-08-02",
        "summary": "We propose an effective denoising diffusion model for generating\nhigh-resolution images (e.g., 1024$\\times$512), trained on small-size image\npatches (e.g., 64$\\times$64). We name our algorithm Patch-DM, in which a new\nfeature collage strategy is designed to avoid the boundary artifact when\nsynthesizing large-size images. Feature collage systematically crops and\ncombines partial features of the neighboring patches to predict the features of\na shifted image patch, allowing the seamless generation of the entire image due\nto the overlap in the patch feature space. Patch-DM produces high-quality image\nsynthesis results on our newly collected dataset of nature images\n(1024$\\times$512), as well as on standard benchmarks of smaller sizes\n(256$\\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ. We compare our\nmethod with previous patch-based generation methods and achieve\nstate-of-the-art FID scores on all four datasets. Further, Patch-DM also\nreduces memory complexity compared to the classic diffusion models.",
        "translated": ""
    },
    {
        "title": "More Context, Less Distraction: Visual Classification by Inferring and\n  Conditioning on Contextual Attributes",
        "url": "http://arxiv.org/abs/2308.01313v1",
        "pub_date": "2023-08-02",
        "summary": "CLIP, as a foundational vision language model, is widely used in zero-shot\nimage classification due to its ability to understand various visual concepts\nand natural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better zero-shot\nclassification is still an open question. This paper draws inspiration from the\nhuman visual perception process: a modern neuroscience view suggests that in\nclassifying an object, humans first infer its class-independent attributes\n(e.g., background and orientation) which help separate the foreground object\nfrom the background, and then make decisions based on this information.\nInspired by this, we observe that providing CLIP with contextual attributes\nimproves zero-shot classification and mitigates reliance on spurious features.\nWe also observe that CLIP itself can reasonably infer the attributes from an\nimage. With these observations, we propose a training-free, two-step zero-shot\nclassification method named PerceptionCLIP. Given an image, it first infers\ncontextual attributes (e.g., background) and then performs object\nclassification conditioning on them. Our experiments show that PerceptionCLIP\nachieves better generalization, group robustness, and better interpretability.\nFor example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by\n16.5% on the Waterbirds dataset and by 3.5% on CelebA.",
        "translated": ""
    },
    {
        "title": "Revisiting DETR Pre-training for Object Detection",
        "url": "http://arxiv.org/abs/2308.01300v1",
        "pub_date": "2023-08-02",
        "summary": "Motivated by that DETR-based approaches have established new records on COCO\ndetection and segmentation benchmarks, many recent endeavors show increasing\ninterest in how to further improve DETR-based approaches by pre-training the\nTransformer in a self-supervised manner while keeping the backbone frozen. Some\nstudies already claimed significant improvements in accuracy. In this paper, we\ntake a closer look at their experimental methodology and check if their\napproaches are still effective on the very recent state-of-the-art such as\n$\\mathcal{H}$-Deformable-DETR. We conduct thorough experiments on COCO object\ndetection tasks to study the influence of the choice of pre-training datasets,\nlocalization, and classification target generation schemes. Unfortunately, we\nfind the previous representative self-supervised approach such as DETReg, fails\nto boost the performance of the strong DETR-based approaches on full data\nregimes. We further analyze the reasons and find that simply combining a more\naccurate box predictor and Objects$365$ benchmark can significantly improve the\nresults in follow-up experiments. We demonstrate the effectiveness of our\napproach by achieving strong object detection results of AP=$59.3\\%$ on COCO\nval set, which surpasses $\\mathcal{H}$-Deformable-DETR + Swin-L by +$1.4\\%$.\nLast, we generate a series of synthetic pre-training datasets by combining the\nvery recent image-to-text captioning models (LLaVA) and text-to-image\ngenerative models (SDXL). Notably, pre-training on these synthetic datasets\nleads to notable improvements in object detection performance. Looking ahead,\nwe anticipate substantial advantages through the future expansion of the\nsynthetic pre-training dataset.",
        "translated": ""
    },
    {
        "title": "Incorporating Season and Solar Specificity into Renderings made by a\n  NeRF Architecture using Satellite Images",
        "url": "http://arxiv.org/abs/2308.01262v1",
        "pub_date": "2023-08-02",
        "summary": "As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar\nangle into account in a NeRF-based framework for rendering a scene from a novel\nviewpoint using satellite images for training. Our work extends those\ncontributions and shows how one can make the renderings season-specific. Our\nmain challenge was creating a Neural Radiance Field (NeRF) that could render\nseasonal features independently of viewing angle and solar angle while still\nbeing able to render shadows. We teach our network to render seasonal features\nby introducing one more input variable -- time of the year. However, the small\ntraining datasets typical of satellite imagery can introduce ambiguities in\ncases where shadows are present in the same location for every image of a\nparticular season. We add additional terms to the loss function to discourage\nthe network from using seasonal features for accounting for shadows. We show\nthe performance of our network on eight Areas of Interest containing images\ncaptured by the Maxar WorldView-3 satellite. This evaluation includes tests\nmeasuring the ability of our framework to accurately render novel views,\ngenerate height maps, predict shadows, and specify seasonal features\nindependently from shadows. Our ablation studies justify the choices made for\nnetwork design parameters.",
        "translated": ""
    },
    {
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and\n  Understanding of the Open World",
        "url": "http://arxiv.org/abs/2308.01907v1",
        "pub_date": "2023-08-03",
        "summary": "We present the All-Seeing (AS) project: a large-scale data and model for\nrecognizing and understanding everything in the open world. Using a scalable\ndata engine that incorporates human feedback and efficient models in the loop,\nwe create a new dataset (AS-1B) with over 1 billion regions annotated with\nsemantic tags, question-answering pairs, and detailed captions. It covers a\nwide range of 3.5 million common and rare concepts in the real world, and has\n132.2 billion tokens that describe the concepts and their attributes.\nLeveraging this new dataset, we develop the All-Seeing model (ASM), a unified\nframework for panoptic visual recognition and understanding. The model is\ntrained with open-ended language prompts and locations, which allows it to\ngeneralize to various vision and language tasks with remarkable zero-shot\nperformance, including region-text retrieval, region recognition, captioning,\nand question-answering. We hope that this project can serve as a foundation for\nvision-language artificial general intelligence research. Models and the\ndataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo\ncan be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.",
        "translated": ""
    },
    {
        "title": "Revisiting Deformable Convolution for Depth Completion",
        "url": "http://arxiv.org/abs/2308.01905v1",
        "pub_date": "2023-08-03",
        "summary": "Depth completion, which aims to generate high-quality dense depth maps from\nsparse depth maps, has attracted increasing attention in recent years. Previous\nwork usually employs RGB images as guidance, and introduces iterative spatial\npropagation to refine estimated coarse depth maps. However, most of the\npropagation refinement methods require several iterations and suffer from a\nfixed receptive field, which may contain irrelevant and useless information\nwith very sparse input. In this paper, we address these two challenges\nsimultaneously by revisiting the idea of deformable convolution. We propose an\neffective architecture that leverages deformable kernel convolution as a\nsingle-pass refinement module, and empirically demonstrate its superiority. To\nbetter understand the function of deformable convolution and exploit it for\ndepth completion, we further systematically investigate a variety of\nrepresentative strategies. Our study reveals that, different from prior work,\ndeformable convolution needs to be applied on an estimated depth map with a\nrelatively high density for better performance. We evaluate our model on the\nlarge-scale KITTI dataset and achieve state-of-the-art level performance in\nboth accuracy and inference speed. Our code is available at\nhttps://github.com/AlexSunNik/ReDC.",
        "translated": ""
    },
    {
        "title": "DETR Doesn't Need Multi-Scale or Locality Design",
        "url": "http://arxiv.org/abs/2308.01904v1",
        "pub_date": "2023-08-03",
        "summary": "This paper presents an improved DETR detector that maintains a \"plain\"\nnature: using a single-scale feature map and global cross-attention\ncalculations without specific locality constraints, in contrast to previous\nleading DETR-based detectors that reintroduce architectural inductive biases of\nmulti-scale and locality into the decoder. We show that two simple technologies\nare surprisingly effective within a plain design to compensate for the lack of\nmulti-scale feature maps and locality constraints. The first is a box-to-pixel\nrelative position bias (BoxRPB) term added to the cross-attention formulation,\nwhich well guides each query to attend to the corresponding object region while\nalso providing encoding flexibility. The second is masked image modeling\n(MIM)-based backbone pre-training which helps learn representation with\nfine-grained localization ability and proves crucial for remedying dependencies\non the multi-scale feature maps. By incorporating these technologies and recent\nadvancements in training and problem formation, the improved \"plain\" DETR\nshowed exceptional improvements over the original DETR detector. By leveraging\nthe Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using a\nSwin-L backbone, which is highly competitive with state-of-the-art detectors\nwhich all heavily rely on multi-scale feature maps and region-based feature\nextraction. Code is available at https://github.com/impiga/Plain-DETR .",
        "translated": ""
    },
    {
        "title": "UniSim: A Neural Closed-Loop Sensor Simulator",
        "url": "http://arxiv.org/abs/2308.01898v1",
        "pub_date": "2023-08-03",
        "summary": "Rigorously testing autonomy systems is essential for making safe self-driving\nvehicles (SDV) a reality. It requires one to generate safety critical scenarios\nbeyond what can be collected safely in the world, as many scenarios happen\nrarely on public roads. To accurately evaluate performance, we need to test the\nSDV on these scenarios in closed-loop, where the SDV and other actors interact\nwith each other at each timestep. Previously recorded driving logs provide a\nrich resource to build these new scenarios from, but for closed loop\nevaluation, we need to modify the sensor data based on the new scene\nconfiguration and the SDV's decisions, as actors might be added or removed and\nthe trajectories of existing actors and the SDV will differ from the original\nlog. In this paper, we present UniSim, a neural sensor simulator that takes a\nsingle recorded log captured by a sensor-equipped vehicle and converts it into\na realistic closed-loop multi-sensor simulation. UniSim builds neural feature\ngrids to reconstruct both the static background and dynamic actors in the\nscene, and composites them together to simulate LiDAR and camera data at new\nviewpoints, with actors added or removed and at new placements. To better\nhandle extrapolated views, we incorporate learnable priors for dynamic objects,\nand leverage a convolutional network to complete unseen regions. Our\nexperiments show UniSim can simulate realistic sensor data with small domain\ngap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation of\nan autonomy system on safety-critical scenarios as if it were in the real\nworld.",
        "translated": ""
    },
    {
        "title": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition\n  with Limited Annotations",
        "url": "http://arxiv.org/abs/2308.01890v1",
        "pub_date": "2023-08-03",
        "summary": "Multi-label image recognition in the low-label regime is a task of great\nchallenge and practical significance. Previous works have focused on learning\nthe alignment between textual and visual spaces to compensate for limited image\nlabels, yet may suffer from reduced accuracy due to the scarcity of\nhigh-quality multi-label annotations. In this research, we leverage the\npowerful alignment between textual and visual features pretrained with millions\nof auxiliary image-text pairs. We introduce an efficient and effective\nframework called Evidence-guided Dual Context Optimization (DualCoOp++), which\nserves as a unified approach for addressing partial-label and zero-shot\nmulti-label recognition. In DualCoOp++ we separately encode evidential,\npositive, and negative contexts for target classes as parametric components of\nthe linguistic input (i.e., prompts). The evidential context aims to discover\nall the related visual content for the target class, and serves as guidance to\naggregate positive and negative contexts from the spatial domain of the image,\nenabling better distinguishment between similar categories. Additionally, we\nintroduce a Winner-Take-All module that promotes inter-class interaction during\ntraining, while avoiding the need for extra parameters and costs. As DualCoOp++\nimposes minimal additional learnable overhead on the pretrained vision-language\nframework, it enables rapid adaptation to multi-label recognition tasks with\nlimited annotations and even unseen classes. Experiments on standard\nmulti-label recognition benchmarks across two challenging low-label settings\ndemonstrate the superior performance of our approach compared to\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
        "url": "http://arxiv.org/abs/2308.02490v1",
        "pub_date": "2023-08-04",
        "summary": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models. Code and data are\navailable at https://github.com/yuweihao/MM-Vet.",
        "translated": ""
    },
    {
        "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen\n  Convolutional CLIP",
        "url": "http://arxiv.org/abs/2308.02487v1",
        "pub_date": "2023-08-04",
        "summary": "Open-vocabulary segmentation is a challenging task requiring segmenting and\nrecognizing objects from an open set of categories. One way to address this\nchallenge is to leverage multi-modal models, such as CLIP, to provide image and\ntext features in a shared embedding space, which bridges the gap between\nclosed-vocabulary and open-vocabulary recognition. Hence, existing methods\noften adopt a two-stage framework to tackle the problem, where the inputs first\ngo through a mask generator and then through the CLIP model along with the\npredicted masks. This process involves extracting features from images multiple\ntimes, which can be ineffective and inefficient. By contrast, we propose to\nbuild everything into a single-stage framework using a shared Frozen\nConvolutional CLIP backbone, which not only significantly simplifies the\ncurrent two-stage pipeline, but also remarkably yields a better accuracy-cost\ntrade-off. The proposed FC-CLIP, benefits from the following observations: the\nfrozen CLIP backbone maintains the ability of open-vocabulary classification\nand can also serve as a strong mask generator, and the convolutional CLIP\ngeneralizes well to a larger input resolution than the one used during\ncontrastive image-text pretraining. When training on COCO panoptic data only\nand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1\nmIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2\nmIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU\non ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,\nrespectively. Additionally, the training and testing time of FC-CLIP is 7.5x\nand 6.6x significantly faster than the same prior art, while using 5.9x fewer\nparameters. FC-CLIP also sets a new state-of-the-art performance across various\nopen-vocabulary semantic segmentation datasets. Code at\nhttps://github.com/bytedance/fc-clip",
        "translated": ""
    },
    {
        "title": "Towards Generalist Foundation Model for Radiology",
        "url": "http://arxiv.org/abs/2308.02463v1",
        "pub_date": "2023-08-04",
        "summary": "In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM.We consider the construction of foundational models from\nthe perspectives of data, model design, and evaluation thoroughly. Our\ncontribution can be concluded as follows: (i), we construct a large-scale\nMedical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.\nTo the best of our knowledge, this is the first multi-modal dataset containing\n3D medical scans. (ii), We propose an architecture that enables visually\nconditioned generative pre-training, allowing for the integration of text input\ninterleaved with 2D or 3D medical scans to generate response for diverse\nradiologic tasks. The model was initially pre-trained on MedMD and subsequently\ndomain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs. (iii), we propose a new\nevaluation benchmark that comprises five tasks, aiming to comprehensively\nassess the capability of foundation models in handling practical clinical\nproblems. Our experimental results confirm that RadFM significantly outperforms\nexisting multi-modal foundation models. The codes, data, and model checkpoint\nwill all be made publicly available to promote further research and development\nin the field.",
        "translated": ""
    },
    {
        "title": "A Bi-variant Variational Model for Diffeomorphic Image Registration with\n  Relaxed Jacobian Determinant Constraints",
        "url": "http://arxiv.org/abs/2308.02393v1",
        "pub_date": "2023-08-04",
        "summary": "Diffeomorphic registration has become a powerful approach for seeking a\nsmooth and invertible spatial transformation between two coordinate systems\nwhich have been measured via the template and reference images. While the\npointwise volume-preserving constraint is effective for some problems, it is\ntoo stringent for many other problems especially when the local deformations\nare relatively large, because it may lead to a poor large-deformation for\nenforcing local matching.In this paper, we propose a novel bi-variant\ndiffeomorphic image registration model with the soft constraint of Jacobian\nequation, which allows local deformations to shrink and grow in a flexible\nrange.The Jacobian determinant of the transformation is explicitly controlled\nby optimizing the relaxation function. To prevent deformation folding and\nenhance the smoothness of deformation, we not only impose a positivity\nconstraint in optimizing the relaxation function, but also employ a regularizer\nto ensure the smoothness of the relaxation function.Furthermore, the positivity\nconstraint ensures that is as close to one as possible, which helps to obtain a\nvolume-preserving transformation on average.We further analyze the existence of\nthe minimizer for the variational model and propose a penalty splitting method\nwith a multilevel strategy to solve this model. Numerical experiments show that\nthe proposed algorithm is convergent, and the positivity constraint can control\nthe range of relative volume and not compromise registration accuracy.\nMoreover, the proposed model produces diffeomorphic maps for large deformation,\nand achieves better performance compared to the several existing registration\nmodels.",
        "translated": ""
    },
    {
        "title": "Universal Defensive Underpainting Patch: Making Your Text Invisible to\n  Optical Character Recognition",
        "url": "http://arxiv.org/abs/2308.02369v1",
        "pub_date": "2023-08-04",
        "summary": "Optical Character Recognition (OCR) enables automatic text extraction from\nscanned or digitized text images, but it also makes it easy to pirate valuable\nor sensitive text from these images. Previous methods to prevent OCR piracy by\ndistorting characters in text images are impractical in real-world scenarios,\nas pirates can capture arbitrary portions of the text images, rendering the\ndefenses ineffective. In this work, we propose a novel and effective defense\nmechanism termed the Universal Defensive Underpainting Patch (UDUP) that\nmodifies the underpainting of text images instead of the characters. UDUP is\ncreated through an iterative optimization process to craft a small, fixed-size\ndefensive patch that can generate non-overlapping underpainting for text images\nof any size. Experimental results show that UDUP effectively defends against\nunauthorized OCR under the setting of any screenshot range or complex image\nbackground. It is agnostic to the content, size, colors, and languages of\ncharacters, and is robust to typical image operations such as scaling and\ncompressing. In addition, the transferability of UDUP is demonstrated by\nevading several off-the-shelf OCRs. The code is available at\nhttps://github.com/QRICKDD/UDUP.",
        "translated": ""
    },
    {
        "title": "3D Motion Magnification: Visualizing Subtle Motions with Time Varying\n  Radiance Fields",
        "url": "http://arxiv.org/abs/2308.03757v1",
        "pub_date": "2023-08-07",
        "summary": "Motion magnification helps us visualize subtle, imperceptible motion.\nHowever, prior methods only work for 2D videos captured with a fixed camera. We\npresent a 3D motion magnification method that can magnify subtle motions from\nscenes captured by a moving camera, while supporting novel view rendering. We\nrepresent the scene with time-varying radiance fields and leverage the Eulerian\nprinciple for motion magnification to extract and amplify the variation of the\nembedding of a fixed point over time. We study and validate our proposed\nprinciple for 3D motion magnification using both implicit and tri-plane-based\nradiance fields as our underlying 3D scene representation. We evaluate the\neffectiveness of our method on both synthetic and real-world scenes captured\nunder various camera setups.",
        "translated": ""
    },
    {
        "title": "FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels",
        "url": "http://arxiv.org/abs/2308.03755v1",
        "pub_date": "2023-08-07",
        "summary": "LiDAR-based fully sparse architecture has garnered increasing attention.\nFSDv1 stands out as a representative work, achieving impressive efficacy and\nefficiency, albeit with intricate structures and handcrafted designs. In this\npaper, we present FSDv2, an evolution that aims to simplify the previous FSDv1\nwhile eliminating the inductive bias introduced by its handcrafted\ninstance-level representation, thus promoting better general applicability. To\nthis end, we introduce the concept of \\textbf{virtual voxels}, which takes over\nthe clustering-based instance segmentation in FSDv1. Virtual voxels not only\naddress the notorious issue of the Center Feature Missing problem in fully\nsparse detectors but also endow the framework with a more elegant and\nstreamlined approach. Consequently, we develop a suite of components to\ncomplement the virtual voxel concept, including a virtual voxel encoder, a\nvirtual voxel mixer, and a virtual voxel assignment strategy. Through empirical\nvalidation, we demonstrate that the virtual voxel mechanism is functionally\nsimilar to the handcrafted clustering in FSDv1 while being more general. We\nconduct experiments on three large-scale datasets: Waymo Open Dataset,\nArgoverse 2 dataset, and nuScenes dataset. Our results showcase\nstate-of-the-art performance on all three datasets, highlighting the\nsuperiority of FSDv2 in long-range scenarios and its general applicability to\nachieve competitive performance across diverse scenarios. Moreover, we provide\ncomprehensive experimental analysis to elucidate the workings of FSDv2. To\nfoster reproducibility and further research, we have open-sourced FSDv2 at\nhttps://github.com/tusen-ai/SST.",
        "translated": ""
    },
    {
        "title": "Mask Frozen-DETR: High Quality Instance Segmentation with One GPU",
        "url": "http://arxiv.org/abs/2308.03747v1",
        "pub_date": "2023-08-07",
        "summary": "In this paper, we aim to study how to build a strong instance segmenter with\nminimal training time and GPUs, as opposed to the majority of current\napproaches that pursue more accurate instance segmenter by building more\nadvanced frameworks at the cost of longer training time and higher GPU\nrequirements. To achieve this, we introduce a simple and general framework,\ntermed Mask Frozen-DETR, which can convert any existing DETR-based object\ndetection model into a powerful instance segmentation model. Our method only\nrequires training an additional lightweight mask network that predicts instance\nmasks within the bounding boxes given by a frozen DETR-based object detector.\nRemarkably, our method outperforms the state-of-the-art instance segmentation\nmethod Mask DINO in terms of performance on the COCO test-dev split (55.3% vs.\n54.7%) while being over 10X times faster to train. Furthermore, all of our\nexperiments can be trained using only one Tesla V100 GPU with 16 GB of memory,\ndemonstrating the significant efficiency of our proposed framework.",
        "translated": ""
    },
    {
        "title": "Tiny LVLM-eHub: Early Multimodal Experiments with Bard",
        "url": "http://arxiv.org/abs/2308.03729v1",
        "pub_date": "2023-08-07",
        "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nsignificant progress in tackling complex multimodal tasks. Among these\ncutting-edge developments, Google's Bard stands out for its remarkable\nmultimodal capabilities, promoting comprehensive comprehension and reasoning\nacross various domains. This work presents an early and holistic evaluation of\nLVLMs' multimodal abilities, with a particular focus on Bard, by proposing a\nlightweight variant of LVLM-eHub, named Tiny LVLM-eHub. In comparison to the\nvanilla version, Tiny LVLM-eHub possesses several appealing properties.\nFirstly, it provides a systematic assessment of six categories of multimodal\ncapabilities, including visual perception, visual knowledge acquisition, visual\nreasoning, visual commonsense, object hallucination, and embodied intelligence,\nthrough quantitative evaluation of $42$ standard text-related visual\nbenchmarks. Secondly, it conducts an in-depth analysis of LVLMs' predictions\nusing the ChatGPT Ensemble Evaluation (CEE), which leads to a robust and\naccurate evaluation and exhibits improved alignment with human evaluation\ncompared to the word matching approach. Thirdly, it comprises a mere $2.1$K\nimage-text pairs, facilitating ease of use for practitioners to evaluate their\nown offline LVLMs. Through extensive experimental analysis, this study\ndemonstrates that Bard outperforms previous LVLMs in most multimodal\ncapabilities except object hallucination, to which Bard is still susceptible.\nTiny LVLM-eHub serves as a baseline evaluation for various LVLMs and encourages\ninnovative strategies aimed at advancing multimodal techniques. Our project is\npublicly available at \\url{https://github.com/OpenGVLab/Multi-Modality-Arena}.",
        "translated": ""
    },
    {
        "title": "AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene\n  Segmentation",
        "url": "http://arxiv.org/abs/2308.03726v1",
        "pub_date": "2023-08-07",
        "summary": "Segmentation is a fundamental problem in surgical scene analysis using\nartificial intelligence. However, the inherent data scarcity in this domain\nmakes it challenging to adapt traditional segmentation techniques for this\ntask. To tackle this issue, current research employs pretrained models and\nfinetunes them on the given data. Even so, these require training deep networks\nwith millions of parameters every time new data becomes available. A recently\npublished foundation model, Segment-Anything (SAM), generalizes well to a large\nvariety of natural images, hence tackling this challenge to a reasonable\nextent. However, SAM does not generalize well to the medical domain as is\nwithout utilizing a large amount of compute resources for fine-tuning and using\ntask-specific prompts. Moreover, these prompts are in the form of\nbounding-boxes or foreground/background points that need to be annotated\nexplicitly for every image, making this solution increasingly tedious with\nhigher data size. In this work, we propose AdaptiveSAM - an adaptive\nmodification of SAM that can adjust to new datasets quickly and efficiently,\nwhile enabling text-prompted segmentation. For finetuning AdaptiveSAM, we\npropose an approach called bias-tuning that requires a significantly smaller\nnumber of trainable parameters than SAM (less than 2\\%). At the same time,\nAdaptiveSAM requires negligible expert intervention since it uses free-form\ntext as prompt and can segment the object of interest with just the label name\nas prompt. Our experiments show that AdaptiveSAM outperforms current\nstate-of-the-art methods on various medical imaging datasets including surgery,\nultrasound and X-ray. Code is available at\nhttps://github.com/JayParanjape/biastuning",
        "translated": ""
    },
    {
        "title": "When More is Less: Incorporating Additional Datasets Can Hurt\n  Performance By Introducing Spurious Correlations",
        "url": "http://arxiv.org/abs/2308.04431v1",
        "pub_date": "2023-08-08",
        "summary": "In machine learning, incorporating more data is often seen as a reliable\nstrategy for improving model performance; this work challenges that notion by\ndemonstrating that the addition of external datasets in many cases can hurt the\nresulting model's performance. In a large-scale empirical study across\ncombinations of four different open-source chest x-ray datasets and 9 different\nlabels, we demonstrate that in 43% of settings, a model trained on data from\ntwo hospitals has poorer worst group accuracy over both hospitals than a model\ntrained on just a single hospital's data. This surprising result occurs even\nthough the added hospital makes the training distribution more similar to the\ntest distribution. We explain that this phenomenon arises from the spurious\ncorrelation that emerges between the disease and hospital, due to\nhospital-specific image artifacts. We highlight the trade-off one encounters\nwhen training on multiple datasets, between the obvious benefit of additional\ndata and insidious cost of the introduced spurious correlation. In some cases,\nbalancing the dataset can remove the spurious correlation and improve\nperformance, but it is not always an effective strategy. We contextualize our\nresults within the literature on spurious correlations to help explain these\noutcomes. Our experiments underscore the importance of exercising caution when\nselecting training data for machine learning models, especially in settings\nwhere there is a risk of spurious correlations such as with medical imaging.\nThe risks outlined highlight the need for careful data selection and model\nevaluation in future research and practice.",
        "translated": ""
    },
    {
        "title": "A Deep-Learning Method Using Auto-encoder and Generative Adversarial\n  Network for Anomaly Detection on Ancient Stone Stele Surfaces",
        "url": "http://arxiv.org/abs/2308.04426v1",
        "pub_date": "2023-08-08",
        "summary": "Accurate detection of natural deterioration and man-made damage on the\nsurfaces of ancient stele in the first instance is essential for their\npreventive conservation. Existing methods for cultural heritage preservation\nare not able to achieve this goal perfectly due to the difficulty of balancing\naccuracy, efficiency, timeliness, and cost. This paper presents a deep-learning\nmethod to automatically detect above mentioned emergencies on ancient stone\nstele in real time, employing autoencoder (AE) and generative adversarial\nnetwork (GAN). The proposed method overcomes the limitations of existing\nmethods by requiring no extensive anomaly samples while enabling comprehensive\ndetection of unpredictable anomalies. the method includes stages of monitoring,\ndata acquisition, pre-processing, model structuring, and post-processing.\nTaking the Longmen Grottoes' stone steles as a case study, an unsupervised\nlearning model based on AE and GAN architectures is proposed and validated with\na reconstruction accuracy of 99.74\\%. The method's evaluation revealed the\nproficient detection of seven artificially designed anomalies and demonstrated\nprecision and reliability without false alarms. This research provides novel\nideas and possibilities for the application of deep learning in the field of\ncultural heritage.",
        "translated": ""
    },
    {
        "title": "DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from\n  Optical Satellite Images",
        "url": "http://arxiv.org/abs/2308.04417v1",
        "pub_date": "2023-08-08",
        "summary": "Optical satellite images are a critical data source; however, cloud cover\noften compromises their quality, hindering image applications and analysis.\nConsequently, effectively removing clouds from optical satellite images has\nemerged as a prominent research direction. While recent advancements in cloud\nremoval primarily rely on generative adversarial networks, which may yield\nsuboptimal image quality, diffusion models have demonstrated remarkable success\nin diverse image-generation tasks, showcasing their potential in addressing\nthis challenge. This paper presents a novel framework called DiffCR, which\nleverages conditional guided diffusion with deep convolutional networks for\nhigh-performance cloud removal for optical satellite imagery. Specifically, we\nintroduce a decoupled encoder for conditional image feature extraction,\nproviding a robust color representation to ensure the close similarity of\nappearance information between the conditional input and the synthesized\noutput. Moreover, we propose a novel and efficient time and condition fusion\nblock within the cloud removal model to accurately simulate the correspondence\nbetween the appearance in the conditional image and the target image at a low\ncomputational cost. Extensive experimental evaluations on two commonly used\nbenchmark datasets demonstrate that DiffCR consistently achieves\nstate-of-the-art performance on all metrics, with parameter and computational\ncomplexities amounting to only 5.1% and 5.4%, respectively, of those previous\nbest methods. The source code, pre-trained models, and all the experimental\nresults will be publicly available at https://github.com/XavierJiezou/DiffCR\nupon the paper's acceptance of this work.",
        "translated": ""
    },
    {
        "title": "Digging into Depth Priors for Outdoor Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2308.04413v1",
        "pub_date": "2023-08-08",
        "summary": "Neural Radiance Fields (NeRF) have demonstrated impressive performance in\nvision and graphics tasks, such as novel view synthesis and immersive reality.\nHowever, the shape-radiance ambiguity of radiance fields remains a challenge,\nespecially in the sparse viewpoints setting. Recent work resorts to integrating\ndepth priors into outdoor NeRF training to alleviate the issue. However, the\ncriteria for selecting depth priors and the relative merits of different priors\nhave not been thoroughly investigated. Moreover, the relative merits of\nselecting different approaches to use the depth priors is also an unexplored\nproblem. In this paper, we provide a comprehensive study and evaluation of\nemploying depth priors to outdoor neural radiance fields, covering common depth\nsensing technologies and most application ways. Specifically, we conduct\nextensive experiments with two representative NeRF methods equipped with four\ncommonly-used depth priors and different depth usages on two widely used\noutdoor datasets. Our experimental results reveal several interesting findings\nthat can potentially benefit practitioners and researchers in training their\nNeRF models with depth priors. Project Page:\nhttps://cwchenwang.github.io/outdoor-nerf-depth",
        "translated": ""
    },
    {
        "title": "V-DETR: DETR with Vertex Relative Position Encoding for 3D Object\n  Detection",
        "url": "http://arxiv.org/abs/2308.04409v1",
        "pub_date": "2023-08-08",
        "summary": "We introduce a highly performant 3D object detector for point clouds using\nthe DETR framework. The prior attempts all end up with suboptimal results\nbecause they fail to learn accurate inductive biases from the limited scale of\ntraining data. In particular, the queries often attend to points that are far\naway from the target objects, violating the locality principle in object\ndetection. To address the limitation, we introduce a novel 3D Vertex Relative\nPosition Encoding (3DV-RPE) method which computes position encoding for each\npoint based on its relative position to the 3D boxes predicted by the queries\nin each decoder layer, thus providing clear information to guide the model to\nfocus on points near the objects, in accordance with the principle of locality.\nIn addition, we systematically improve the pipeline from various aspects such\nas data normalization based on our understanding of the task. We show\nexceptional results on the challenging ScanNetV2 benchmark, achieving\nsignificant improvements over the previous 3DETR in\n$\\rm{AP}_{25}$/$\\rm{AP}_{50}$ from 65.0\\%/47.0\\% to 77.8\\%/66.0\\%,\nrespectively. In addition, our method sets a new record on ScanNetV2 and SUN\nRGB-D datasets.Code will be released at http://github.com/yichaoshen-MS/V-DETR.",
        "translated": ""
    },
    {
        "title": "Scene-Generalizable Interactive Segmentation of Radiance Fields",
        "url": "http://arxiv.org/abs/2308.05104v1",
        "pub_date": "2023-08-09",
        "summary": "Existing methods for interactive segmentation in radiance fields entail\nscene-specific optimization and thus cannot generalize across different scenes,\nwhich greatly limits their applicability. In this work we make the first\nattempt at Scene-Generalizable Interactive Segmentation in Radiance Fields\n(SGISRF) and propose a novel SGISRF method, which can perform 3D object\nsegmentation for novel (unseen) scenes represented by radiance fields, guided\nby only a few interactive user clicks in a given set of multi-view 2D images.\nIn particular, the proposed SGISRF focuses on addressing three crucial\nchallenges with three specially designed techniques. First, we devise the\nCross-Dimension Guidance Propagation to encode the scarce 2D user clicks into\ninformative 3D guidance representations. Second, the Uncertainty-Eliminated 3D\nSegmentation module is designed to achieve efficient yet effective 3D\nsegmentation. Third, Concealment-Revealed Supervised Learning scheme is\nproposed to reveal and correct the concealed 3D segmentation errors resulted\nfrom the supervision in 2D space with only 2D mask annotations. Extensive\nexperiments on two real-world challenging benchmarks covering diverse scenes\ndemonstrate 1) effectiveness and scene-generalizability of the proposed method,\n2) favorable performance compared to classical method requiring scene-specific\noptimization.",
        "translated": ""
    },
    {
        "title": "LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2308.05095v1",
        "pub_date": "2023-08-09",
        "summary": "In the text-to-image generation field, recent remarkable progress in Stable\nDiffusion makes it possible to generate rich kinds of novel photorealistic\nimages. However, current models still face misalignment issues (e.g.,\nproblematic spatial relation understanding and numeration failure) in complex\nnatural scenes, which impedes the high-faithfulness text-to-image generation.\nAlthough recent efforts have been made to improve controllability by giving\nfine-grained guidance (e.g., sketch and scribbles), this issue has not been\nfundamentally tackled since users have to provide such guidance information\nmanually. In this work, we strive to synthesize high-fidelity images that are\nsemantically aligned with a given textual prompt without any guidance. Toward\nthis end, we propose a coarse-to-fine paradigm to achieve layout planning and\nimage generation. Concretely, we first generate the coarse-grained layout\nconditioned on a given textual prompt via in-context learning based on Large\nLanguage Models. Afterward, we propose a fine-grained object-interaction\ndiffusion method to synthesize high-faithfulness images conditioned on the\nprompt and the automatically generated layout. Extensive experiments\ndemonstrate that our proposed method outperforms the state-of-the-art models in\nterms of layout and image generation. Our code and settings are available at\n\\url{https://layoutllm-t2i.github.io}.",
        "translated": ""
    },
    {
        "title": "A degree of image identification at sub-human scales could be possible\n  with more advanced clusters",
        "url": "http://arxiv.org/abs/2308.05092v1",
        "pub_date": "2023-08-09",
        "summary": "The purpose of the research is to determine if currently available\nself-supervised learning techniques can accomplish human level comprehension of\nvisual images using the same degree and amount of sensory input that people\nacquire from. Initial research on this topic solely considered data volume\nscaling. Here, we scale both the volume of data and the quality of the image.\nThis scaling experiment is a self-supervised learning method that may be done\nwithout any outside financing. We find that scaling up data volume and picture\nresolution at the same time enables human-level item detection performance at\nsub-human sizes.We run a scaling experiment with vision transformers trained on\nup to 200000 images up to 256 ppi.",
        "translated": ""
    },
    {
        "title": "Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic\n  Role Labeling",
        "url": "http://arxiv.org/abs/2308.05081v1",
        "pub_date": "2023-08-09",
        "summary": "Video Semantic Role Labeling (VidSRL) aims to detect the salient events from\ngiven videos, by recognizing the predict-argument event structures and the\ninterrelationships between events. While recent endeavors have put forth\nmethods for VidSRL, they can be mostly subject to two key drawbacks, including\nthe lack of fine-grained spatial scene perception and the insufficiently\nmodeling of video temporality. Towards this end, this work explores a novel\nholistic spatio-temporal scene graph (namely HostSG) representation based on\nthe existing dynamic scene graph structures, which well model both the\nfine-grained spatial semantics and temporal dynamics of videos for VidSRL.\nBuilt upon the HostSG, we present a nichetargeting VidSRL framework. A\nscene-event mapping mechanism is first designed to bridge the gap between the\nunderlying scene structure and the high-level event semantic structure,\nresulting in an overall hierarchical scene-event (termed ICE) graph structure.\nWe further perform iterative structure refinement to optimize the ICE graph,\nsuch that the overall structure representation can best coincide with end task\ndemand. Finally, three subtask predictions of VidSRL are jointly decoded, where\nthe end-to-end paradigm effectively avoids error propagation. On the benchmark\ndataset, our framework boosts significantly over the current best-performing\nmodel. Further analyses are shown for a better understanding of the advances of\nour methods.",
        "translated": ""
    },
    {
        "title": "Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI",
        "url": "http://arxiv.org/abs/2308.05074v1",
        "pub_date": "2023-08-09",
        "summary": "In order to respond effectively in the aftermath of a disaster, emergency\nservices and relief organizations rely on timely and accurate information about\nthe affected areas. Remote sensing has the potential to significantly reduce\nthe time and effort required to collect such information by enabling a rapid\nsurvey of large areas. To achieve this, the main challenge is the automatic\nextraction of relevant information from remotely sensed data. In this work, we\nshow how the combination of drone-based data with deep learning methods enables\nautomated and large-scale situation assessment. In addition, we demonstrate the\nintegration of onboard image processing techniques for the deployment of\nautonomous drone-based aid delivery. The results show the feasibility of a\nrapid and large-scale image analysis in the field, and that onboard image\nprocessing can increase the safety of drone-based aid deliveries.",
        "translated": ""
    },
    {
        "title": "Iterative Reweighted Least Squares Networks With Convergence Guarantees\n  for Solving Inverse Imaging Problems",
        "url": "http://arxiv.org/abs/2308.05745v1",
        "pub_date": "2023-08-10",
        "summary": "In this work we present a novel optimization strategy for image\nreconstruction tasks under analysis-based image regularization, which promotes\nsparse and/or low-rank solutions in some learned transform domain. We\nparameterize such regularizers using potential functions that correspond to\nweighted extensions of the $\\ell_p^p$-vector and $\\mathcal{S}_p^p$\nSchatten-matrix quasi-norms with $0 &lt; p \\le 1$. Our proposed minimization\nstrategy extends the Iteratively Reweighted Least Squares (IRLS) method,\ntypically used for synthesis-based $\\ell_p$ and $\\mathcal{S}_p$ norm and\nanalysis-based $\\ell_1$ and nuclear norm regularization. We prove that under\nmild conditions our minimization algorithm converges linearly to a stationary\npoint, and we provide an upper bound for its convergence rate. Further, to\nselect the parameters of the regularizers that deliver the best results for the\nproblem at hand, we propose to learn them from training data by formulating the\nsupervised learning process as a stochastic bilevel optimization problem. We\nshow that thanks to the convergence guarantees of our proposed minimization\nstrategy, such optimization can be successfully performed with a\nmemory-efficient implicit back-propagation scheme. We implement our learned\nIRLS variants as recurrent networks and assess their performance on the\nchallenging image reconstruction tasks of non-blind deblurring,\nsuper-resolution and demosaicking. The comparisons against other existing\nlearned reconstruction approaches demonstrate that our overall method is very\ncompetitive and in many cases outperforms existing unrolled networks, whose\nnumber of parameters is orders of magnitude higher than in our case.",
        "translated": ""
    },
    {
        "title": "PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views\n  with Learnt Shape Programs",
        "url": "http://arxiv.org/abs/2308.05744v1",
        "pub_date": "2023-08-10",
        "summary": "In this paper, we develop a new method to automatically convert 2D line\ndrawings from three orthographic views into 3D CAD models. Existing methods for\nthis problem reconstruct 3D models by back-projecting the 2D observations into\n3D space while maintaining explicit correspondence between the input and\noutput. Such methods are sensitive to errors and noises in the input, thus\noften fail in practice where the input drawings created by human designers are\nimperfect. To overcome this difficulty, we leverage the attention mechanism in\na Transformer-based sequence generation model to learn flexible mappings\nbetween the input and output. Further, we design shape programs which are\nsuitable for generating the objects of interest to boost the reconstruction\naccuracy and facilitate CAD modeling applications. Experiments on a new\nbenchmark dataset show that our method significantly outperforms existing ones\nwhen the inputs are noisy or incomplete.",
        "translated": ""
    },
    {
        "title": "Neural Progressive Meshes",
        "url": "http://arxiv.org/abs/2308.05741v1",
        "pub_date": "2023-08-10",
        "summary": "The recent proliferation of 3D content that can be consumed on hand-held\ndevices necessitates efficient tools for transmitting large geometric data,\ne.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a\nchallenge to storage as well as transmission bandwidth, and level-of-detail\ntechniques are often used to transmit an asset using an appropriate bandwidth\nbudget. It is especially desirable for these methods to transmit data\nprogressively, improving the quality of the geometry with more data. Our key\ninsight is that the geometric details of 3D meshes often exhibit similar local\npatterns even across different shapes, and thus can be effectively represented\nwith a shared learned generative space. We learn this space using a\nsubdivision-based encoder-decoder architecture trained in advance on a large\ncollection of surfaces. We further observe that additional residual features\ncan be transmitted progressively between intermediate levels of subdivision\nthat enable the client to control the tradeoff between bandwidth cost and\nquality of reconstruction, providing a neural progressive mesh representation.\nWe evaluate our method on a diverse set of complex 3D shapes and demonstrate\nthat it outperforms baselines in terms of compression ratio and reconstruction\nquality.",
        "translated": ""
    },
    {
        "title": "Zero Grads Ever Given: Learning Local Surrogate Losses for\n  Non-Differentiable Graphics",
        "url": "http://arxiv.org/abs/2308.05739v1",
        "pub_date": "2023-08-10",
        "summary": "Gradient-based optimization is now ubiquitous across graphics, but\nunfortunately can not be applied to problems with undefined or zero gradients.\nTo circumvent this issue, the loss function can be manually replaced by a\n\"surrogate\" that has similar minima but is differentiable. Our proposed\nframework, ZeroGrads, automates this process by learning a neural approximation\nof the objective function, the surrogate, which in turn can be used to\ndifferentiate through arbitrary black-box graphics pipelines. We train the\nsurrogate on an actively smoothed version of the objective and encourage\nlocality, focusing the surrogate's capacity on what matters at the current\ntraining episode. The fitting is performed online, alongside the parameter\noptimization, and self-supervised, without pre-computed data or pre-trained\nmodels. As sampling the objective is expensive (it requires a full rendering or\nsimulator run), we devise an efficient sampling scheme that allows for\ntractable run-times and competitive performance at little overhead. We\ndemonstrate optimizing diverse non-convex, non-differentiable black-box\nproblems in graphics, such as visibility in rendering, discrete parameter\nspaces in procedural modelling or optimal control in physics-driven animation.\nIn contrast to more traditional algorithms, our approach scales well to higher\ndimensions, which we demonstrate on problems with up to 35k interlinked\nvariables.",
        "translated": ""
    },
    {
        "title": "Follow Anything: Open-set detection, tracking, and following in\n  real-time",
        "url": "http://arxiv.org/abs/2308.05737v1",
        "pub_date": "2023-08-10",
        "summary": "Tracking and following objects of interest is critical to several robotics\nuse cases, ranging from industrial automation to logistics and warehousing, to\nhealthcare and security. In this paper, we present a robotic system to detect,\ntrack, and follow any object in real-time. Our approach, dubbed ``follow\nanything'' (FAn), is an open-vocabulary and multimodal model -- it is not\nrestricted to concepts seen at training time and can be applied to novel\nclasses at inference time using text, images, or click queries. Leveraging rich\nvisual descriptors from large-scale pre-trained models (foundation models), FAn\ncan detect and segment objects by matching multimodal queries (text, images,\nclicks) against an input image sequence. These detected and segmented objects\nare tracked across image frames, all while accounting for occlusion and object\nre-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial\nvehicle) and report its ability to seamlessly follow the objects of interest in\na real-time control loop. FAn can be deployed on a laptop with a lightweight\n(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To\nenable rapid adoption, deployment, and extensibility, we open-source all our\ncode on our project webpage at https://github.com/alaamaalouf/FollowAnything .\nWe also encourage the reader the watch our 5-minutes explainer video in this\nhttps://www.youtube.com/watch?v=6Mgt3EPytrw .",
        "translated": ""
    },
    {
        "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of\n  Explainable AI Methods",
        "url": "http://arxiv.org/abs/2308.06248v1",
        "pub_date": "2023-08-11",
        "summary": "The field of explainable artificial intelligence (XAI) aims to uncover the\ninner workings of complex deep neural models. While being crucial for\nsafety-critical domains, XAI inherently lacks ground-truth explanations, making\nits automatic evaluation an unsolved problem. We address this challenge by\nproposing a novel synthetic vision dataset, named FunnyBirds, and accompanying\nautomatic evaluation protocols. Our dataset allows performing semantically\nmeaningful image interventions, e.g., removing individual object parts, which\nhas three important implications. First, it enables analyzing explanations on a\npart level, which is closer to human comprehension than existing methods that\nevaluate on a pixel level. Second, by comparing the model output for inputs\nwith removed parts, we can estimate ground-truth part importances that should\nbe reflected in the explanations. Third, by mapping individual explanations\ninto a common space of part importances, we can analyze a variety of different\nexplanation types in a single common framework. Using our tools, we report\nresults for 24 different combinations of neural models and XAI methods,\ndemonstrating the strengths and weaknesses of the assessed methods in a fully\nautomatic and systematic manner.",
        "translated": ""
    },
    {
        "title": "Continual Face Forgery Detection via Historical Distribution Preserving",
        "url": "http://arxiv.org/abs/2308.06217v1",
        "pub_date": "2023-08-11",
        "summary": "Face forgery techniques have advanced rapidly and pose serious security\nthreats. Existing face forgery detection methods try to learn generalizable\nfeatures, but they still fall short of practical application. Additionally,\nfinetuning these methods on historical training data is resource-intensive in\nterms of time and storage. In this paper, we focus on a novel and challenging\nproblem: Continual Face Forgery Detection (CFFD), which aims to efficiently\nlearn from new forgery attacks without forgetting previous ones. Specifically,\nwe propose a Historical Distribution Preserving (HDP) framework that reserves\nand preserves the distributions of historical faces. To achieve this, we use\nuniversal adversarial perturbation (UAP) to simulate historical forgery\ndistribution, and knowledge distillation to maintain the distribution variation\nof real faces across different models. We also construct a new benchmark for\nCFFD with three evaluation protocols. Our extensive experiments on the\nbenchmarks show that our method outperforms the state-of-the-art competitors.",
        "translated": ""
    },
    {
        "title": "Exploring Predicate Visual Context in Detecting of Human-Object\n  Interactions",
        "url": "http://arxiv.org/abs/2308.06202v1",
        "pub_date": "2023-08-11",
        "summary": "Recently, the DETR framework has emerged as the dominant approach for\nhuman--object interaction (HOI) research. In particular, two-stage\ntransformer-based HOI detectors are amongst the most performant and\ntraining-efficient approaches. However, these often condition HOI\nclassification on object features that lack fine-grained contextual\ninformation, eschewing pose and orientation information in favour of visual\ncues about object identity and box extremities. This naturally hinders the\nrecognition of complex or ambiguous interactions. In this work, we study these\nissues through visualisations and carefully designed experiments. Accordingly,\nwe investigate how best to re-introduce image features via cross-attention.\nWith an improved query design, extensive exploration of keys and values, and\nbox pair positional embeddings as spatial guidance, our model with enhanced\npredicate visual context (PViC) outperforms state-of-the-art methods on the\nHICO-DET and V-COCO benchmarks, while maintaining low training cost.",
        "translated": ""
    },
    {
        "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for\n  Geographic Diversity",
        "url": "http://arxiv.org/abs/2308.06198v1",
        "pub_date": "2023-08-11",
        "summary": "The unprecedented photorealistic results achieved by recent text-to-image\ngenerative systems and their increasing use as plug-and-play content creation\nsolutions make it crucial to understand their potential biases. In this work,\nwe introduce three indicators to evaluate the realism, diversity and\nprompt-generation consistency of text-to-image generative systems when prompted\nto generate objects from across the world. Our indicators complement\nqualitative analysis of the broader impact of such systems by enabling\nautomatic and efficient benchmarking of geographic disparities, an important\nstep towards building responsible visual content creation systems. We use our\nproposed indicators to analyze potential geographic biases in state-of-the-art\nvisual content creation systems and find that: (1) models have less realism and\ndiversity of generations when prompting for Africa and West Asia than Europe,\n(2) prompting with geographic information comes at a cost to prompt-consistency\nand diversity of generated images, and (3) models exhibit more region-level\ndisparities for some objects than others. Perhaps most interestingly, our\nindicators suggest that progress in image generation quality has come at the\ncost of real-world geographic representation. Our comprehensive evaluation\nconstitutes a crucial step towards ensuring a positive experience of visual\ncontent creation for everyone.",
        "translated": ""
    },
    {
        "title": "Complex Facial Expression Recognition Using Deep Knowledge Distillation\n  of Basic Features",
        "url": "http://arxiv.org/abs/2308.06197v1",
        "pub_date": "2023-08-11",
        "summary": "Complex emotion recognition is a cognitive task that has so far eluded the\nsame excellent performance of other tasks that are at or above the level of\nhuman cognition. Emotion recognition through facial expressions is particularly\ndifficult due to the complexity of emotions expressed by the human face. For a\nmachine to approach the same level of performance in this domain as a human, it\nmay need to synthesise knowledge and understand new concepts in real-time as\nhumans do. Humans are able to learn new concepts using only few examples, by\ndistilling the important information from memories and discarding the rest.\nSimilarly, continual learning methods learn new classes whilst retaining the\nknowledge of known classes, whilst few-shot learning methods are able to learn\nnew classes using very few training examples. We propose a novel continual\nlearning method inspired by human cognition and learning that can accurately\nrecognise new compound expression classes using few training samples, by\nbuilding on and retaining its knowledge of basic expression classes. Using\nGradCAM visualisations, we demonstrate the relationship between basic and\ncompound facial expressions, which our method leverages through knowledge\ndistillation and a novel Predictive Sorting Memory Replay. Our method achieves\nthe current state-of-the-art in continual learning for complex facial\nexpression recognition with 74.28% Overall Accuracy on new classes. We also\ndemonstrate that using continual learning for complex facial expression\nrecognition achieves far better performance than non-continual learning\nmethods, improving on state-of-the-art non-continual learning methods by\n13.95%. To the best of our knowledge, our work is also the first to apply\nfew-shot learning to complex facial expression recognition, achieving the\nstate-of-the-art with 100% accuracy using a single training sample for each\nexpression class.",
        "translated": ""
    },
    {
        "title": "Jurassic World Remake: Bringing Ancient Fossils Back to Life via\n  Zero-Shot Long Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2308.07316v1",
        "pub_date": "2023-08-14",
        "summary": "With a strong understanding of the target domain from natural language, we\nproduce promising results in translating across large domain gaps and bringing\nskeletons back to life. In this work, we use text-guided latent diffusion\nmodels for zero-shot image-to-image translation (I2I) across large domain gaps\n(longI2I), where large amounts of new visual features and new geometry need to\nbe generated to enter the target domain. Being able to perform translations\nacross large domain gaps has a wide variety of real-world applications in\ncriminology, astrology, environmental conservation, and paleontology. In this\nwork, we introduce a new task Skull2Animal for translating between skulls and\nliving animals. On this task, we find that unguided Generative Adversarial\nNetworks (GANs) are not capable of translating across large domain gaps.\nInstead of these traditional I2I methods, we explore the use of guided\ndiffusion and image editing models and provide a new benchmark model,\nRevive-2I, capable of performing zero-shot I2I via text-prompting latent\ndiffusion models. We find that guidance is necessary for longI2I because, to\nbridge the large domain gap, prior knowledge about the target domain is needed.\nIn addition, we find that prompting provides the best and most scalable\ninformation about the target domain as classifier-guided diffusion models\nrequire retraining for specific use cases and lack stronger constraints on the\ntarget domain because of the wide variety of images they are trained on.",
        "translated": ""
    },
    {
        "title": "Dual Associated Encoder for Face Restoration",
        "url": "http://arxiv.org/abs/2308.07314v1",
        "pub_date": "2023-08-14",
        "summary": "Restoring facial details from low-quality (LQ) images has remained a\nchallenging problem due to its ill-posedness induced by various degradations in\nthe wild. The existing codebook prior mitigates the ill-posedness by leveraging\nan autoencoder and learned codebook of high-quality (HQ) features, achieving\nremarkable quality. However, existing approaches in this paradigm frequently\ndepend on a single encoder pre-trained on HQ data for restoring HQ images,\ndisregarding the domain gap between LQ and HQ images. As a result, the encoding\nof LQ inputs may be insufficient, resulting in suboptimal performance. To\ntackle this problem, we propose a novel dual-branch framework named DAEFR. Our\nmethod introduces an auxiliary LQ branch that extracts crucial information from\nthe LQ inputs. Additionally, we incorporate association training to promote\neffective synergy between the two branches, enhancing code prediction and\noutput quality. We evaluate the effectiveness of DAEFR on both synthetic and\nreal-world datasets, demonstrating its superior performance in restoring facial\ndetails.",
        "translated": ""
    },
    {
        "title": "Group Pose: A Simple Baseline for End-to-End Multi-person Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2308.07313v1",
        "pub_date": "2023-08-14",
        "summary": "In this paper, we study the problem of end-to-end multi-person pose\nestimation. State-of-the-art solutions adopt the DETR-like framework, and\nmainly develop the complex decoder, e.g., regarding pose estimation as keypoint\nbox detection and combining with human detection in ED-Pose, hierarchically\npredicting with pose decoder and joint (keypoint) decoder in PETR. We present a\nsimple yet effective transformer approach, named Group Pose. We simply regard\n$K$-keypoint pose estimation as predicting a set of $N\\times K$ keypoint\npositions, each from a keypoint query, as well as representing each pose with\nan instance query for scoring $N$ pose predictions. Motivated by the intuition\nthat the interaction, among across-instance queries of different types, is not\ndirectly helpful, we make a simple modification to decoder self-attention. We\nreplace single self-attention over all the $N\\times(K+1)$ queries with two\nsubsequent group self-attentions: (i) $N$ within-instance self-attention, with\neach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$\nsame-type across-instance self-attention, each over $N$ queries of the same\ntype. The resulting decoder removes the interaction among across-instance\ntype-different queries, easing the optimization and thus improving the\nperformance. Experimental results on MS COCO and CrowdPose show that our\napproach without human box supervision is superior to previous methods with\ncomplex decoders, and even is slightly better than ED-Pose that uses human box\nsupervision. $\\href{https://github.com/Michel-liu/GroupPose-Paddle}{\\rm\nPaddle}$ and $\\href{https://github.com/Michel-liu/GroupPose}{\\rm PyTorch}$ code\nare available.",
        "translated": ""
    },
    {
        "title": "A Unified Masked Autoencoder with Patchified Skeletons for Motion\n  Synthesis",
        "url": "http://arxiv.org/abs/2308.07301v1",
        "pub_date": "2023-08-14",
        "summary": "The synthesis of human motion has traditionally been addressed through\ntask-dependent models that focus on specific challenges, such as predicting\nfuture motions or filling in intermediate poses conditioned on known key-poses.\nIn this paper, we present a novel task-independent model called UNIMASK-M,\nwhich can effectively address these challenges using a unified architecture.\nOur model obtains comparable or better performance than the state-of-the-art in\neach field. Inspired by Vision Transformers (ViTs), our UNIMASK-M model\ndecomposes a human pose into body parts to leverage the spatio-temporal\nrelationships existing in human motion. Moreover, we reformulate various\npose-conditioned motion synthesis tasks as a reconstruction problem with\ndifferent masking patterns given as input. By explicitly informing our model\nabout the masked joints, our UNIMASK-M becomes more robust to occlusions.\nExperimental results show that our model successfully forecasts human motion on\nthe Human3.6M dataset. Moreover, it achieves state-of-the-art results in motion\ninbetweening on the LaFAN1 dataset, particularly in long transition periods.\nMore information can be found on the project website\nhttps://sites.google.com/view/estevevallsmascaro/publications/unimask-m.",
        "translated": ""
    },
    {
        "title": "Accurate Eye Tracking from Dense 3D Surface Reconstructions using\n  Single-Shot Deflectometry",
        "url": "http://arxiv.org/abs/2308.07298v1",
        "pub_date": "2023-08-14",
        "summary": "Eye-tracking plays a crucial role in the development of virtual reality\ndevices, neuroscience research, and psychology. Despite its significance in\nnumerous applications, achieving an accurate, robust, and fast eye-tracking\nsolution remains a considerable challenge for current state-of-the-art methods.\nWhile existing reflection-based techniques (e.g., \"glint tracking\") are\nconsidered the most accurate, their performance is limited by their reliance on\nsparse 3D surface data acquired solely from the cornea surface. In this paper,\nwe rethink the way how specular reflections can be used for eye tracking: We\npropose a novel method for accurate and fast evaluation of the gaze direction\nthat exploits teachings from single-shot phase-measuring-deflectometry (PMD).\nIn contrast to state-of-the-art reflection-based methods, our method acquires\ndense 3D surface information of both cornea and sclera within only one single\ncamera frame (single-shot). Improvements in acquired reflection surface\npoints(\"glints\") of factors $&gt;3300 \\times$ are easily achievable. We show the\nfeasibility of our approach with experimentally evaluated gaze errors of only\n$\\leq 0.25^\\circ$ demonstrating a significant improvement over the current\nstate-of-the-art.",
        "translated": ""
    },
    {
        "title": "CoDeF: Content Deformation Fields for Temporally Consistent Video\n  Processing",
        "url": "http://arxiv.org/abs/2308.07926v1",
        "pub_date": "2023-08-15",
        "summary": "We present the content deformation field CoDeF as a new type of video\nrepresentation, which consists of a canonical content field aggregating the\nstatic contents in the entire video and a temporal deformation field recording\nthe transformations from the canonical image (i.e., rendered from the canonical\ncontent field) to each individual frame along the time axis.Given a target\nvideo, these two fields are jointly optimized to reconstruct it through a\ncarefully tailored rendering pipeline.We advisedly introduce some\nregularizations into the optimization process, urging the canonical content\nfield to inherit semantics (e.g., the object shape) from the video.With such a\ndesign, CoDeF naturally supports lifting image algorithms for video processing,\nin the sense that one can apply an image algorithm to the canonical image and\neffortlessly propagate the outcomes to the entire video with the aid of the\ntemporal deformation field.We experimentally show that CoDeF is able to lift\nimage-to-image translation to video-to-video translation and lift keypoint\ndetection to keypoint tracking without any training.More importantly, thanks to\nour lifting strategy that deploys the algorithms on only one image, we achieve\nsuperior cross-frame consistency in processed videos compared to existing\nvideo-to-video translation approaches, and even manage to track non-rigid\nobjects like water and smog.Project page can be found at\nhttps://qiuyu96.github.io/CoDeF/.",
        "translated": ""
    },
    {
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with\n  Code-based Self-Verification",
        "url": "http://arxiv.org/abs/2308.07921v1",
        "pub_date": "2023-08-15",
        "summary": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has\nbrought significant advancements in addressing math reasoning problems. In\nparticular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,\nshows remarkable performance on challenging math datasets. In this paper, we\nexplore the effect of code on enhancing LLMs' reasoning capability by\nintroducing different constraints on the \\textit{Code Usage Frequency} of GPT-4\nCode Interpreter. We found that its success can be largely attributed to its\npowerful skills in generating and executing code, evaluating the output of code\nexecution, and rectifying its solution when receiving unreasonable outputs.\nBased on this insight, we propose a novel and effective prompting method,\nexplicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further\nboost the mathematical reasoning potential of GPT-4 Code Interpreter. This\nmethod employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to\nuse code to self-verify its answers. In instances where the verification state\nregisters as ``False'', the model shall automatically amend its solution,\nanalogous to our approach of rectifying errors during a mathematics\nexamination. Furthermore, we recognize that the states of the verification\nresult indicate the confidence of a solution, which can improve the\neffectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we\nachieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$\n84.3\\%)}.",
        "translated": ""
    },
    {
        "title": "Helping Hands: An Object-Aware Ego-Centric Video Recognition Model",
        "url": "http://arxiv.org/abs/2308.07918v1",
        "pub_date": "2023-08-15",
        "summary": "We introduce an object-aware decoder for improving the performance of\nspatio-temporal representations on ego-centric videos. The key idea is to\nenhance object-awareness during training by tasking the model to predict hand\npositions, object positions, and the semantic label of the objects using paired\ncaptions when available. At inference time the model only requires RGB frames\nas inputs, and is able to track and ground objects (although it has not been\ntrained explicitly for this). We demonstrate the performance of the\nobject-aware representations learnt by our model, by: (i) evaluating it for\nstrong transfer, i.e. through zero-shot testing, on a number of downstream\nvideo-text retrieval and classification benchmarks; and (ii) by using the\nrepresentations learned as input for long-term video understanding tasks (e.g.\nEpisodic Memory in Ego4D). In all cases the performance improves over the state\nof the art -- even compared to networks trained with far larger batch sizes. We\nalso show that by using noisy image-level detection as pseudo-labels in\ntraining, the model learns to provide better bounding boxes using video\nconsistency, as well as grounding the words in the associated text\ndescriptions. Overall, we show that the model can act as a drop-in replacement\nfor an ego-centric video model to improve performance through visual-text\ngrounding.",
        "translated": ""
    },
    {
        "title": "Relightable and Animatable Neural Avatar from Sparse-View Video",
        "url": "http://arxiv.org/abs/2308.07903v1",
        "pub_date": "2023-08-15",
        "summary": "This paper tackles the challenge of creating relightable and animatable\nneural avatars from sparse-view (or even monocular) videos of dynamic humans\nunder unknown illumination. Compared to studio environments, this setting is\nmore practical and accessible but poses an extremely challenging ill-posed\nproblem. Previous neural human reconstruction methods are able to reconstruct\nanimatable avatars from sparse views using deformed Signed Distance Fields\n(SDF) but cannot recover material parameters for relighting. While\ndifferentiable inverse rendering-based methods have succeeded in material\nrecovery of static objects, it is not straightforward to extend them to dynamic\nhumans as it is computationally intensive to compute pixel-surface intersection\nand light visibility on deformed SDFs for inverse rendering. To solve this\nchallenge, we propose a Hierarchical Distance Query (HDQ) algorithm to\napproximate the world space distances under arbitrary human poses.\nSpecifically, we estimate coarse distances based on a parametric human model\nand compute fine distances by exploiting the local deformation invariance of\nSDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently\nestimate the surface intersection and light visibility. This allows us to\ndevelop the first system to recover animatable and relightable neural avatars\nfrom sparse view (or monocular) inputs. Experiments demonstrate that our\napproach is able to produce superior results compared to state-of-the-art\nmethods. Our code will be released for reproducibility.",
        "translated": ""
    },
    {
        "title": "A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert\n  knowledge in text supervision",
        "url": "http://arxiv.org/abs/2308.07898v1",
        "pub_date": "2023-08-15",
        "summary": "Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 37 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 97 different target conditions and 284,660\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a large margin more\ngeneralist, larger-scale image-language models, which emphasizes the potential\nof embedding experts' domain knowledge and the limitations of generalist models\nin medical imaging.",
        "translated": ""
    },
    {
        "title": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans",
        "url": "http://arxiv.org/abs/2308.08545v1",
        "pub_date": "2023-08-16",
        "summary": "Despite recent research advancements in reconstructing clothed humans from a\nsingle image, accurately restoring the \"unseen regions\" with high-level details\nremains an unsolved challenge that lacks attention. Existing methods often\ngenerate overly smooth back-side surfaces with a blurry texture. But how to\neffectively capture all visual attributes of an individual from a single image,\nwhich are sufficient to reconstruct unseen areas (e.g., the back view)?\nMotivated by the power of foundation models, TeCH reconstructs the 3D human by\nleveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles)\nwhich are automatically generated via a garment parsing model and Visual\nQuestion Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion\nmodel (T2I) which learns the \"indescribable\" appearance. To represent\nhigh-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D\nrepresentation based on DMTet, which consists of an explicit body shape grid\nand an implicit distance field. Guided by the descriptive prompts +\npersonalized T2I diffusion model, the geometry and texture of the 3D humans are\noptimized through multi-view Score Distillation Sampling (SDS) and\nreconstruction losses based on the original observation. TeCH produces\nhigh-fidelity 3D clothed humans with consistent &amp; delicate texture, and\ndetailed full-body geometry. Quantitative and qualitative experiments\ndemonstrate that TeCH outperforms the state-of-the-art methods in terms of\nreconstruction accuracy and rendering quality. The code will be publicly\navailable for research purposes at https://huangyangyi.github.io/tech",
        "translated": ""
    },
    {
        "title": "MeViS: A Large-scale Benchmark for Video Segmentation with Motion\n  Expressions",
        "url": "http://arxiv.org/abs/2308.08544v1",
        "pub_date": "2023-08-16",
        "summary": "This paper strives for motion expressions guided video segmentation, which\nfocuses on segmenting objects in video content based on a sentence describing\nthe motion of the objects. Existing referring video object datasets typically\nfocus on salient objects and use language expressions that contain excessive\nstatic attributes that could potentially enable the target object to be\nidentified in a single frame. These datasets downplay the importance of motion\nin video content for language-guided video object segmentation. To investigate\nthe feasibility of using motion expressions to ground and segment objects in\nvideos, we propose a large-scale dataset called MeViS, which contains numerous\nmotion expressions to indicate target objects in complex environments. We\nbenchmarked 5 existing referring video object segmentation (RVOS) methods and\nconducted a comprehensive comparison on the MeViS dataset. The results show\nthat current RVOS methods cannot effectively address motion expression-guided\nvideo segmentation. We further analyze the challenges and propose a baseline\napproach for the proposed MeViS dataset. The goal of our benchmark is to\nprovide a platform that enables the development of effective language-guided\nvideo segmentation algorithms that leverage motion expressions as a primary cue\nfor object segmentation in complex video scenes. The proposed MeViS dataset has\nbeen released at https://henghuiding.github.io/MeViS.",
        "translated": ""
    },
    {
        "title": "InsightMapper: A Closer Look at Inner-instance Information for\n  Vectorized High-Definition Mapping",
        "url": "http://arxiv.org/abs/2308.08543v1",
        "pub_date": "2023-08-16",
        "summary": "Vectorized high-definition (HD) maps contain detailed information about\nsurrounding road elements, which are crucial for various downstream tasks in\nmodern autonomous driving vehicles, such as vehicle planning and control.\nRecent works have attempted to directly detect the vectorized HD map as a point\nset prediction task, resulting in significant improvements in detection\nperformance. However, these approaches fail to analyze and exploit the\ninner-instance correlations between predicted points, impeding further\nadvancements. To address these challenges, we investigate the utilization of\ninner-$\\textbf{INS}$tance information for vectorized h$\\textbf{IGH}$-definition\nmapping through $\\textbf{T}$ransformers and introduce InsightMapper. This paper\npresents three novel designs within InsightMapper that leverage inner-instance\ninformation in distinct ways, including hybrid query generation, inner-instance\nquery fusion, and inner-instance feature aggregation. Comparative experiments\nare conducted on the NuScenes dataset, showcasing the superiority of our\nproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)\nmethods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.\nSimultaneously, InsightMapper maintains high efficiency during both training\nand inference phases, resulting in remarkable comprehensive performance. The\nproject page for this work is available at\nhttps://tonyxuqaq.github.io/projects/InsightMapper .",
        "translated": ""
    },
    {
        "title": "Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an\n  Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstructio",
        "url": "http://arxiv.org/abs/2308.08530v1",
        "pub_date": "2023-08-16",
        "summary": "Neural Radiance Fields (NeRFs) have revolutionized the field of novel view\nsynthesis, demonstrating remarkable performance. However, the modeling and\nrendering of reflective objects remain challenging problems. Recent methods\nhave shown significant improvements over the baselines in handling reflective\nscenes, albeit at the expense of efficiency. In this work, we aim to strike a\nbalance between efficiency and quality. To this end, we investigate an\nimplicit-explicit approach based on conventional volume rendering to enhance\nthe reconstruction quality and accelerate the training and rendering processes.\nWe adopt an efficient density-based grid representation and reparameterize the\nreflected radiance in our pipeline. Our proposed reflection-aware approach\nachieves a competitive quality efficiency trade-off compared to competing\nmethods. Based on our experimental results, we propose and discuss hypotheses\nregarding the factors influencing the results of density-based methods for\nreconstructing reflective objects. The source code is available at:\nhttps://github.com/gkouros/ref-dvgo",
        "translated": ""
    },
    {
        "title": "Diagnosing Human-object Interaction Detectors",
        "url": "http://arxiv.org/abs/2308.08529v1",
        "pub_date": "2023-08-16",
        "summary": "Although we have witnessed significant progress in human-object interaction\n(HOI) detection with increasingly high mAP (mean Average Precision), a single\nmAP score is too concise to obtain an informative summary of a model's\nperformance and to understand why one approach is better than another. In this\npaper, we introduce a diagnosis toolbox for analyzing the error sources of the\nexisting HOI detection models. We first conduct holistic investigations in the\npipeline of HOI detection, consisting of human-object pair detection and then\ninteraction classification. We define a set of errors and the oracles to fix\neach of them. By measuring the mAP improvement obtained from fixing an error\nusing its oracle, we can have a detailed analysis of the significance of\ndifferent errors. We then delve into the human-object detection and interaction\nclassification, respectively, and check the model's behavior. For the first\ndetection task, we investigate both recall and precision, measuring the\ncoverage of ground-truth human-object pairs as well as the noisiness level in\nthe detections. For the second classification task, we compute mAP for\ninteraction classification only, without considering the detection scores. We\nalso measure the performance of the models in differentiating human-object\npairs with and without actual interactions using the AP (Average Precision)\nscore. Our toolbox is applicable for different methods across different\ndatasets and available at https://github.com/neu-vi/Diag-HOI.",
        "translated": ""
    },
    {
        "title": "Towards Large-scale 3D Representation Learning with Multi-dataset Point\n  Prompt Training",
        "url": "http://arxiv.org/abs/2308.09718v1",
        "pub_date": "2023-08-18",
        "summary": "The rapid advancement of deep learning models often attributes to their\nability to leverage massive training data. In contrast, such privilege has not\nyet fully benefited 3D deep learning, mainly due to the limited availability of\nlarge-scale 3D datasets. Merging multiple available data sources and letting\nthem collaboratively train a single model is a potential solution. However, due\nto the large domain gap between 3D point cloud datasets, such mixed supervision\ncould adversely affect the model's performance and lead to degenerated\nperformance (i.e., negative transfer) compared to single-dataset training. In\nview of this challenge, we introduce Point Prompt Training (PPT), a novel\nframework for multi-dataset synergistic learning in the context of 3D\nrepresentation learning that supports multiple pre-training paradigms. Based on\nthis framework, we propose Prompt-driven Normalization, which adapts the model\nto different datasets with domain-specific prompts and Language-guided\nCategorical Alignment that decently unifies the multiple-dataset label spaces\nby leveraging the relationship between label text. Extensive experiments verify\nthat PPT can overcome the negative transfer associated with synergistic\nlearning and produce generalizable representations. Notably, it achieves\nstate-of-the-art performance on each dataset using a single weight-shared model\nwith supervised multi-dataset training. Moreover, when served as a pre-training\nframework, it outperforms other pre-training approaches regarding\nrepresentation quality and attains remarkable state-of-the-art performance\nacross over ten diverse downstream tasks spanning both indoor and outdoor 3D\nscenarios.",
        "translated": ""
    },
    {
        "title": "Smoothness Similarity Regularization for Few-Shot GAN Adaptation",
        "url": "http://arxiv.org/abs/2308.09717v1",
        "pub_date": "2023-08-18",
        "summary": "The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model to\na small dataset with very few training images. While existing methods perform\nwell when the dataset for pre-training is structurally similar to the target\ndataset, the approaches suffer from training instabilities or memorization\nissues when the objects in the two domains have a very different structure. To\nmitigate this limitation, we propose a new smoothness similarity regularization\nthat transfers the inherently learned smoothness of the pre-trained GAN to the\nfew-shot target domain even if the two domains are very different. We evaluate\nour approach by adapting an unconditional and a class-conditional GAN to\ndiverse few-shot target domains. Our proposed method significantly outperforms\nprior few-shot GAN adaptation methods in the challenging case of structurally\ndissimilar source-target domains, while performing on par with the state of the\nart for similar source-target domains.",
        "translated": ""
    },
    {
        "title": "Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization",
        "url": "http://arxiv.org/abs/2308.09716v1",
        "pub_date": "2023-08-18",
        "summary": "The task of lip synchronization (lip-sync) seeks to match the lips of human\nfaces with different audio. It has various applications in the film industry as\nwell as for creating virtual avatars and for video conferencing. This is a\nchallenging problem as one needs to simultaneously introduce detailed,\nrealistic lip movements while preserving the identity, pose, emotions, and\nimage quality. Many of the previous methods trying to solve this problem suffer\nfrom image quality degradation due to a lack of complete contextual\ninformation. In this paper, we present Diff2Lip, an audio-conditioned\ndiffusion-based model which is able to do lip synchronization in-the-wild while\npreserving these qualities. We train our model on Voxceleb2, a video dataset\ncontaining in-the-wild talking face videos. Extensive studies show that our\nmethod outperforms popular methods like Wav2Lip and PC-AVS in Fr\\'echet\ninception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We\nshow results on both reconstruction (same audio-video inputs) as well as cross\n(different audio-video inputs) settings on Voxceleb2 and LRW datasets. Video\nresults and code can be accessed from our project page (\nhttps://soumik-kanad.github.io/diff2lip ).",
        "translated": ""
    },
    {
        "title": "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis",
        "url": "http://arxiv.org/abs/2308.09713v1",
        "pub_date": "2023-08-18",
        "summary": "We present a method that simultaneously addresses the tasks of dynamic scene\nnovel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense\nscene elements. We follow an analysis-by-synthesis framework, inspired by\nrecent work that models scenes as a collection of 3D Gaussians which are\noptimized to reconstruct input images via differentiable rendering. To model\ndynamic scenes, we allow Gaussians to move and rotate over time while enforcing\nthat they have persistent color, opacity, and size. By regularizing Gaussians'\nmotion and rotation with local-rigidity constraints, we show that our Dynamic\n3D Gaussians correctly model the same area of physical space over time,\nincluding the rotation of that space. Dense 6-DOF tracking and dynamic\nreconstruction emerges naturally from persistent dynamic view synthesis,\nwithout requiring any correspondence or flow as input. We demonstrate a large\nnumber of downstream applications enabled by our representation, including\nfirst-person view synthesis, dynamic compositional scene synthesis, and 4D\nvideo editing.",
        "translated": ""
    },
    {
        "title": "HumanLiff: Layer-wise 3D Human Generation with Diffusion Model",
        "url": "http://arxiv.org/abs/2308.09712v1",
        "pub_date": "2023-08-18",
        "summary": "3D human generation from 2D images has achieved remarkable progress through\nthe synergistic utilization of neural rendering and generative models. Existing\n3D human generative models mainly generate a clothed 3D human as an\nundetectable 3D model in a single pass, while rarely considering the layer-wise\nnature of a clothed human body, which often consists of the human body and\nvarious clothes such as underwear, outerwear, trousers, shoes, etc. In this\nwork, we propose HumanLiff, the first layer-wise 3D human generative model with\na unified diffusion process. Specifically, HumanLiff firstly generates\nminimal-clothed humans, represented by tri-plane features, in a canonical\nspace, and then progressively generates clothes in a layer-wise manner. In this\nway, the 3D human generation is thus formulated as a sequence of\ndiffusion-based 3D conditional generation. To reconstruct more fine-grained 3D\nhumans with tri-plane representation, we propose a tri-plane shift operation\nthat splits each tri-plane into three sub-planes and shifts these sub-planes to\nenable feature grid subdivision. To further enhance the controllability of 3D\ngeneration with 3D layered conditions, HumanLiff hierarchically fuses tri-plane\nfeatures and 3D layered conditions to facilitate the 3D diffusion model\nlearning. Extensive experiments on two layer-wise 3D human datasets, SynBody\n(synthetic) and TightCap (real-world), validate that HumanLiff significantly\noutperforms state-of-the-art methods in layer-wise 3D human generation. Our\ncode will be available at https://skhu101.github.io/HumanLiff.",
        "translated": ""
    },
    {
        "title": "CamP: Camera Preconditioning for Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2308.10902v1",
        "pub_date": "2023-08-21",
        "summary": "Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D\nscene reconstructions of objects and large-scale scenes. However, NeRFs require\naccurate camera parameters as input -- inaccurate camera parameters result in\nblurry renderings. Extrinsic and intrinsic camera parameters are usually\nestimated using Structure-from-Motion (SfM) methods as a pre-processing step to\nNeRF, but these techniques rarely yield perfect estimates. Thus, prior works\nhave proposed jointly optimizing camera parameters alongside a NeRF, but these\nmethods are prone to local minima in challenging settings. In this work, we\nanalyze how different camera parameterizations affect this joint optimization\nproblem, and observe that standard parameterizations exhibit large differences\nin magnitude with respect to small perturbations, which can lead to an\nill-conditioned optimization problem. We propose using a proxy problem to\ncompute a whitening transform that eliminates the correlation between camera\nparameters and normalizes their effects, and we propose to use this transform\nas a preconditioner for the camera parameters during joint optimization. Our\npreconditioned camera optimization significantly improves reconstruction\nquality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)\nby 67% compared to state-of-the-art NeRF approaches that do not optimize for\ncameras like Zip-NeRF, and by 29% relative to state-of-the-art joint\noptimization approaches using the camera parameterization of SCNeRF. Our\napproach is easy to implement, does not significantly increase runtime, can be\napplied to a wide variety of camera parameterizations, and can\nstraightforwardly be incorporated into other NeRF-like models.",
        "translated": ""
    },
    {
        "title": "Structured World Models from Human Videos",
        "url": "http://arxiv.org/abs/2308.10901v1",
        "pub_date": "2023-08-21",
        "summary": "We tackle the problem of learning complex, general behaviors directly in the\nreal world. We propose an approach for robots to efficiently learn manipulation\nskills using only a handful of real-world interaction trajectories from many\ndifferent settings. Inspired by the success of learning from large-scale\ndatasets in the fields of computer vision and natural language, our belief is\nthat in order to efficiently learn, a robot must be able to leverage\ninternet-scale, human video data. Humans interact with the world in many\ninteresting ways, which can allow a robot to not only build an understanding of\nuseful actions and affordances but also how these actions affect the world for\nmanipulation. Our approach builds a structured, human-centric action space\ngrounded in visual affordances learned from human videos. Further, we train a\nworld model on human videos and fine-tune on a small amount of robot\ninteraction data without any task supervision. We show that this approach of\naffordance-space world models enables different robots to learn various\nmanipulation skills in complex settings, in under 30 minutes of interaction.\nVideos can be found at https://human-world-model.github.io",
        "translated": ""
    },
    {
        "title": "Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical\n  Deformation",
        "url": "http://arxiv.org/abs/2308.10898v1",
        "pub_date": "2023-08-21",
        "summary": "We study the problem of few-shot physically-aware articulated mesh\ngeneration. By observing an articulated object dataset containing only a few\nexamples, we wish to learn a model that can generate diverse meshes with high\nvisual fidelity and physical validity. Previous mesh generative models either\nhave difficulties in depicting a diverse data space from only a few examples or\nfail to ensure physical validity of their samples. Regarding the above\nchallenges, we propose two key innovations, including 1) a hierarchical mesh\ndeformation-based generative model based upon the divide-and-conquer philosophy\nto alleviate the few-shot challenge by borrowing transferrable deformation\npatterns from large scale rigid meshes and 2) a physics-aware deformation\ncorrection scheme to encourage physically plausible generations. We conduct\nextensive experiments on 6 articulated categories to demonstrate the\nsuperiority of our method in generating articulated meshes with better\ndiversity, higher visual fidelity, and better physical validity over previous\nmethods in the few-shot setting. Further, we validate solid contributions of\nour two innovations in the ablation study. Project page with code is available\nat https://meowuu7.github.io/few-arti-obj-gen.",
        "translated": ""
    },
    {
        "title": "Can Language Models Learn to Listen?",
        "url": "http://arxiv.org/abs/2308.10897v1",
        "pub_date": "2023-08-21",
        "summary": "We present a framework for generating appropriate facial responses from a\nlistener in dyadic social interactions based on the speaker's words. Given an\ninput transcription of the speaker's words with their timestamps, our approach\nautoregressively predicts a response of a listener: a sequence of listener\nfacial gestures, quantized using a VQ-VAE. Since gesture is a language\ncomponent, we propose treating the quantized atomic motion elements as\nadditional language token inputs to a transformer-based large language model.\nInitializing our transformer with the weights of a language model pre-trained\nonly on text results in significantly higher quality listener responses than\ntraining a transformer from scratch. We show that our generated listener motion\nis fluent and reflective of language semantics through quantitative metrics and\na qualitative user study. In our evaluation, we analyze the model's ability to\nutilize temporal and semantic aspects of spoken text. Project page:\nhttps://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/",
        "translated": ""
    },
    {
        "title": "Differentiable Shadow Mapping for Efficient Inverse Graphics",
        "url": "http://arxiv.org/abs/2308.10896v1",
        "pub_date": "2023-08-21",
        "summary": "We show how shadows can be efficiently generated in differentiable rendering\nof triangle meshes. Our central observation is that pre-filtered shadow\nmapping, a technique for approximating shadows based on rendering from the\nperspective of a light, can be combined with existing differentiable\nrasterizers to yield differentiable visibility information. We demonstrate at\nseveral inverse graphics problems that differentiable shadow maps are orders of\nmagnitude faster than differentiable light transport simulation with similar\naccuracy -- while differentiable rasterization without shadows often fails to\nconverge.",
        "translated": ""
    },
    {
        "title": "GRIP: Generating Interaction Poses Using Latent Consistency and Spatial\n  Cues",
        "url": "http://arxiv.org/abs/2308.11617v1",
        "pub_date": "2023-08-22",
        "summary": "Hands are dexterous and highly versatile manipulators that are central to how\nhumans interact with objects and their environment. Consequently, modeling\nrealistic hand-object interactions, including the subtle motion of individual\nfingers, is critical for applications in computer graphics, computer vision,\nand mixed reality. Prior work on capturing and modeling humans interacting with\nobjects in 3D focuses on the body and object motion, often ignoring hand pose.\nIn contrast, we introduce GRIP, a learning-based method that takes, as input,\nthe 3D motion of the body and the object, and synthesizes realistic motion for\nboth hands before, during, and after object interaction. As a preliminary step\nbefore synthesizing the hand motion, we first use a network, ANet, to denoise\nthe arm motion. Then, we leverage the spatio-temporal relationship between the\nbody and the object to extract two types of novel temporal interaction cues,\nand use them in a two-stage inference pipeline to generate the hand motion. In\nthe first stage, we introduce a new approach to enforce motion temporal\nconsistency in the latent space (LTC), and generate consistent interaction\nmotions. In the second stage, GRIP generates refined hand poses to avoid\nhand-object penetrations. Given sequences of noisy body and object motion, GRIP\nupgrades them to include hand-object interaction. Quantitative experiments and\nperceptual studies demonstrate that GRIP outperforms baseline methods and\ngeneralizes to unseen objects and motions from different motion-capture\ndatasets.",
        "translated": ""
    },
    {
        "title": "Delving into Motion-Aware Matching for Monocular 3D Object Tracking",
        "url": "http://arxiv.org/abs/2308.11607v1",
        "pub_date": "2023-08-22",
        "summary": "Recent advances of monocular 3D object detection facilitate the 3D\nmulti-object tracking task based on low-cost camera sensors. In this paper, we\nfind that the motion cue of objects along different time frames is critical in\n3D multi-object tracking, which is less explored in existing monocular-based\napproaches. In this paper, we propose a motion-aware framework for monocular 3D\nMOT. To this end, we propose MoMA-M3T, a framework that mainly consists of\nthree motion-aware components. First, we represent the possible movement of an\nobject related to all object tracklets in the feature space as its motion\nfeatures. Then, we further model the historical object tracklet along the time\nframe in a spatial-temporal perspective via a motion transformer. Finally, we\npropose a motion-aware matching module to associate historical object tracklets\nand current observations as final tracking results. We conduct extensive\nexperiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T\nachieves competitive performance against state-of-the-art methods. Moreover,\nthe proposed tracker is flexible and can be easily plugged into existing\nimage-based 3D object detectors without re-training. Code and models are\navailable at https://github.com/kuanchihhuang/MoMA-M3T.",
        "translated": ""
    },
    {
        "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
        "url": "http://arxiv.org/abs/2308.11606v1",
        "pub_date": "2023-08-22",
        "summary": "Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.",
        "translated": ""
    },
    {
        "title": "GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised\n  Learning",
        "url": "http://arxiv.org/abs/2308.11605v1",
        "pub_date": "2023-08-22",
        "summary": "Large-scale foundation models, such as CLIP, have demonstrated remarkable\nsuccess in visual recognition tasks by embedding images in a semantically rich\nspace. Self-supervised learning (SSL) has also shown promise in improving\nvisual recognition by learning invariant features. However, the combination of\nCLIP with SSL is found to face challenges due to the multi-task framework that\nblends CLIP's contrastive loss and SSL's loss, including difficulties with loss\nweighting and inconsistency among different views of images in CLIP's output\nspace. To overcome these challenges, we propose a prompt learning-based model\ncalled GOPro, which is a unified framework that ensures similarity between\nvarious augmented views of input images in a shared image-text embedding space,\nusing a pair of learnable image and text projectors atop CLIP, to promote\ninvariance and generalizability. To automatically learn such prompts, we\nleverage the visual content and style primitives extracted from pre-trained\nCLIP and adapt them to the target task. In addition to CLIP's cross-domain\ncontrastive loss, we introduce a visual contrastive loss and a novel prompt\nconsistency loss, considering the different views of the images. GOPro is\ntrained end-to-end on all three loss objectives, combining the strengths of\nCLIP and SSL in a principled manner. Empirical evaluations demonstrate that\nGOPro outperforms the state-of-the-art prompting techniques on three\nchallenging domain generalization tasks across multiple benchmarks by a\nsignificant margin. Our code is available at\nhttps://github.com/mainaksingha01/GOPro.",
        "translated": ""
    },
    {
        "title": "G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid\n  Model",
        "url": "http://arxiv.org/abs/2308.11573v1",
        "pub_date": "2023-08-22",
        "summary": "This study introduces a novel framework, G3Reg, for fast and robust global\nregistration of LiDAR point clouds. In contrast to conventional complex\nkeypoints and descriptors, we extract fundamental geometric primitives\nincluding planes, clusters, and lines (PCL) from the raw point cloud to obtain\nlow-level semantic segments. Each segment is formulated as a unified Gaussian\nEllipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground\ntruth centers are encompassed with a certain degree of probability. Utilizing\nthese GEMs, we then present a distrust-and-verify scheme based on a Pyramid\nCompatibility Graph for Global Registration (PAGOR). Specifically, we establish\nan upper bound, which can be traversed based on the confidence level for\ncompatibility testing to construct the pyramid graph. Gradually, we solve\nmultiple maximum cliques (MAC) for each level of the graph, generating numerous\ntransformation candidates. In the verification phase, we adopt a precise and\nefficient metric for point cloud alignment quality, founded on geometric\nprimitives, to identify the optimal candidate. The performance of the algorithm\nis extensively validated on three publicly available datasets and a\nself-collected multi-session dataset, without changing any parameter settings\nin the experimental evaluation. The results exhibit superior robustness and\nreal-time performance of the G3Reg framework compared to state-of-the-art\nmethods. Furthermore, we demonstrate the potential for integrating individual\nGEM and PAGOR components into other algorithmic frameworks to enhance their\nefficacy. To advance further research and promote community understanding, we\nhave publicly shared the source code.",
        "translated": ""
    },
    {
        "title": "CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from\n  Unbounded Synthesized Images",
        "url": "http://arxiv.org/abs/2308.12288v1",
        "pub_date": "2023-08-23",
        "summary": "We present a method for teaching machines to understand and model the\nunderlying spatial common sense of diverse human-object interactions in 3D in a\nself-supervised way. This is a challenging task, as there exist specific\nmanifolds of the interactions that can be considered human-like and natural,\nbut the human pose and the geometry of objects can vary even for similar\ninteractions. Such diversity makes the annotating task of 3D interactions\ndifficult and hard to scale, which limits the potential to reason about that in\na supervised way. One way of learning the 3D spatial relationship between\nhumans and objects during interaction is by showing multiple 2D images captured\nfrom different viewpoints when humans interact with the same type of objects.\nThe core idea of our method is to leverage a generative model that produces\nhigh-quality 2D images from an arbitrary text prompt input as an \"unbounded\"\ndata generator with effective controllability and view diversity. Despite its\nimperfection of the image quality over real images, we demonstrate that the\nsynthesized images are sufficient to learn the 3D human-object spatial\nrelations. We present multiple strategies to leverage the synthesized images,\nincluding (1) the first method to leverage a generative image model for 3D\nhuman-object spatial relation learning; (2) a framework to reason about the 3D\nspatial relations from inconsistent 2D cues in a self-supervised manner via 3D\noccupancy reasoning with pose canonicalization; (3) semantic clustering to\ndisambiguate different types of interactions with the same object types; and\n(4) a novel metric to assess the quality of 3D spatial learning of interaction.\nProject Page: https://jellyheadandrew.github.io/projects/chorus",
        "translated": ""
    },
    {
        "title": "A Generative Approach for Image Registration of Visible-Thermal (VT)\n  Cancer Faces",
        "url": "http://arxiv.org/abs/2308.12271v1",
        "pub_date": "2023-08-23",
        "summary": "Since thermal imagery offers a unique modality to investigate pain, the U.S.\nNational Institutes of Health (NIH) has collected a large and diverse set of\ncancer patient facial thermograms for AI-based pain research. However,\ndiffering angles from camera capture between thermal and visible sensors has\nled to misalignment between Visible-Thermal (VT) images. We modernize the\nclassic computer vision task of image registration by applying and modifying a\ngenerative alignment algorithm to register VT cancer faces, without the need\nfor a reference or alignment parameters. By registering VT faces, we\ndemonstrate that the quality of thermal images produced in the generative AI\ndownstream task of Visible-to-Thermal (V2T) image translation significantly\nimproves up to 52.5\\%, than without registration. Images in this paper have\nbeen approved by the NIH NCI for public dissemination.",
        "translated": ""
    },
    {
        "title": "MolGrapher: Graph-based Visual Recognition of Chemical Structures",
        "url": "http://arxiv.org/abs/2308.12234v1",
        "pub_date": "2023-08-23",
        "summary": "The automatic analysis of chemical literature has immense potential to\naccelerate the discovery of new materials and drugs. Much of the critical\ninformation in patent documents and scientific articles is contained in\nfigures, depicting the molecule structures. However, automatically parsing the\nexact chemical structure is a formidable challenge, due to the amount of\ndetailed information, the diversity of drawing styles, and the need for\ntraining data. In this work, we introduce MolGrapher to recognize chemical\nstructures visually. First, a deep keypoint detector detects the atoms. Second,\nwe treat all candidate atoms and bonds as nodes and put them in a graph. This\nconstruct allows a natural graph representation of the molecule. Last, we\nclassify atom and bond nodes in the graph with a Graph Neural Network. To\naddress the lack of real training data, we propose a synthetic data generation\npipeline producing diverse and realistic results. In addition, we introduce a\nlarge-scale benchmark of annotated real molecule images, USPTO-30K, to spur\nresearch on this critical topic. Extensive experiments on five datasets show\nthat our approach significantly outperforms classical and learning-based\nmethods in most settings. Code, models, and datasets are available.",
        "translated": ""
    },
    {
        "title": "SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation",
        "url": "http://arxiv.org/abs/2308.12231v1",
        "pub_date": "2023-08-23",
        "summary": "Image segmentation plays an essential role in nuclei image analysis.\nRecently, the segment anything model has made a significant breakthrough in\nsuch tasks. However, the current model exists two major issues for cell\nsegmentation: (1) the image encoder of the segment anything model involves a\nlarge number of parameters. Retraining or even fine-tuning the model still\nrequires expensive computational resources. (2) in point prompt mode, points\nare sampled from the center of the ground truth and more than one set of points\nis expected to achieve reliable performance, which is not efficient for\npractical applications. In this paper, a single-point prompt network is\nproposed for nuclei image segmentation, called SPPNet. We replace the original\nimage encoder with a lightweight vision transformer. Also, an effective\nconvolutional block is added in parallel to extract the low-level semantic\ninformation from the image and compensate for the performance degradation due\nto the small image encoder. We propose a new point-sampling method based on the\nGaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset.\nThe result demonstrated that SPPNet outperforms existing U-shape architectures\nand shows faster convergence in training. Compared to the segment anything\nmodel, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and\ncomputational cost. Particularly, only one set of points is required in both\nthe training and inference phases, which is more reasonable for clinical\napplications. The code for our work and more technical details can be found at\nhttps://github.com/xq141839/SPPNet.",
        "translated": ""
    },
    {
        "title": "CIParsing: Unifying Causality Properties into Multiple Human Parsing",
        "url": "http://arxiv.org/abs/2308.12218v1",
        "pub_date": "2023-08-23",
        "summary": "Existing methods of multiple human parsing (MHP) apply statistical models to\nacquire underlying associations between images and labeled body parts. However,\nacquired associations often contain many spurious correlations that degrade\nmodel generalization, leading statistical models to be vulnerable to visually\ncontextual variations in images (e.g., unseen image styles/external\ninterventions). To tackle this, we present a causality inspired parsing\nparadigm termed CIParsing, which follows fundamental causal principles\ninvolving two causal properties for human parsing (i.e., the causal diversity\nand the causal invariance). Specifically, we assume that an input image is\nconstructed by a mix of causal factors (the characteristics of body parts) and\nnon-causal factors (external contexts), where only the former ones cause the\ngeneration process of human parsing.Since causal/non-causal factors are\nunobservable, a human parser in proposed CIParsing is required to construct\nlatent representations of causal factors and learns to enforce representations\nto satisfy the causal properties. In this way, the human parser is able to rely\non causal factors w.r.t relevant evidence rather than non-causal factors w.r.t\nspurious correlations, thus alleviating model degradation and yielding improved\nparsing ability. Notably, the CIParsing is designed in a plug-and-play fashion\nand can be integrated into any existing MHP models. Extensive experiments\nconducted on two widely used benchmarks demonstrate the effectiveness and\ngeneralizability of our method.",
        "translated": ""
    },
    {
        "title": "ROAM: Robust and Object-aware Motion Generation using Neural Pose\n  Descriptors",
        "url": "http://arxiv.org/abs/2308.12969v1",
        "pub_date": "2023-08-24",
        "summary": "Existing automatic approaches for 3D virtual character motion synthesis\nsupporting scene interactions do not generalise well to new objects outside\ntraining distributions, even when trained on extensive motion capture datasets\nwith diverse objects and annotated interactions. This paper addresses this\nlimitation and shows that robustness and generalisation to novel scene objects\nin 3D object-aware character synthesis can be achieved by training a motion\nmodel with as few as one reference object. We leverage an implicit feature\nrepresentation trained on object-only datasets, which encodes an\nSE(3)-equivariant descriptor field around the object. Given an unseen object\nand a reference pose-object pair, we optimise for the object-aware pose that is\nclosest in the feature space to the reference pose. Finally, we use l-NSM,\ni.e., our motion generation model that is trained to seamlessly transition from\nlocomotion to object interaction with the proposed bidirectional pose blending\nscheme. Through comprehensive numerical comparisons to state-of-the-art methods\nand in a user study, we demonstrate substantial improvements in 3D virtual\ncharacter motion and interaction quality and robustness to scenarios with\nunseen objects. Our project page is available at\nhttps://vcai.mpi-inf.mpg.de/projects/ROAM/.",
        "translated": ""
    },
    {
        "title": "NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes",
        "url": "http://arxiv.org/abs/2308.12967v1",
        "pub_date": "2023-08-24",
        "summary": "Recent implicit neural representations have shown great results for novel\nview synthesis. However, existing methods require expensive per-scene\noptimization from many views hence limiting their application to real-world\nunbounded urban settings where the objects of interest or backgrounds are\nobserved from very few views. To mitigate this challenge, we introduce a new\napproach called NeO 360, Neural fields for sparse view synthesis of outdoor\nscenes. NeO 360 is a generalizable method that reconstructs 360{\\deg} scenes\nfrom a single or a few posed RGB images. The essence of our approach is in\ncapturing the distribution of complex real-world outdoor 3D scenes and using a\nhybrid image-conditional triplanar representation that can be queried from any\nworld point. Our representation combines the best of both voxel-based and\nbird's-eye-view (BEV) representations and is more effective and expressive than\neach. NeO 360's representation allows us to learn from a large collection of\nunbounded 3D scenes while offering generalizability to new views and novel\nscenes from as few as a single image during inference. We demonstrate our\napproach on the proposed challenging 360{\\deg} unbounded dataset, called NeRDS\n360, and show that NeO 360 outperforms state-of-the-art generalizable methods\nfor novel view synthesis while also offering editing and composition\ncapabilities. Project page:\nhttps://zubair-irshad.github.io/projects/neo360.html",
        "translated": ""
    },
    {
        "title": "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised\n  Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2308.12968v1",
        "pub_date": "2023-08-24",
        "summary": "Automatic high-quality rendering of anime scenes from complex real-world\nimages is of significant practical value. The challenges of this task lie in\nthe complexity of the scenes, the unique features of anime style, and the lack\nof high-quality datasets to bridge the domain gap. Despite promising attempts,\nprevious efforts are still incompetent in achieving satisfactory results with\nconsistent semantic preservation, evident stylization, and fine details. In\nthis study, we propose Scenimefy, a novel semi-supervised image-to-image\ntranslation framework that addresses these challenges. Our approach guides the\nlearning with structure-consistent pseudo paired data, simplifying the pure\nunsupervised setting. The pseudo data are derived uniquely from a\nsemantic-constrained StyleGAN leveraging rich model priors like CLIP. We\nfurther apply segmentation-guided data selection to obtain high-quality pseudo\nsupervision. A patch-wise contrastive style loss is introduced to improve\nstylization and fine details. Besides, we contribute a high-resolution anime\nscene dataset to facilitate future research. Our extensive experiments\ndemonstrate the superiority of our method over state-of-the-art baselines in\nterms of both perceptual quality and quantitative performance.",
        "translated": ""
    },
    {
        "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities",
        "url": "http://arxiv.org/abs/2308.12966v1",
        "pub_date": "2023-08-24",
        "summary": "We introduce the Qwen-VL series, a set of large-scale vision-language models\ndesigned to perceive and understand both text and images. Comprising Qwen-VL\nand Qwen-VL-Chat, these models exhibit remarkable performance in tasks like\nimage captioning, question answering, visual localization, and flexible\ninteraction. The evaluation covers a wide range of tasks including zero-shot\ncaptioning, visual or document visual question answering, and grounding. We\ndemonstrate the Qwen-VL outperforms existing Large Vision Language Models\n(LVLMs). We present their architecture, training, capabilities, and\nperformance, highlighting their contributions to advancing multimodal\nartificial intelligence. Code, demo and models are available at\nhttps://github.com/QwenLM/Qwen-VL.",
        "translated": ""
    },
    {
        "title": "POCO: 3D Pose and Shape Estimation with Confidence",
        "url": "http://arxiv.org/abs/2308.12965v1",
        "pub_date": "2023-08-24",
        "summary": "The regression of 3D Human Pose and Shape (HPS) from an image is becoming\nincreasingly accurate. This makes the results useful for downstream tasks like\nhuman action recognition or 3D graphics. Yet, no regressor is perfect, and\naccuracy can be affected by ambiguous image evidence or by poses and appearance\nthat are unseen during training. Most current HPS regressors, however, do not\nreport the confidence of their outputs, meaning that downstream tasks cannot\ndifferentiate accurate estimates from inaccurate ones. To address this, we\ndevelop POCO, a novel framework for training HPS regressors to estimate not\nonly a 3D human body, but also their confidence, in a single feed-forward pass.\nSpecifically, POCO estimates both the 3D body pose and a per-sample variance.\nThe key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing\nuncertainty that is highly correlated to pose reconstruction quality. The POCO\nframework can be applied to any HPS regressor and here we evaluate it by\nmodifying HMR, PARE, and CLIFF. In all cases, training the network to reason\nabout uncertainty helps it learn to more accurately estimate 3D pose. While\nthis was not our goal, the improvement is modest but consistent. Our main\nmotivation is to provide uncertainty estimates for downstream tasks; we\ndemonstrate this in two ways: (1) We use the confidence estimates to bootstrap\nHPS training. Given unlabelled image data, we take the confident estimates of a\nPOCO-trained regressor as pseudo ground truth. Retraining with this\nautomatically-curated data improves accuracy. (2) We exploit uncertainty in\nvideo pose estimation by automatically identifying uncertain frames (e.g. due\nto occlusion) and inpainting these from confident frames. Code and models will\nbe available for research at https://poco.is.tue.mpg.de.",
        "translated": ""
    },
    {
        "title": "Joint Modeling of Feature, Correspondence, and a Compressed Memory for\n  Video Object Segmentation",
        "url": "http://arxiv.org/abs/2308.13505v1",
        "pub_date": "2023-08-25",
        "summary": "Current prevailing Video Object Segmentation (VOS) methods usually perform\ndense matching between the current and reference frames after extracting their\nfeatures. One on hand, the decoupled modeling restricts the targets information\npropagation only at high-level feature space. On the other hand, the pixel-wise\nmatching leads to a lack of holistic understanding of the targets. To overcome\nthese issues, we propose a unified VOS framework, coined as JointFormer, for\njoint modeling the three elements of feature, correspondence, and a compressed\nmemory. The core design is the Joint Block, utilizing the flexibility of\nattention to simultaneously extract feature and propagate the targets\ninformation to the current tokens and the compressed memory token. This scheme\nallows to perform extensive information propagation and discriminative feature\nlearning. To incorporate the long-term temporal targets information, we also\ndevise a customized online updating mechanism for the compressed memory token,\nwhich can prompt the information flow along the temporal dimension and thus\nimprove the global modeling capability. Under the design, our method achieves a\nnew state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) and\nYouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existing\nworks by a large margin.",
        "translated": ""
    },
    {
        "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance",
        "url": "http://arxiv.org/abs/2308.13504v1",
        "pub_date": "2023-08-25",
        "summary": "We present accumulator-aware quantization (A2Q), a novel weight quantization\nmethod designed to train quantized neural networks (QNNs) to avoid overflow\nwhen using low-precision accumulators during inference. A2Q introduces a unique\nformulation inspired by weight normalization that constrains the L1-norm of\nmodel weights according to accumulator bit width bounds that we derive. Thus,\nin training QNNs for low-precision accumulation, A2Q also inherently promotes\nunstructured weight sparsity to guarantee overflow avoidance. We apply our\nmethod to deep learning-based computer vision tasks to show that A2Q can train\nQNNs for low-precision accumulators while maintaining model accuracy\ncompetitive with a floating-point baseline. In our evaluations, we consider the\nimpact of A2Q on both general-purpose platforms and programmable hardware.\nHowever, we primarily target model deployment on FPGAs because they can be\nprogrammed to fully exploit custom accumulator bit widths. Our experimentation\nshows accumulator bit width significantly impacts the resource efficiency of\nFPGA-based accelerators. On average across our benchmarks, A2Q offers up to a\n2.3x reduction in resource utilization over 32-bit accumulator counterparts\nwith 99.2% of the floating-point model accuracy.",
        "translated": ""
    },
    {
        "title": "Attending Generalizability in Course of Deep Fake Detection by Exploring\n  Multi-task Learning",
        "url": "http://arxiv.org/abs/2308.13503v1",
        "pub_date": "2023-08-25",
        "summary": "This work explores various ways of exploring multi-task learning (MTL)\ntechniques aimed at classifying videos as original or manipulated in\ncross-manipulation scenario to attend generalizability in deep fake scenario.\nThe dataset used in our evaluation is FaceForensics++, which features 1000\noriginal videos manipulated by four different techniques, with a total of 5000\nvideos. We conduct extensive experiments on multi-task learning and contrastive\ntechniques, which are well studied in literature for their generalization\nbenefits. It can be concluded that the proposed detection model is quite\ngeneralized, i.e., accurately detects manipulation methods not encountered\nduring training as compared to the state-of-the-art.",
        "translated": ""
    },
    {
        "title": "Open Gaze: An Open-Source Implementation Replicating Google's Eye\n  Tracking Paper",
        "url": "http://arxiv.org/abs/2308.13495v1",
        "pub_date": "2023-08-25",
        "summary": "Eye tracking has been a pivotal tool in diverse fields such as vision\nresearch, language analysis, and usability assessment. The majority of prior\ninvestigations, however, have concentrated on expansive desktop displays\nemploying specialized, costly eye tracking hardware that lacks scalability.\nRemarkably little insight exists into ocular movement patterns on smartphones,\ndespite their widespread adoption and significant usage. In this manuscript, we\npresent an open-source implementation of a smartphone-based gaze tracker that\nemulates the methodology proposed by a GooglePaper (whose source code remains\nproprietary). Our focus is on attaining accuracy comparable to that attained\nthrough the GooglePaper's methodology, without the necessity for supplementary\nhardware. Through the integration of machine learning techniques, we unveil an\naccurate eye tracking solution that is native to smartphones. Our approach\ndemonstrates precision akin to the state-of-the-art mobile eye trackers, which\nare characterized by a cost that is two orders of magnitude higher. Leveraging\nthe vast MIT GazeCapture dataset, which is available through registration on\nthe dataset's website, we successfully replicate crucial findings from previous\nstudies concerning ocular motion behavior in oculomotor tasks and saliency\nanalyses during natural image observation. Furthermore, we emphasize the\napplicability of smartphone-based gaze tracking in discerning reading\ncomprehension challenges. Our findings exhibit the inherent potential to\namplify eye movement research by significant proportions, accommodating\nparticipation from thousands of subjects with explicit consent. This\nscalability not only fosters advancements in vision research, but also extends\nits benefits to domains such as accessibility enhancement and healthcare\napplications.",
        "translated": ""
    },
    {
        "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision\n  Transformers",
        "url": "http://arxiv.org/abs/2308.13494v1",
        "pub_date": "2023-08-25",
        "summary": "Vision Transformers achieve impressive accuracy across a range of visual\nrecognition tasks. Unfortunately, their accuracy frequently comes with high\ncomputational costs. This is a particular issue in video recognition, where\nmodels are often applied repeatedly across frames or temporal chunks. In this\nwork, we exploit temporal redundancy between subsequent inputs to reduce the\ncost of Transformers for video processing. We describe a method for identifying\nand re-processing only those tokens that have changed significantly over time.\nOur proposed family of models, Eventful Transformers, can be converted from\nexisting Transformers (often without any re-training) and give adaptive control\nover the compute cost at runtime. We evaluate our method on large-scale\ndatasets for video object detection (ImageNet VID) and action recognition\n(EPIC-Kitchens 100). Our approach leads to significant computational savings\n(on the order of 2-4x) with only minor reductions in accuracy.",
        "translated": ""
    },
    {
        "title": "Efficient Discovery and Effective Evaluation of Visual Perceptual\n  Similarity: A Benchmark and Beyond",
        "url": "http://arxiv.org/abs/2308.14753v1",
        "pub_date": "2023-08-28",
        "summary": "Visual similarities discovery (VSD) is an important task with broad\ne-commerce applications. Given an image of a certain object, the goal of VSD is\nto retrieve images of different objects with high perceptual visual similarity.\nAlthough being a highly addressed problem, the evaluation of proposed methods\nfor VSD is often based on a proxy of an identification-retrieval task,\nevaluating the ability of a model to retrieve different images of the same\nobject. We posit that evaluating VSD methods based on identification tasks is\nlimited, and faithful evaluation must rely on expert annotations. In this\npaper, we introduce the first large-scale fashion visual similarity benchmark\ndataset, consisting of more than 110K expert-annotated image pairs. Besides\nthis major contribution, we share insight from the challenges we faced while\ncurating this dataset. Based on these insights, we propose a novel and\nefficient labeling procedure that can be applied to any dataset. Our analysis\nexamines its limitations and inductive biases, and based on these findings, we\npropose metrics to mitigate those limitations. Though our primary focus lies on\nvisual similarity, the methodologies we present have broader applications for\ndiscovering and evaluating perceptual similarity across various domains.",
        "translated": ""
    },
    {
        "title": "MagicEdit: High-Fidelity and Temporally Coherent Video Editing",
        "url": "http://arxiv.org/abs/2308.14749v1",
        "pub_date": "2023-08-28",
        "summary": "In this report, we present MagicEdit, a surprisingly simple yet effective\nsolution to the text-guided video editing task. We found that high-fidelity and\ntemporally coherent video-to-video translation can be achieved by explicitly\ndisentangling the learning of content, structure and motion signals during\ntraining. This is in contradict to most existing methods which attempt to\njointly model both the appearance and temporal representation within a single\nframework, which we argue, would lead to degradation in per-frame quality.\nDespite its simplicity, we show that MagicEdit supports various downstream\nvideo editing tasks, including video stylization, local editing, video-MagicMix\nand video outpainting.",
        "translated": ""
    },
    {
        "title": "MagicAvatar: Multimodal Avatar Generation and Animation",
        "url": "http://arxiv.org/abs/2308.14748v1",
        "pub_date": "2023-08-28",
        "summary": "This report presents MagicAvatar, a framework for multimodal video generation\nand animation of human avatars. Unlike most existing methods that generate\navatar-centric videos directly from multimodal inputs (e.g., text prompts),\nMagicAvatar explicitly disentangles avatar video generation into two stages:\n(1) multimodal-to-motion and (2) motion-to-video generation. The first stage\ntranslates the multimodal inputs into motion/ control signals (e.g., human\npose, depth, DensePose); while the second stage generates avatar-centric video\nguided by these motion signals. Additionally, MagicAvatar supports avatar\nanimation by simply providing a few images of the target person. This\ncapability enables the animation of the provided human identity according to\nthe specific motion derived from the first stage. We demonstrate the\nflexibility of MagicAvatar through various applications, including text-guided\nand video-guided avatar generation, as well as multimodal avatar animation.",
        "translated": ""
    },
    {
        "title": "CoVR: Learning Composed Video Retrieval from Web Video Captions",
        "url": "http://arxiv.org/abs/2308.14746v1",
        "pub_date": "2023-08-28",
        "summary": "Composed Image Retrieval (CoIR) has recently gained popularity as a task that\nconsiders both text and image queries together, to search for relevant images\nin a database. Most CoIR approaches require manually annotated datasets,\ncomprising image-text-image triplets, where the text describes a modification\nfrom the query image to the target image. However, manual curation of CoIR\ntriplets is expensive and prevents scalability. In this work, we instead\npropose a scalable automatic dataset creation methodology that generates\ntriplets given video-caption pairs, while also expanding the scope of the task\nto include composed video retrieval (CoVR). To this end, we mine paired videos\nwith a similar caption from a large database, and leverage a large language\nmodel to generate the corresponding modification text. Applying this\nmethodology to the extensive WebVid2M collection, we automatically construct\nour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we\nintroduce a new benchmark for CoVR with a manually annotated evaluation set,\nalong with baseline results. Our experiments further demonstrate that training\na CoVR model on our dataset effectively transfers to CoIR, leading to improved\nstate-of-the-art performance in the zero-shot setup on both the CIRR and\nFashionIQ benchmarks. Our code, datasets, and models are publicly available at\nhttps://imagine.enpc.fr/~ventural/covr.",
        "translated": ""
    },
    {
        "title": "Total Selfie: Generating Full-Body Selfies",
        "url": "http://arxiv.org/abs/2308.14740v1",
        "pub_date": "2023-08-28",
        "summary": "We present a method to generate full-body selfies -- photos that you take of\nyourself, but capturing your whole body as if someone else took the photo of\nyou from a few feet away. Our approach takes as input a pre-captured video of\nyour body, a target pose photo, and a selfie + background pair for each\nlocation. We introduce a novel diffusion-based approach to combine all of this\ninformation into high quality, well-composed photos of you with the desired\npose and background.",
        "translated": ""
    },
    {
        "title": "3D Adversarial Augmentations for Robust Out-of-Domain Predictions",
        "url": "http://arxiv.org/abs/2308.15479v1",
        "pub_date": "2023-08-29",
        "summary": "Since real-world training datasets cannot properly sample the long tail of\nthe underlying data distribution, corner cases and rare out-of-domain samples\ncan severely hinder the performance of state-of-the-art models. This problem\nbecomes even more severe for dense tasks, such as 3D semantic segmentation,\nwhere points of non-standard objects can be confidently associated to the wrong\nclass. In this work, we focus on improving the generalization to out-of-domain\ndata. We achieve this by augmenting the training set with adversarial examples.\nFirst, we learn a set of vectors that deform the objects in an adversarial\nfashion. To prevent the adversarial examples from being too far from the\nexisting data distribution, we preserve their plausibility through a series of\nconstraints, ensuring sensor-awareness and shapes smoothness. Then, we perform\nadversarial augmentation by applying the learned sample-independent vectors to\nthe available objects when training a model. We conduct extensive experiments\nacross a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D\nobject detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D\nsemantic segmentation. Despite training on a standard single dataset, our\napproach substantially improves the robustness and generalization of both 3D\nobject detection and 3D semantic segmentation methods to out-of-domain data.",
        "translated": ""
    },
    {
        "title": "An Adaptive Tangent Feature Perspective of Neural Networks",
        "url": "http://arxiv.org/abs/2308.15478v1",
        "pub_date": "2023-08-29",
        "summary": "In order to better understand feature learning in neural networks, we propose\na framework for understanding linear models in tangent feature space where the\nfeatures are allowed to be transformed during training. We consider linear\ntransformations of features, resulting in a joint optimization over parameters\nand transformations with a bilinear interpolation constraint. We show that this\noptimization problem has an equivalent linearly constrained optimization with\nstructured regularization that encourages approximately low rank solutions.\nSpecializing to neural network structure, we gain insights into how the\nfeatures and thus the kernel function change, providing additional nuance to\nthe phenomenon of kernel alignment when the target function is poorly\nrepresented using tangent features. In addition to verifying our theoretical\nobservations in real neural networks on a simple regression problem, we\nempirically show that an adaptive feature implementation of tangent feature\nclassification has an order of magnitude lower sample complexity than the fixed\ntangent feature model on MNIST and CIFAR-10.",
        "translated": ""
    },
    {
        "title": "A General-Purpose Self-Supervised Model for Computational Pathology",
        "url": "http://arxiv.org/abs/2308.15474v1",
        "pub_date": "2023-08-29",
        "summary": "Tissue phenotyping is a fundamental computational pathology (CPath) task in\nlearning objective characterizations of histopathologic biomarkers in anatomic\npathology. However, whole-slide imaging (WSI) poses a complex computer vision\nproblem in which the large-scale image resolutions of WSIs and the enormous\ndiversity of morphological phenotypes preclude large-scale data annotation.\nCurrent efforts have proposed using pretrained image encoders with either\ntransfer learning from natural image datasets or self-supervised pretraining on\npublicly-available histopathology datasets, but have not been extensively\ndeveloped and evaluated across diverse tissue types at scale. We introduce UNI,\na general-purpose self-supervised model for pathology, pretrained using over\n100 million tissue patches from over 100,000 diagnostic haematoxylin and\neosin-stained WSIs across 20 major tissue types, and evaluated on 33\nrepresentative CPath clinical tasks in CPath of varying diagnostic\ndifficulties. In addition to outperforming previous state-of-the-art models, we\ndemonstrate new modeling capabilities in CPath such as resolution-agnostic\ntissue classification, slide classification using few-shot class prototypes,\nand disease subtyping generalization in classifying up to 108 cancer types in\nthe OncoTree code classification system. UNI advances unsupervised\nrepresentation learning at scale in CPath in terms of both pretraining data and\ndownstream evaluation, enabling data-efficient AI models that can generalize\nand transfer to a gamut of diagnostically-challenging tasks and clinical\nworkflows in anatomic pathology.",
        "translated": ""
    },
    {
        "title": "Learning Modulated Transformation in GANs",
        "url": "http://arxiv.org/abs/2308.15472v1",
        "pub_date": "2023-08-29",
        "summary": "The success of style-based generators largely benefits from style modulation,\nwhich helps take care of the cross-instance variation within data. However, the\ninstance-wise stochasticity is typically introduced via regular convolution,\nwhere kernels interact with features at some fixed locations, limiting its\ncapacity for modeling geometric variation. To alleviate this problem, we equip\nthe generator in generative adversarial networks (GANs) with a plug-and-play\nmodule, termed as modulated transformation module (MTM). This module predicts\nspatial offsets under the control of latent codes, based on which the\nconvolution operation can be applied at variable locations for different\ninstances, and hence offers the model an additional degree of freedom to handle\ngeometry deformation. Extensive experiments suggest that our approach can be\nfaithfully generalized to various generative tasks, including image generation,\n3D-aware image synthesis, and video generation, and get compatible with\nstate-of-the-art frameworks without any hyper-parameter tuning. It is\nnoteworthy that, towards human generation on the challenging TaiChi dataset, we\nimprove the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of\nlearning modulated geometry transformation.",
        "translated": ""
    },
    {
        "title": "Multimodal Contrastive Learning and Tabular Attention for Automated\n  Alzheimer's Disease Prediction",
        "url": "http://arxiv.org/abs/2308.15469v1",
        "pub_date": "2023-08-29",
        "summary": "Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)\ndatasets contain valuable tabular data including AD biomarkers and clinical\nassessments. Existing computer vision approaches struggle to utilize this\nadditional information. To address these needs, we propose a generalizable\nframework for multimodal contrastive learning of image data and tabular data, a\nnovel tabular attention module for amplifying and ranking salient features in\ntables, and the application of these techniques onto Alzheimer's disease\nprediction. Experimental evaulations demonstrate the strength of our framework\nby detecting Alzheimer's disease (AD) from over 882 MR image slices from the\nADNI database. We take advantage of the high interpretability of tabular data\nand our novel tabular attention approach and through attribution of the\nattention scores for each row of the table, we note and rank the most\npredominant features. Results show that the model is capable of an accuracy of\nover 83.8%, almost a 10% increase from previous state of the art.",
        "translated": ""
    },
    {
        "title": "Boosting Detection in Crowd Analysis via Underutilized Output Features",
        "url": "http://arxiv.org/abs/2308.16187v1",
        "pub_date": "2023-08-30",
        "summary": "Detection-based methods have been viewed unfavorably in crowd analysis due to\ntheir poor performance in dense crowds. However, we argue that the potential of\nthese methods has been underestimated, as they offer crucial information for\ncrowd analysis that is often ignored. Specifically, the area size and\nconfidence score of output proposals and bounding boxes provide insight into\nthe scale and density of the crowd. To leverage these underutilized features,\nwe propose Crowd Hat, a plug-and-play module that can be easily integrated with\nexisting detection models. This module uses a mixed 2D-1D compression technique\nto refine the output features and obtain the spatial and numerical distribution\nof crowd-specific information. Based on these features, we further propose\nregion-adaptive NMS thresholds and a decouple-then-align paradigm that address\nthe major limitations of detection-based methods. Our extensive evaluations on\nvarious crowd analysis tasks, including crowd counting, localization, and\ndetection, demonstrate the effectiveness of utilizing output features and the\npotential of detection-based methods in crowd analysis.",
        "translated": ""
    },
    {
        "title": "SAM-Med2D",
        "url": "http://arxiv.org/abs/2308.16184v1",
        "pub_date": "2023-08-30",
        "summary": "The Segment Anything Model (SAM) represents a state-of-the-art research\nadvancement in natural image segmentation, achieving impressive results with\ninput prompts such as points and bounding boxes. However, our evaluation and\nrecent research indicate that directly applying the pretrained SAM to medical\nimage segmentation does not yield satisfactory performance. This limitation\nprimarily arises from significant domain gap between natural images and medical\nimages. To bridge this gap, we introduce SAM-Med2D, the most comprehensive\nstudies on applying SAM to medical 2D images. Specifically, we first collect\nand curate approximately 4.6M images and 19.7M masks from public and private\ndatasets, constructing a large-scale medical image segmentation dataset\nencompassing various modalities and objects. Then, we comprehensively fine-tune\nSAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that\nonly adopt bounding box or point prompts as interactive segmentation approach,\nwe adapt SAM to medical image segmentation through more comprehensive prompts\ninvolving bounding boxes, points, and masks. We additionally fine-tune the\nencoder and decoder of the original SAM to obtain a well-performed SAM-Med2D,\nleading to the most comprehensive fine-tuning strategies to date. Finally, we\nconducted a comprehensive evaluation and analysis to investigate the\nperformance of SAM-Med2D in medical image segmentation across various\nmodalities, anatomical structures, and organs. Concurrently, we validated the\ngeneralization capability of SAM-Med2D on 9 datasets from MICCAI 2023\nchallenge. Overall, our approach demonstrated significantly superior\nperformance and generalization capability compared to SAM.",
        "translated": ""
    },
    {
        "title": "GREC: Generalized Referring Expression Comprehension",
        "url": "http://arxiv.org/abs/2308.16182v1",
        "pub_date": "2023-08-30",
        "summary": "The objective of Classic Referring Expression Comprehension (REC) is to\nproduce a bounding box corresponding to the object mentioned in a given textual\ndescription. Commonly, existing datasets and techniques in classic REC are\ntailored for expressions that pertain to a single target, meaning a sole\nexpression is linked to one specific object. Expressions that refer to multiple\ntargets or involve no specific target have not been taken into account. This\nconstraint hinders the practical applicability of REC. This study introduces a\nnew benchmark termed as Generalized Referring Expression Comprehension (GREC).\nThis benchmark extends the classic REC by permitting expressions to describe\nany number of target objects. To achieve this goal, we have built the first\nlarge-scale GREC dataset named gRefCOCO. This dataset encompasses a range of\nexpressions: those referring to multiple targets, expressions with no specific\ntarget, and the single-target expressions. The design of GREC and gRefCOCO\nensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a\nGREC method implementation code, and GREC evaluation code are available at\nhttps://github.com/henghuiding/gRefCOCO.",
        "translated": ""
    },
    {
        "title": "MMVP: Motion-Matrix-based Video Prediction",
        "url": "http://arxiv.org/abs/2308.16154v1",
        "pub_date": "2023-08-30",
        "summary": "A central challenge of video prediction lies where the system has to reason\nthe objects' future motions from image frames while simultaneously maintaining\nthe consistency of their appearances across frames. This work introduces an\nend-to-end trainable two-stream video prediction framework, Motion-Matrix-based\nVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods that\nusually handle motion prediction and appearance maintenance within the same set\nof modules, MMVP decouples motion and appearance information by constructing\nappearance-agnostic motion matrices. The motion matrices represent the temporal\nsimilarity of each and every pair of feature patches in the input frames, and\nare the sole input of the motion prediction module in MMVP. This design\nimproves video prediction in both accuracy and efficiency, and reduces the\nmodel size. Results of extensive experiments demonstrate that MMVP outperforms\nstate-of-the-art systems on public data sets by non-negligible large margins\n(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the\nsize or smaller). Please refer to\nhttps://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the\nofficial code and the datasets used in this paper.",
        "translated": ""
    },
    {
        "title": "Modality Cycles with Masked Conditional Diffusion for Unsupervised\n  Anomaly Segmentation in MRI",
        "url": "http://arxiv.org/abs/2308.16150v1",
        "pub_date": "2023-08-30",
        "summary": "Unsupervised anomaly segmentation aims to detect patterns that are distinct\nfrom any patterns processed during training, commonly called abnormal or\nout-of-distribution patterns, without providing any associated manual\nsegmentations. Since anomalies during deployment can lead to model failure,\ndetecting the anomaly can enhance the reliability of models, which is valuable\nin high-risk domains like medical imaging. This paper introduces Masked\nModality Cycles with Conditional Diffusion (MMCCD), a method that enables\nsegmentation of anomalies across diverse patterns in multimodal MRI. The method\nis based on two fundamental ideas. First, we propose the use of cyclic modality\ntranslation as a mechanism for enabling abnormality detection.\nImage-translation models learn tissue-specific modality mappings, which are\ncharacteristic of tissue physiology. Thus, these learned mappings fail to\ntranslate tissues or image patterns that have never been encountered during\ntraining, and the error enables their segmentation. Furthermore, we combine\nimage translation with a masked conditional diffusion model, which attempts to\n`imagine' what tissue exists under a masked area, further exposing unknown\npatterns as the generative model fails to recreate them. We evaluate our method\non a proxy task by training on healthy-looking slices of BraTS2021\nmulti-modality MRIs and testing on slices with tumors. We show that our method\ncompares favorably to previous unsupervised approaches based on image\nreconstruction and denoising with autoencoders and diffusion models.",
        "translated": ""
    },
    {
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
        "url": "http://arxiv.org/abs/2308.16911v1",
        "pub_date": "2023-08-31",
        "summary": "The unprecedented advancements in Large Language Models (LLMs) have created a\nprofound impact on natural language processing but are yet to fully embrace the\nrealm of 3D understanding. This paper introduces PointLLM, a preliminary effort\nto fill this gap, thereby enabling LLMs to understand point clouds and offering\na new avenue beyond 2D visual data. PointLLM processes colored object point\nclouds with human instructions and generates contextually appropriate\nresponses, illustrating its grasp of point clouds and common sense.\nSpecifically, it leverages a point cloud encoder with a powerful LLM to\neffectively fuse geometric, appearance, and linguistic information. We collect\na novel dataset comprising 660K simple and 70K complex point-text instruction\npairs to enable a two-stage training strategy: initially aligning latent spaces\nand subsequently instruction-tuning the unified model. To rigorously evaluate\nour model's perceptual abilities and its generalization capabilities, we\nestablish two benchmarks: Generative 3D Object Classification and 3D Object\nCaptioning, assessed through three different methods, including human\nevaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment\nresults show that PointLLM demonstrates superior performance over existing 2D\nbaselines. Remarkably, in human-evaluated object captioning tasks, PointLLM\noutperforms human annotators in over 50% of the samples. Codes, datasets, and\nbenchmarks are available at https://github.com/OpenRobotLab/PointLLM .",
        "translated": ""
    },
    {
        "title": "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional\n  Video Generation",
        "url": "http://arxiv.org/abs/2308.16909v1",
        "pub_date": "2023-08-31",
        "summary": "Unconditional video generation is a challenging task that involves\nsynthesizing high-quality videos that are both coherent and of extended\nduration. To address this challenge, researchers have used pretrained StyleGAN\nimage generators for high-quality frame synthesis and focused on motion\ngenerator design. The motion generator is trained in an autoregressive manner\nusing heavy 3D convolutional discriminators to ensure motion coherence during\nvideo generation. In this paper, we introduce a novel motion generator design\nthat uses a learning-based inversion network for GAN. The encoder in our method\ncaptures rich and smooth priors from encoding images to latents, and given the\nlatent of an initially generated frame as guidance, our method can generate\nsmooth future latent by modulating the inversion encoder temporally. Our method\nenjoys the advantage of sparse training and naturally constrains the generation\nspace of our motion generator with the inversion network guided by the initial\nframe, eliminating the need for heavy discriminators. Moreover, our method\nsupports style transfer with simple fine-tuning when the encoder is paired with\na pretrained StyleGAN generator. Extensive experiments conducted on various\nbenchmarks demonstrate the superiority of our method in generating long and\nhigh-resolution videos with decent single-frame quality and temporal\nconsistency.",
        "translated": ""
    },
    {
        "title": "Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware\n  Homography Estimator",
        "url": "http://arxiv.org/abs/2308.16906v1",
        "pub_date": "2023-08-31",
        "summary": "In this paper, we introduce a novel approach to fine-grained cross-view\ngeo-localization. Our method aligns a warped ground image with a corresponding\nGPS-tagged satellite image covering the same area using homography estimation.\nWe first employ a differentiable spherical transform, adhering to geometric\nprinciples, to accurately align the perspective of the ground image with the\nsatellite map. This transformation effectively places ground and aerial images\nin the same view and on the same plane, reducing the task to an image alignment\nproblem. To address challenges such as occlusion, small overlapping range, and\nseasonal variations, we propose a robust correlation-aware homography estimator\nto align similar parts of the transformed ground image with the satellite\nimage. Our method achieves sub-pixel resolution and meter-level GPS accuracy by\nmapping the center point of the transformed ground image to the satellite image\nusing a homography matrix and determining the orientation of the ground camera\nusing a point above the central axis. Operating at a speed of 30 FPS, our\nmethod outperforms state-of-the-art techniques, reducing the mean metric\nlocalization error by 21.3% and 32.4% in same-area and cross-area\ngeneralization tasks on the VIGOR benchmark, respectively, and by 34.4% on the\nKITTI benchmark in same-area evaluation.",
        "translated": ""
    },
    {
        "title": "InterDiff: Generating 3D Human-Object Interactions with Physics-Informed\n  Diffusion",
        "url": "http://arxiv.org/abs/2308.16905v1",
        "pub_date": "2023-08-31",
        "summary": "This paper addresses a novel task of anticipating 3D human-object\ninteractions (HOIs). Most existing research on HOI synthesis lacks\ncomprehensive whole-body interactions with dynamic objects, e.g., often limited\nto manipulating small or static objects. Our task is significantly more\nchallenging, as it requires modeling dynamic objects with various shapes,\ncapturing whole-body motion, and ensuring physically valid interactions. To\nthis end, we propose InterDiff, a framework comprising two key steps: (i)\ninteraction diffusion, where we leverage a diffusion model to encode the\ndistribution of future human-object interactions; (ii) interaction correction,\nwhere we introduce a physics-informed predictor to correct denoised HOIs in a\ndiffusion step. Our key insight is to inject prior knowledge that the\ninteractions under reference with respect to contact points follow a simple\npattern and are easily predictable. Experiments on multiple human-object\ninteraction datasets demonstrate the effectiveness of our method for this task,\ncapable of producing realistic, vivid, and remarkably long-term 3D HOI\npredictions.",
        "translated": ""
    },
    {
        "title": "PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic\n  Occupancy Prediction",
        "url": "http://arxiv.org/abs/2308.16896v1",
        "pub_date": "2023-08-31",
        "summary": "Semantic segmentation in autonomous driving has been undergoing an evolution\nfrom sparse point segmentation to dense voxel segmentation, where the objective\nis to predict the semantic occupancy of each voxel in the concerned 3D space.\nThe dense nature of the prediction space has rendered existing efficient\n2D-projection-based methods (e.g., bird's eye view, range view, etc.)\nineffective, as they can only describe a subspace of the 3D scene. To address\nthis, we propose a cylindrical tri-perspective view to represent point clouds\neffectively and comprehensively and a PointOcc model to process them\nefficiently. Considering the distance distribution of LiDAR point clouds, we\nconstruct the tri-perspective view in the cylindrical coordinate system for\nmore fine-grained modeling of nearer areas. We employ spatial group pooling to\nmaintain structural details during projection and adopt 2D backbones to\nefficiently process each TPV plane. Finally, we obtain the features of each\npoint by aggregating its projected features on each of the processed TPV planes\nwithout the need for any post-processing. Extensive experiments on both 3D\noccupancy prediction and LiDAR segmentation benchmarks demonstrate that the\nproposed PointOcc achieves state-of-the-art performance with much faster speed.\nSpecifically, despite only using LiDAR, PointOcc significantly outperforms all\nother methods, including multi-modal methods, with a large margin on the\nOpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.",
        "translated": ""
    },
    {
        "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.00616v1",
        "pub_date": "2023-09-01",
        "summary": "Current 3D open-vocabulary scene understanding methods mostly utilize\nwell-aligned 2D images as the bridge to learn 3D features with language.\nHowever, applying these approaches becomes challenging in scenarios where 2D\nimages are absent. In this work, we introduce a completely new pipeline,\nnamely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary\nscene understanding at the instance level. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds. The \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision language models to extract\ninteresting objects. The \"Lookup\" module searches through the outcomes of\n\"Snap\" with the help of Mask2Pixel maps, which contain the precise\ncorrespondence between 3D masks and synthetic images, to assign category names\nto the proposed masks. This 2D input-free, easy-to-train, and flexible approach\nachieved state-of-the-art results on a wide range of indoor and outdoor\ndatasets with a large margin. Furthermore, OpenIns3D allows for effortless\nswitching of 2D detectors without re-training. When integrated with\nstate-of-the-art 2D open-world models such as ODISE and GroundingDINO, superb\nresults are observed on open-vocabulary instance segmentation. When integrated\nwith LLM-powered 2D models like LISA, it demonstrates a remarkable capacity to\nprocess highly complex text queries, including those that require intricate\nreasoning and world knowledge. The code and model will be made publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D\n  Understanding, Generation, and Instruction Following",
        "url": "http://arxiv.org/abs/2309.00615v1",
        "pub_date": "2023-09-01",
        "summary": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with\n2D image, language, audio, and video. Guided by ImageBind, we construct a joint\nembedding space between 3D and multi-modalities, enabling many promising\napplications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D\nopen-world understanding. On top of this, we further present Point-LLM, the\nfirst 3D large language model (LLM) following 3D multi-modal instructions. By\nparameter-efficient fine-tuning techniques, Point-LLM injects the semantics of\nPoint-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction\ndata, but exhibits superior 3D and multi-modal question-answering capacity. We\nhope our work may cast a light on the community for extending 3D point clouds\nto multi-modality applications. Code is available at\nhttps://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",
        "translated": ""
    },
    {
        "title": "Iterative Multi-granular Image Editing using Diffusion Models",
        "url": "http://arxiv.org/abs/2309.00613v1",
        "pub_date": "2023-09-01",
        "summary": "Recent advances in text-guided image synthesis has dramatically changed how\ncreative professionals generate artistic and aesthetically pleasing visual\nassets. To fully support such creative endeavors, the process should possess\nthe ability to: 1) iteratively edit the generations and 2) control the spatial\nreach of desired changes (global, local or anything in between). We formalize\nthis pragmatic problem setting as Iterative Multi-granular Editing. While there\nhas been substantial progress with diffusion-based models for image synthesis\nand editing, they are all one shot (i.e., no iterative editing capabilities)\nand do not naturally yield multi-granular control (i.e., covering the full\nspectrum of local-to-global edits). To overcome these drawbacks, we propose\nEMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent\niteration strategy, which re-purposes a pre-trained diffusion model to\nfacilitate iterative editing. This is complemented by a gradient control\noperation for multi-granular control. We introduce a new benchmark dataset to\nevaluate our newly proposed setting. We conduct exhaustive quantitatively and\nqualitatively evaluation against recent state-of-the-art approaches adapted to\nour task, to being out the mettle of EMILIE. We hope our work would attract\nattention to this newly identified, pragmatic problem setting.",
        "translated": ""
    },
    {
        "title": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities",
        "url": "http://arxiv.org/abs/2309.00610v1",
        "pub_date": "2023-09-01",
        "summary": "In recent years, extensive research has focused on 3D natural scene\ngeneration, but the domain of 3D city generation has not received as much\nexploration. This is due to the greater challenges posed by 3D city generation,\nmainly because humans are more sensitive to structural distortions in urban\nenvironments. Additionally, generating 3D cities is more complex than 3D\nnatural scenes since buildings, as objects of the same class, exhibit a wider\nrange of appearances compared to the relatively consistent appearance of\nobjects like trees in natural scenes. To address these challenges, we propose\nCityDreamer, a compositional generative model designed specifically for\nunbounded 3D cities, which separates the generation of building instances from\nother background objects, such as roads, green lands, and water areas, into\ndistinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth,\ncontaining a vast amount of real-world city imagery to enhance the realism of\nthe generated 3D cities both in their layouts and appearances. Through\nextensive experiments, CityDreamer has proven its superiority over\nstate-of-the-art methods in generating a wide range of lifelike 3D cities.",
        "translated": ""
    },
    {
        "title": "Time Series Analysis of Urban Liveability",
        "url": "http://arxiv.org/abs/2309.00594v1",
        "pub_date": "2023-09-01",
        "summary": "In this paper we explore deep learning models to monitor longitudinal\nliveability changes in Dutch cities at the neighbourhood level. Our liveability\nreference data is defined by a country-wise yearly survey based on a set of\nindicators combined into a liveability score, the Leefbaarometer. We pair this\nreference data with yearly-available high-resolution aerial images, which\ncreates yearly timesteps at which liveability can be monitored. We deploy a\nconvolutional neural network trained on an aerial image from 2016 and the\nLeefbaarometer score to predict liveability at new timesteps 2012 and 2020. The\nresults in a city used for training (Amsterdam) and one never seen during\ntraining (Eindhoven) show some trends which are difficult to interpret,\nespecially in light of the differences in image acquisitions at the different\ntime steps. This demonstrates the complexity of liveability monitoring across\ntime periods and the necessity for more sophisticated methods compensating for\nchanges unrelated to liveability dynamics.",
        "translated": ""
    },
    {
        "title": "GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction",
        "url": "http://arxiv.org/abs/2309.02436v1",
        "pub_date": "2023-09-05",
        "summary": "Neural implicit representations have recently demonstrated compelling results\non dense Simultaneous Localization And Mapping (SLAM) but suffer from the\naccumulation of errors in camera tracking and distortion in the reconstruction.\nPurposely, we present GO-SLAM, a deep-learning-based dense visual SLAM\nframework globally optimizing poses and 3D reconstruction in real-time. Robust\npose estimation is at its core, supported by efficient loop closing and online\nfull bundle adjustment, which optimize per frame by utilizing the learned\nglobal geometry of the complete history of input frames. Simultaneously, we\nupdate the implicit and continuous surface representation on-the-fly to ensure\nglobal consistency of 3D reconstruction. Results on various synthetic and\nreal-world datasets demonstrate that GO-SLAM outperforms state-of-the-art\napproaches at tracking robustness and reconstruction accuracy. Furthermore,\nGO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.",
        "translated": ""
    },
    {
        "title": "Efficient RL via Disentangled Environment and Agent Representations",
        "url": "http://arxiv.org/abs/2309.02435v1",
        "pub_date": "2023-09-05",
        "summary": "Agents that are aware of the separation between themselves and their\nenvironments can leverage this understanding to form effective representations\nof visual input. We propose an approach for learning such structured\nrepresentations for RL algorithms, using visual knowledge of the agent, such as\nits shape or mask, which is often inexpensive to obtain. This is incorporated\ninto the RL objective using a simple auxiliary loss. We show that our method,\nStructured Environment-Agent Representations, outperforms state-of-the-art\nmodel-free approaches over 18 different challenging visual simulation\nenvironments spanning 5 different robots. Website at https://sear-rl.github.io/",
        "translated": ""
    },
    {
        "title": "ReliTalk: Relightable Talking Portrait Generation from a Single Video",
        "url": "http://arxiv.org/abs/2309.02434v1",
        "pub_date": "2023-09-05",
        "summary": "Recent years have witnessed great progress in creating vivid audio-driven\nportraits from monocular videos. However, how to seamlessly adapt the created\nvideo avatars to other scenarios with different backgrounds and lighting\nconditions remains unsolved. On the other hand, existing relighting studies\nmostly rely on dynamically lighted or multi-view data, which are too expensive\nfor creating video portraits. To bridge this gap, we propose ReliTalk, a novel\nframework for relightable audio-driven talking portrait generation from\nmonocular videos. Our key insight is to decompose the portrait's reflectance\nfrom implicitly learned audio-driven facial normals and images. Specifically,\nwe involve 3D facial priors derived from audio features to predict delicate\nnormal maps through implicit functions. These initially predicted normals then\ntake a crucial part in reflectance decomposition by dynamically estimating the\nlighting condition of the given video. Moreover, the stereoscopic face\nrepresentation is refined using the identity-consistent loss under simulated\nmultiple lighting conditions, addressing the ill-posed problem caused by\nlimited views available from a single monocular video. Extensive experiments\nvalidate the superiority of our proposed framework on both real and synthetic\ndatasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.",
        "translated": ""
    },
    {
        "title": "Building a Winning Team: Selecting Source Model Ensembles using a\n  Submodular Transferability Estimation Approach",
        "url": "http://arxiv.org/abs/2309.02429v1",
        "pub_date": "2023-09-05",
        "summary": "Estimating the transferability of publicly available pretrained models to a\ntarget task has assumed an important place for transfer learning tasks in\nrecent years. Existing efforts propose metrics that allow a user to choose one\nmodel from a pool of pre-trained models without having to fine-tune each model\nindividually and identify one explicitly. With the growth in the number of\navailable pre-trained models and the popularity of model ensembles, it also\nbecomes essential to study the transferability of multiple-source models for a\ngiven target task. The few existing efforts study transferability in such\nmulti-source ensemble settings using just the outputs of the classification\nlayer and neglect possible domain or task mismatch. Moreover, they overlook the\nmost important factor while selecting the source models, viz., the cohesiveness\nfactor between them, which can impact the performance and confidence in the\nprediction of the ensemble. To address these gaps, we propose a novel Optimal\ntranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the\ntransferability of an ensemble of models to a downstream task. OSBORN\ncollectively accounts for image domain difference, task difference, and\ncohesiveness of models in the ensemble to provide reliable estimates of\ntransferability. We gauge the performance of OSBORN on both image\nclassification and semantic segmentation tasks. Our setup includes 28 source\ndatasets, 11 target datasets, 5 model architectures, and 2 pre-training\nmethods. We benchmark our method against current state-of-the-art metrics\nMS-LEEP and E-LEEP, and outperform them consistently using the proposed\napproach.",
        "translated": ""
    },
    {
        "title": "EgoPCA: A New Framework for Egocentric Hand-Object Interaction\n  Understanding",
        "url": "http://arxiv.org/abs/2309.02423v1",
        "pub_date": "2023-09-05",
        "summary": "With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),\nlarge-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.\nHowever, most current research is built on resources derived from third-person\nvideo action recognition. This inherent domain gap between first- and\nthird-person action videos, which have not been adequately addressed before,\nmakes current Ego-HOI suboptimal. This paper rethinks and proposes a new\nframework as an infrastructure to advance Ego-HOI recognition by Probing,\nCuration and Adaption (EgoPCA). We contribute comprehensive pre-train sets,\nbalanced test sets and a new baseline, which are complete with a\ntraining-finetuning strategy. With our new framework, we not only achieve\nstate-of-the-art performance on Ego-HOI benchmarks but also build several new\nand effective mechanisms and settings to advance further research. We believe\nour data and the findings will pave a new way for Ego-HOI understanding. Code\nand data are available at https://mvig-rhos.com/ego_pca",
        "translated": ""
    },
    {
        "title": "My Art My Choice: Adversarial Protection Against Unruly AI",
        "url": "http://arxiv.org/abs/2309.03198v1",
        "pub_date": "2023-09-06",
        "summary": "Generative AI is on the rise, enabling everyone to produce realistic content\nvia publicly available interfaces. Especially for guided image generation,\ndiffusion models are changing the creator economy by producing high quality low\ncost content. In parallel, artists are rising against unruly AI, since their\nartwork are leveraged, distributed, and dissimulated by large generative\nmodels. Our approach, My Art My Choice (MAMC), aims to empower content owners\nby protecting their copyrighted materials from being utilized by diffusion\nmodels in an adversarial fashion. MAMC learns to generate adversarially\nperturbed \"protected\" versions of images which can in turn \"break\" diffusion\nmodels. The perturbation amount is decided by the artist to balance distortion\nvs. protection of the content. MAMC is designed with a simple UNet-based\ngenerator, attacking black box diffusion models, combining several losses to\ncreate adversarial twins of the original artwork. We experiment on three\ndatasets for various image-to-image tasks, with different user control values.\nBoth protected image and diffusion output results are evaluated in visual,\nnoise, structure, pixel, and generative spaces to validate our claims. We\nbelieve that MAMC is a crucial step for preserving ownership information for AI\ngenerated content in a flawless, based-on-need, and human-centric way.",
        "translated": ""
    },
    {
        "title": "Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.03185v1",
        "pub_date": "2023-09-06",
        "summary": "Neural Radiance Fields (NeRFs) have shown promise in applications like view\nsynthesis and depth estimation, but learning from multiview images faces\ninherent uncertainties. Current methods to quantify them are either heuristic\nor computationally demanding. We introduce BayesRays, a post-hoc framework to\nevaluate uncertainty in any pre-trained NeRF without modifying the training\nprocess. Our method establishes a volumetric uncertainty field using spatial\nperturbations and a Bayesian Laplace approximation. We derive our algorithm\nstatistically and show its superior performance in key metrics and\napplications. Additional results available at: https://bayesrays.github.io.",
        "translated": ""
    },
    {
        "title": "3D Transformer based on deformable patch location for differential\n  diagnosis between Alzheimer's disease and Frontotemporal dementia",
        "url": "http://arxiv.org/abs/2309.03183v1",
        "pub_date": "2023-09-06",
        "summary": "Alzheimer's disease and Frontotemporal dementia are common types of\nneurodegenerative disorders that present overlapping clinical symptoms, making\ntheir differential diagnosis very challenging. Numerous efforts have been done\nfor the diagnosis of each disease but the problem of multi-class differential\ndiagnosis has not been actively explored. In recent years, transformer-based\nmodels have demonstrated remarkable success in various computer vision tasks.\nHowever, their use in disease diagnostic is uncommon due to the limited amount\nof 3D medical data given the large size of such models. In this paper, we\npresent a novel 3D transformer-based architecture using a deformable patch\nlocation module to improve the differential diagnosis of Alzheimer's disease\nand Frontotemporal dementia. Moreover, to overcome the problem of data\nscarcity, we propose an efficient combination of various data augmentation\ntechniques, adapted for training transformer-based models on 3D structural\nmagnetic resonance imaging data. Finally, we propose to combine our\ntransformer-based model with a traditional machine learning model using brain\nstructure volumes to better exploit the available data. Our experiments\ndemonstrate the effectiveness of the proposed approach, showing competitive\nresults compared to state-of-the-art methods. Moreover, the deformable patch\nlocations can be visualized, revealing the most relevant brain regions used to\nestablish the diagnosis of each disease.",
        "translated": ""
    },
    {
        "title": "SLiMe: Segment Like Me",
        "url": "http://arxiv.org/abs/2309.03179v1",
        "pub_date": "2023-09-06",
        "summary": "Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.",
        "translated": ""
    },
    {
        "title": "3D Object Positioning Using Differentiable Multimodal Learning",
        "url": "http://arxiv.org/abs/2309.03177v1",
        "pub_date": "2023-09-06",
        "summary": "This article describes a multi-modal method using simulated Lidar data via\nray tracing and image pixel loss with differentiable rendering to optimize an\nobject's position with respect to an observer or some referential objects in a\ncomputer graphics scene. Object position optimization is completed using\ngradient descent with the loss function being influenced by both modalities.\nTypical object placement optimization is done using image pixel loss with\ndifferentiable rendering only, this work shows the use of a second modality\n(Lidar) leads to faster convergence. This method of fusing sensor input\npresents a potential usefulness for autonomous vehicles, as these methods can\nbe used to establish the locations of multiple actors in a scene. This article\nalso presents a method for the simulation of multiple types of data to be used\nin the training of autonomous vehicles.",
        "translated": ""
    },
    {
        "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
        "url": "http://arxiv.org/abs/2309.03905v1",
        "pub_date": "2023-09-07",
        "summary": "We present ImageBind-LLM, a multi-modality instruction tuning method of large\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\nand image instruction tuning, different from which, our ImageBind-LLM can\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\nand their embedding-space arithmetic by only image-text alignment training.\nDuring training, we adopt a learnable bind network to align the embedding space\nbetween LLaMA and ImageBind's image encoder. Then, the image features\ntransformed by the bind network are added to word tokens of all layers in\nLLaMA, which progressively injects visual instructions via an attention-free\nand zero-initialized gating mechanism. Aided by the joint embedding of\nImageBind, the simple image-text training enables our model to exhibit superior\nmulti-modality instruction-following capabilities. During inference, the\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\nprocessed by a proposed visual cache model for further cross-modal embedding\nenhancement. The training-free cache model retrieves from three million image\nfeatures extracted by ImageBind, which effectively mitigates the\ntraining-inference modality discrepancy. Notably, with our approach,\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\nsignificant language generation quality. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.",
        "translated": ""
    },
    {
        "title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
        "url": "http://arxiv.org/abs/2309.03904v1",
        "pub_date": "2023-09-07",
        "summary": "Due to the difficulty in scaling up, generative adversarial networks (GANs)\nseem to be falling from grace on the task of text-conditioned image synthesis.\nSparsely-activated mixture-of-experts (MoE) has recently been demonstrated as a\nvalid solution to training large-scale models with limited computational\nresources. Inspired by such a philosophy, we present Aurora, a GAN-based\ntext-to-image generator that employs a collection of experts to learn feature\nprocessing, together with a sparse router to help select the most suitable\nexpert for each feature point. To faithfully decode the sampling stochasticity\nand the text condition to the final synthesis, our router adaptively makes its\ndecision by taking into account the text-integrated global latent code. At\n64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves\n6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitate\nthe community for further development.",
        "translated": ""
    },
    {
        "title": "Tracking Anything with Decoupled Video Segmentation",
        "url": "http://arxiv.org/abs/2309.03903v1",
        "pub_date": "2023-09-07",
        "summary": "Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA",
        "translated": ""
    },
    {
        "title": "Learning Continuous Exposure Value Representations for Single-Image HDR\n  Reconstruction",
        "url": "http://arxiv.org/abs/2309.03900v1",
        "pub_date": "2023-09-07",
        "summary": "Deep learning is commonly used to reconstruct HDR images from LDR images. LDR\nstack-based methods are used for single-image HDR reconstruction, generating an\nHDR image from a deep learning-generated LDR stack. However, current methods\ngenerate the stack with predetermined exposure values (EVs), which may limit\nthe quality of HDR reconstruction. To address this, we propose the continuous\nexposure value representation (CEVR), which uses an implicit function to\ngenerate LDR images with arbitrary EVs, including those unseen during training.\nOur approach generates a continuous stack with more images containing diverse\nEVs, significantly improving HDR reconstruction. We use a cycle training\nstrategy to supervise the model in generating continuous EV LDR images without\ncorresponding ground truths. Our CEVR model outperforms existing methods, as\ndemonstrated by experimental results.",
        "translated": ""
    },
    {
        "title": "The Making and Breaking of Camouflage",
        "url": "http://arxiv.org/abs/2309.03899v1",
        "pub_date": "2023-09-07",
        "summary": "Not all camouflages are equally effective, as even a partially visible\ncontour or a slight color difference can make the animal stand out and break\nits camouflage. In this paper, we address the question of what makes a\ncamouflage successful, by proposing three scores for automatically assessing\nits effectiveness. In particular, we show that camouflage can be measured by\nthe similarity between background and foreground features and boundary\nvisibility. We use these camouflage scores to assess and compare all available\ncamouflage datasets. We also incorporate the proposed camouflage score into a\ngenerative model as an auxiliary loss and show that effective camouflage images\nor videos can be synthesised in a scalable manner. The generated synthetic\ndataset is used to train a transformer-based model for segmenting camouflaged\nanimals in videos. Experimentally, we demonstrate state-of-the-art camouflage\nbreaking performance on the public MoCA-Mask benchmark.",
        "translated": ""
    },
    {
        "title": "Generalized Cross-domain Multi-label Few-shot Learning for Chest X-rays",
        "url": "http://arxiv.org/abs/2309.04462v1",
        "pub_date": "2023-09-08",
        "summary": "Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.",
        "translated": ""
    },
    {
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2309.04461v1",
        "pub_date": "2023-09-08",
        "summary": "Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.",
        "translated": ""
    },
    {
        "title": "WiSARD: A Labeled Visual and Thermal Image Dataset for Wilderness Search\n  and Rescue",
        "url": "http://arxiv.org/abs/2309.04453v1",
        "pub_date": "2023-09-08",
        "summary": "Sensor-equipped unoccupied aerial vehicles (UAVs) have the potential to help\nreduce search times and alleviate safety risks for first responders carrying\nout Wilderness Search and Rescue (WiSAR) operations, the process of finding and\nrescuing person(s) lost in wilderness areas. Unfortunately, visual sensors\nalone do not address the need for robustness across all the possible terrains,\nweather, and lighting conditions that WiSAR operations can be conducted in. The\nuse of multi-modal sensors, specifically visual-thermal cameras, is critical in\nenabling WiSAR UAVs to perform in diverse operating conditions. However, due to\nthe unique challenges posed by the wilderness context, existing dataset\nbenchmarks are inadequate for developing vision-based algorithms for autonomous\nWiSAR UAVs. To this end, we present WiSARD, a dataset with roughly 56,000\nlabeled visual and thermal images collected from UAV flights in various\nterrains, seasons, weather, and lighting conditions. To the best of our\nknowledge, WiSARD is the first large-scale dataset collected with multi-modal\nsensors for autonomous WiSAR operations. We envision that our dataset will\nprovide researchers with a diverse and challenging benchmark that can test the\nrobustness of their algorithms when applied to real-world (life-saving)\napplications.",
        "translated": ""
    },
    {
        "title": "Demographic Disparities in 1-to-Many Facial Identification",
        "url": "http://arxiv.org/abs/2309.04447v1",
        "pub_date": "2023-09-08",
        "summary": "Most studies to date that have examined demographic variations in face\nrecognition accuracy have analyzed 1-to-1 matching accuracy, using images that\ncould be described as \"government ID quality\". This paper analyzes the accuracy\nof 1-to-many facial identification across demographic groups, and in the\npresence of blur and reduced resolution in the probe image as might occur in\n\"surveillance camera quality\" images. Cumulative match characteristic\ncurves(CMC) are not appropriate for comparing propensity for rank-one\nrecognition errors across demographics, and so we introduce three metrics for\nthis: (1) d' metric between mated and non-mated score distributions, (2)\nabsolute score difference between thresholds in the high-similarity tail of the\nnon-mated and the low-similarity tail of the mated distribution, and (3)\ndistribution of (mated - non-mated rank one scores) across the set of probe\nimages. We find that demographic variation in 1-to-many accuracy does not\nentirely follow what has been observed in 1-to-1 matching accuracy. Also,\ndifferent from 1-to-1 accuracy, demographic comparison of 1-to-many accuracy\ncan be affected by different numbers of identities and images across\ndemographics. Finally, we show that increased blur in the probe image, or\nreduced resolution of the face in the probe image, can significantly increase\nthe false positive identification rate. And we show that the demographic\nvariation in these high blur or low resolution conditions is much larger for\nmale/ female than for African-American / Caucasian. The point that 1-to-many\naccuracy can potentially collapse in the context of processing \"surveillance\ncamera quality\" probe images against a \"government ID quality\" gallery is an\nimportant one.",
        "translated": ""
    },
    {
        "title": "Comparative Study of Visual SLAM-Based Mobile Robot Localization Using\n  Fiducial Markers",
        "url": "http://arxiv.org/abs/2309.04441v1",
        "pub_date": "2023-09-08",
        "summary": "This paper presents a comparative study of three modes for mobile robot\nlocalization based on visual SLAM using fiducial markers (i.e., square-shaped\nartificial landmarks with a black-and-white grid pattern): SLAM, SLAM with a\nprior map, and localization with a prior map. The reason for comparing the\nSLAM-based approaches leveraging fiducial markers is because previous work has\nshown their superior performance over feature-only methods, with less\ncomputational burden compared to methods that use both feature and marker\ndetection without compromising the localization performance. The evaluation is\nconducted using indoor image sequences captured with a hand-held camera\ncontaining multiple fiducial markers in the environment. The performance\nmetrics include absolute trajectory error and runtime for the optimization\nprocess per frame. In particular, for the last two modes (SLAM and localization\nwith a prior map), we evaluate their performances by perturbing the quality of\nprior map to study the extent to which each mode is tolerant to such\nperturbations. Hardware experiments show consistent trajectory error levels\nacross the three modes, with the localization mode exhibiting the shortest\nruntime among them. Yet, with map perturbations, SLAM with a prior map\nmaintains performance, while localization mode degrades in both aspects.",
        "translated": ""
    },
    {
        "title": "Robot Parkour Learning",
        "url": "http://arxiv.org/abs/2309.05665v1",
        "pub_date": "2023-09-11",
        "summary": "Parkour is a grand challenge for legged locomotion that requires robots to\novercome various obstacles rapidly in complex environments. Existing methods\ncan generate either diverse but blind locomotion skills or vision-based but\nspecialized skills by using reference animal data or complex rewards. However,\nautonomous parkour requires robots to learn generalizable skills that are both\nvision-based and diverse to perceive and react to various scenarios. In this\nwork, we propose a system for learning a single end-to-end vision-based parkour\npolicy of diverse parkour skills using a simple reward without any reference\nmotion data. We develop a reinforcement learning method inspired by direct\ncollocation to generate parkour skills, including climbing over high obstacles,\nleaping over large gaps, crawling beneath low barriers, squeezing through thin\nslits, and running. We distill these skills into a single vision-based parkour\npolicy and transfer it to a quadrupedal robot using its egocentric depth\ncamera. We demonstrate that our system can empower two different low-cost\nrobots to autonomously select and execute appropriate parkour skills to\ntraverse challenging real-world environments.",
        "translated": ""
    },
    {
        "title": "Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction\n  Clips",
        "url": "http://arxiv.org/abs/2309.05663v1",
        "pub_date": "2023-09-11",
        "summary": "We tackle the task of reconstructing hand-object interactions from short\nvideo clips. Given an input video, our approach casts 3D inference as a\nper-video optimization and recovers a neural 3D representation of the object\nshape, as well as the time-varying motion and hand articulation. While the\ninput video naturally provides some multi-view cues to guide 3D inference,\nthese are insufficient on their own due to occlusions and limited viewpoint\nvariations. To obtain accurate 3D, we augment the multi-view signals with\ngeneric data-driven priors to guide reconstruction. Specifically, we learn a\ndiffusion network to model the conditional distribution of (geometric)\nrenderings of objects conditioned on hand configuration and category label, and\nleverage it as a prior to guide the novel-view renderings of the reconstructed\nscene. We empirically evaluate our approach on egocentric videos across 6\nobject categories, and observe significant improvements over prior single-view\nand multi-view methods. Finally, we demonstrate our system's ability to\nreconstruct arbitrary clips from YouTube, showing both 1st and 3rd person\ninteractions.",
        "translated": ""
    },
    {
        "title": "ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape\n  Completion",
        "url": "http://arxiv.org/abs/2309.05662v1",
        "pub_date": "2023-09-11",
        "summary": "In this letter, we introduce ViHOPE, a novel framework for estimating the 6D\npose of an in-hand object using visuotactile perception. Our key insight is\nthat the accuracy of the 6D object pose estimate can be improved by explicitly\ncompleting the shape of the object. To this end, we introduce a novel\nvisuotactile shape completion module that uses a conditional Generative\nAdversarial Network to complete the shape of an in-hand object based on\nvolumetric representation. This approach improves over prior works that\ndirectly regress visuotactile observations to a 6D pose. By explicitly\ncompleting the shape of the in-hand object and jointly optimizing the shape\ncompletion and pose estimation tasks, we improve the accuracy of the 6D object\npose estimate. We train and test our model on a synthetic dataset and compare\nit with the state-of-the-art. In the visuotactile shape completion task, we\noutperform the state-of-the-art by 265% using the Intersection of Union metric\nand achieve 88% lower Chamfer Distance. In the visuotactile pose estimation\ntask, we present results that suggest our framework reduces position and\nangular errors by 35% and 64%, respectively. Furthermore, we ablate our\nframework to confirm the gain on the 6D object pose estimate from explicitly\ncompleting the shape. Ultimately, we show that our framework produces models\nthat are robust to sim-to-real transfer on a real-world robot platform.",
        "translated": ""
    },
    {
        "title": "An Effective Two-stage Training Paradigm Detector for Small Dataset",
        "url": "http://arxiv.org/abs/2309.05652v1",
        "pub_date": "2023-09-11",
        "summary": "Learning from the limited amount of labeled data to the pre-train model has\nalways been viewed as a challenging task. In this report, an effective and\nrobust solution, the two-stage training paradigm YOLOv8 detector (TP-YOLOv8),\nis designed for the object detection track in VIPriors Challenge 2023. First,\nthe backbone of YOLOv8 is pre-trained as the encoder using the masked image\nmodeling technique. Then the detector is fine-tuned with elaborate\naugmentations. During the test stage, test-time augmentation (TTA) is used to\nenhance each model, and weighted box fusion (WBF) is implemented to further\nboost the performance. With the well-designed structure, our approach has\nachieved 30.4% average precision from 0.50 to 0.95 on the DelftBikes test set,\nranking 4th on the leaderboard.",
        "translated": ""
    },
    {
        "title": "CitDet: A Benchmark Dataset for Citrus Fruit Detection",
        "url": "http://arxiv.org/abs/2309.05645v1",
        "pub_date": "2023-09-11",
        "summary": "In this letter, we present a new dataset to advance the state of the art in\ndetecting citrus fruit and accurately estimate yield on trees affected by the\nHuanglongbing (HLB) disease in orchard environments via imaging. Despite the\nfact that significant progress has been made in solving the fruit detection\nproblem, the lack of publicly available datasets has complicated direct\ncomparison of results. For instance, citrus detection has long been of interest\nin the agricultural research community, yet there is an absence of work,\nparticularly involving public datasets of citrus affected by HLB. To address\nthis issue, we enhance state-of-the-art object detection methods for use in\ntypical orchard settings. Concretely, we provide high-resolution images of\ncitrus trees located in an area known to be highly affected by HLB, along with\nhigh-quality bounding box annotations of citrus fruit. Fruit on both the trees\nand the ground are labeled to allow for identification of fruit location, which\ncontributes to advancements in yield estimation and potential measure of HLB\nimpact via fruit drop. The dataset consists of over 32,000 bounding box\nannotations for fruit instances contained in 579 high-resolution images. In\nsummary, our contributions are the following: (i) we introduce a novel dataset\nalong with baseline performance benchmarks on multiple contemporary object\ndetection algorithms, (ii) we show the ability to accurately capture fruit\nlocation on tree or on ground, and finally (ii) we present a correlation of our\nresults with yield estimations.",
        "translated": ""
    },
    {
        "title": "Learning Disentangled Avatars with Hybrid 3D Representations",
        "url": "http://arxiv.org/abs/2309.06441v1",
        "pub_date": "2023-09-12",
        "summary": "Tremendous efforts have been made to learn animatable and photorealistic\nhuman avatars. Towards this end, both explicit and implicit 3D representations\nare heavily studied for a holistic modeling and capture of the whole human\n(e.g., body, clothing, face and hair), but neither representation is an optimal\nchoice in terms of representation efficacy since different parts of the human\navatar have different modeling desiderata. For example, meshes are generally\nnot suitable for modeling clothing and hair. Motivated by this, we present\nDisentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit\n3D representations. DELTA takes a monocular RGB video as input, and produces a\nhuman avatar with separate body and clothing/hair layers. Specifically, we\ndemonstrate two important applications for DELTA. For the first one, we\nconsider the disentanglement of the human body and clothing and in the second,\nwe disentangle the face and hair. To do so, DELTA represents the body or face\nwith an explicit mesh-based parametric 3D model and the clothing or hair with\nan implicit neural radiance field. To make this possible, we design an\nend-to-end differentiable renderer that integrates meshes into volumetric\nrendering, enabling DELTA to learn directly from monocular videos without any\n3D supervision. Finally, we show that how these two applications can be easily\ncombined to model full-body avatars, such that the hair, face, body and\nclothing can be fully disentangled yet jointly rendered. Such a disentanglement\nenables hair and clothing transfer to arbitrary body shapes. We empirically\nvalidate the effectiveness of DELTA's disentanglement by demonstrating its\npromising performance on disentangled reconstruction, virtual clothing try-on\nand hairstyle transfer. To facilitate future research, we also release an\nopen-sourced pipeline for the study of hybrid human avatar modeling.",
        "translated": ""
    },
    {
        "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot\n  Learning",
        "url": "http://arxiv.org/abs/2309.06440v1",
        "pub_date": "2023-09-12",
        "summary": "Dexterous manipulation has been a long-standing challenge in robotics. While\nmachine learning techniques have shown some promise, results have largely been\ncurrently limited to simulation. This can be mostly attributed to the lack of\nsuitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous\nand anthropomorphic hand for machine learning research. In contrast to previous\nhands, LEAP Hand has a novel kinematic structure that allows maximal dexterity\nregardless of finger pose. LEAP Hand is low-cost and can be assembled in 4\nhours at a cost of 2000 USD from readily available parts. It is capable of\nconsistently exerting large torques over long durations of time. We show that\nLEAP Hand can be used to perform several manipulation tasks in the real world\n-- from visual teleoperation to learning from passive video data and sim2real.\nLEAP Hand significantly outperforms its closest competitor Allegro Hand in all\nour experiments while being 1/8th of the cost. We release detailed assembly\ninstructions, the Sim2Real pipeline and a development platform with useful APIs\non our website at https://leap-hand.github.io/",
        "translated": ""
    },
    {
        "title": "Attention De-sparsification Matters: Inducing Diversity in Digital\n  Pathology Representation Learning",
        "url": "http://arxiv.org/abs/2309.06439v1",
        "pub_date": "2023-09-12",
        "summary": "We propose DiRL, a Diversity-inducing Representation Learning technique for\nhistopathology imaging. Self-supervised learning techniques, such as\ncontrastive and non-contrastive approaches, have been shown to learn rich and\neffective representations of digitized tissue samples with limited pathologist\nsupervision. Our analysis of vanilla SSL-pretrained models' attention\ndistribution reveals an insightful observation: sparsity in attention, i.e,\nmodels tends to localize most of their attention to some prominent patterns in\nthe image. Although attention sparsity can be beneficial in natural images due\nto these prominent patterns being the object of interest itself, this can be\nsub-optimal in digital pathology; this is because, unlike natural images,\ndigital pathology scans are not object-centric, but rather a complex phenotype\nof various spatially intermixed biological components. Inadequate\ndiversification of attention in these complex images could result in crucial\ninformation loss. To address this, we leverage cell segmentation to densely\nextract multiple histopathology-specific representations, and then propose a\nprior-guided dense pretext task for SSL, designed to match the multiple\ncorresponding representations between the views. Through this, the model learns\nto attend to various components more closely and evenly, thus inducing adequate\ndiversification in attention for capturing context rich representations.\nThrough quantitative and qualitative analysis on multiple tasks across cancer\ntypes, we demonstrate the efficacy of our method and observe that the attention\nis more globally distributed.",
        "translated": ""
    },
    {
        "title": "Exploring Non-additive Randomness on ViT against Query-Based Black-Box\n  Attacks",
        "url": "http://arxiv.org/abs/2309.06438v1",
        "pub_date": "2023-09-12",
        "summary": "Deep Neural Networks can be easily fooled by small and imperceptible\nperturbations. The query-based black-box attack (QBBA) is able to create the\nperturbations using model output probabilities of image queries requiring no\naccess to the underlying models. QBBA poses realistic threats to real-world\napplications. Recently, various types of robustness have been explored to\ndefend against QBBA. In this work, we first taxonomize the stochastic defense\nstrategies against QBBA. Following our taxonomy, we propose to explore\nnon-additive randomness in models to defend against QBBA. Specifically, we\nfocus on underexplored Vision Transformers based on their flexible\narchitectures. Extensive experiments show that the proposed defense approach\nachieves effective defense, without much sacrifice in performance.",
        "translated": ""
    },
    {
        "title": "AGMDT: Virtual Staining of Renal Histology Images with Adjacency-Guided\n  Multi-Domain Transfer",
        "url": "http://arxiv.org/abs/2309.06421v1",
        "pub_date": "2023-09-12",
        "summary": "Renal pathology, as the gold standard of kidney disease diagnosis, requires\ndoctors to analyze a serial of tissue slices stained by H\\&amp;E staining and\nspecial staining like Masson, PASM, and PAS, respectively. These special\nstaining methods are costly, time-consuming, and hard to standardize for wide\nuse especially in primary hospitals. Advances of supervised learning methods\ncan virtually convert H\\&amp;E images into special staining images, but the\npixel-to-pixel alignment is hard to achieve for training. As contrast,\nunsupervised learning methods regarding different stains as different style\ntransferring domains can use unpaired data, but they ignore the spatial\ninter-domain correlations and thus decrease the trustworthiness of structural\ndetails for diagnosis. In this paper, we propose a novel virtual staining\nframework AGMDT to translate images into other domains by avoiding pixel-level\nalignment and meanwhile utilizing the correlations among adjacent tissue\nslices. We first build a high-quality multi-domain renal histological dataset\nwhere each specimen case comprises a series of slices stained in various ways.\nBased on it, the proposed framework AGMDT discovers patch-level aligned pairs\nacross the serial slices of multi-domains through glomerulus detection and\nbipartite graph matching, and utilizes such correlations to supervise the\nend-to-end model for multi-domain staining transformation. Experimental results\nshow that the proposed AGMDT achieves a good balance between the precise\npixel-level alignment and unpaired domain transfer by exploiting correlations\nacross multi-domain serial pathological slices, and outperforms the\nstate-of-the-art methods in both quantitative measure and morphological\ndetails.",
        "translated": ""
    },
    {
        "title": "Text-Guided Generation and Editing of Compositional 3D Avatars",
        "url": "http://arxiv.org/abs/2309.07125v1",
        "pub_date": "2023-09-13",
        "summary": "Our goal is to create a realistic 3D facial avatar with hair and accessories\nusing only a text description. While this challenge has attracted significant\nrecent interest, existing methods either lack realism, produce unrealistic\nshapes, or do not support editing, such as modifications to the hairstyle. We\nargue that existing methods are limited because they employ a monolithic\nmodeling approach, using a single representation for the head, face, hair, and\naccessories. Our observation is that the hair and face, for example, have very\ndifferent structural qualities that benefit from different representations.\nBuilding on this insight, we generate avatars with a compositional model, in\nwhich the head, face, and upper body are represented with traditional 3D\nmeshes, and the hair, clothing, and accessories with neural radiance fields\n(NeRF). The model-based mesh representation provides a strong geometric prior\nfor the face region, improving realism while enabling editing of the person's\nappearance. By using NeRFs to represent the remaining components, our method is\nable to model and synthesize parts with complex geometry and appearance, such\nas curly hair and fluffy scarves. Our novel system synthesizes these\nhigh-quality compositional avatars from text descriptions. The experimental\nresults demonstrate that our method, Text-guided generation and Editing of\nCompositional Avatars (TECA), produces avatars that are more realistic than\nthose of recent methods while being editable because of their compositional\nnature. For example, our TECA enables the seamless transfer of compositional\nfeatures like hairstyles, scarves, and other accessories between avatars. This\ncapability supports applications such as virtual try-on.",
        "translated": ""
    },
    {
        "title": "Tree-Structured Shading Decomposition",
        "url": "http://arxiv.org/abs/2309.07122v1",
        "pub_date": "2023-09-13",
        "summary": "We study inferring a tree-structured representation from a single image for\nobject shading. Prior work typically uses the parametric or measured\nrepresentation to model shading, which is neither interpretable nor easily\neditable. We propose using the shade tree representation, which combines basic\nshading nodes and compositing methods to factorize object surface shading. The\nshade tree representation enables novice users who are unfamiliar with the\nphysical shading process to edit object shading in an efficient and intuitive\nmanner. A main challenge in inferring the shade tree is that the inference\nproblem involves both the discrete tree structure and the continuous parameters\nof the tree nodes. We propose a hybrid approach to address this issue. We\nintroduce an auto-regressive inference model to generate a rough estimation of\nthe tree structure and node parameters, and then we fine-tune the inferred\nshade tree through an optimization algorithm. We show experiments on synthetic\nimages, captured reflectance, real images, and non-realistic vector drawings,\nallowing downstream applications such as material editing, vectorized shading,\nand relighting. Project website: https://chen-geng.com/inv-shade-trees",
        "translated": ""
    },
    {
        "title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness\n  and Ethics",
        "url": "http://arxiv.org/abs/2309.07120v1",
        "pub_date": "2023-09-13",
        "summary": "Multi-modal large language models (MLLMs) are trained based on large language\nmodels (LLM), with an enhanced capability to comprehend multi-modal inputs and\ngenerate textual responses. While they excel in multi-modal tasks, the pure NLP\nabilities of MLLMs are often underestimated and left untested. In this study,\nwe get out of the box and unveil an intriguing characteristic of MLLMs -- our\npreliminary results suggest that visual instruction tuning, a prevailing\nstrategy for transitioning LLMs into MLLMs, unexpectedly and interestingly\nhelps models attain both improved truthfulness and ethical alignment in the\npure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model\nsurpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one\nmillion human annotations, on TruthfulQA-mc and Ethics benchmarks. Further\nanalysis reveals that the improved alignment can be attributed to the superior\ninstruction quality inherent to visual-text data. In releasing our code at\ngithub.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration\ninto the intrinsic value of visual-text synergies and, in a broader scope,\nmulti-modal interactions in alignment research.",
        "translated": ""
    },
    {
        "title": "PILOT: A Pre-Trained Model-Based Continual Learning Toolbox",
        "url": "http://arxiv.org/abs/2309.07117v1",
        "pub_date": "2023-09-13",
        "summary": "While traditional machine learning can effectively tackle a wide range of\nproblems, it primarily operates within a closed-world setting, which presents\nlimitations when dealing with streaming data. As a solution, incremental\nlearning emerges to address real-world scenarios involving new data's arrival.\nRecently, pre-training has made significant advancements and garnered the\nattention of numerous researchers. The strong performance of these pre-trained\nmodels (PTMs) presents a promising avenue for developing continual learning\nalgorithms that can effectively adapt to real-world scenarios. Consequently,\nexploring the utilization of PTMs in incremental learning has become essential.\nThis paper introduces a pre-trained model-based continual learning toolbox\nknown as PILOT. On the one hand, PILOT implements some state-of-the-art\nclass-incremental learning algorithms based on pre-trained models, such as L2P,\nDualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical\nclass-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the\ncontext of pre-trained models to evaluate their effectiveness.",
        "translated": ""
    },
    {
        "title": "Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker\n  Verification",
        "url": "http://arxiv.org/abs/2309.07115v1",
        "pub_date": "2023-09-13",
        "summary": "In this paper, we present a methodology for achieving robust multimodal\nperson representations optimized for open-set audio-visual speaker\nverification. Distance Metric Learning (DML) approaches have typically\ndominated this problem space, owing to strong performance on new and unseen\nclasses. In our work, we explored multitask learning techniques to further\nboost performance of the DML approach and show that an auxiliary task with weak\nlabels can increase the compactness of the learned speaker representation. We\nalso extend the Generalized end-to-end loss (GE2E) to multimodal inputs and\ndemonstrate that it can achieve competitive performance in an audio-visual\nspace. Finally, we introduce a non-synchronous audio-visual sampling random\nstrategy during training time that has shown to improve generalization. Our\nnetwork achieves state of the art performance for speaker verification,\nreporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official\ntrial lists of VoxCeleb1-O/E/H, which is to our knowledge, the best published\nresults on VoxCeleb1-E and VoxCeleb1-H.",
        "translated": ""
    },
    {
        "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
        "url": "http://arxiv.org/abs/2309.07920v1",
        "pub_date": "2023-09-14",
        "summary": "Creating diverse and high-quality 3D assets with an automatic generative\nmodel is highly desirable. Despite extensive efforts on 3D generation, most\nexisting works focus on the generation of a single category or a few\ncategories. In this paper, we introduce a diffusion-based feed-forward\nframework for synthesizing massive categories of real-world 3D objects with a\nsingle generative model. Notably, there are three major challenges for this\nlarge-vocabulary 3D generation: a) the need for expressive yet efficient 3D\nrepresentation; b) large diversity in geometry and texture across categories;\nc) complexity in the appearances of real-world objects. To this end, we propose\na novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for\nhandling challenges via three aspects. 1) Considering efficiency and\nrobustness, we adopt a revised triplane representation and improve the fitting\nspeed and accuracy. 2) To handle the drastic variations in geometry and\ntexture, we regard the features of all 3D objects as a combination of\ngeneralized 3D knowledge and specialized 3D features. To extract generalized 3D\nknowledge from diverse categories, we propose a novel 3D-aware transformer with\nshared cross-plane attention. It learns the cross-plane relations across\ndifferent planes and aggregates the generalized 3D knowledge with specialized\n3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance\nthe generalized 3D knowledge in the encoded triplanes for handling categories\nwith complex appearances. Extensive experiments on ShapeNet and OmniObject3D\n(over 200 diverse real-world categories) convincingly demonstrate that a single\nDiffTF model achieves state-of-the-art large-vocabulary 3D object generation\nperformance with large diversity, rich semantics, and high quality.",
        "translated": ""
    },
    {
        "title": "OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering\n  Evaluation on Real Objects",
        "url": "http://arxiv.org/abs/2309.07921v1",
        "pub_date": "2023-09-14",
        "summary": "We introduce OpenIllumination, a real-world dataset containing over 108K\nimages of 64 objects with diverse materials, captured under 72 camera views and\na large number of different illuminations. For each image in the dataset, we\nprovide accurate camera parameters, illumination ground truth, and foreground\nsegmentation masks. Our dataset enables the quantitative evaluation of most\ninverse rendering and material decomposition methods for real objects. We\nexamine several state-of-the-art inverse rendering methods on our dataset and\ncompare their performances. The dataset and code can be found on the project\npage: https://oppo-us-research.github.io/OpenIllumination.",
        "translated": ""
    },
    {
        "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
        "url": "http://arxiv.org/abs/2309.07918v1",
        "pub_date": "2023-09-14",
        "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .",
        "translated": ""
    },
    {
        "title": "Looking at words and points with attention: a benchmark for\n  text-to-shape coherence",
        "url": "http://arxiv.org/abs/2309.07917v1",
        "pub_date": "2023-09-14",
        "summary": "While text-conditional 3D object generation and manipulation have seen rapid\nprogress, the evaluation of coherence between generated 3D shapes and input\ntextual descriptions lacks a clear benchmark. The reason is twofold: a) the low\nquality of the textual descriptions in the only publicly available dataset of\ntext-shape pairs; b) the limited effectiveness of the metrics used to\nquantitatively assess such coherence. In this paper, we propose a comprehensive\nsolution that addresses both weaknesses. Firstly, we employ large language\nmodels to automatically refine textual descriptions associated with shapes.\nSecondly, we propose a quantitative metric to assess text-to-shape coherence,\nthrough cross-attention mechanisms. To validate our approach, we conduct a user\nstudy and compare quantitatively our metric with existing ones. The refined\ndataset, the new metric and a set of text-shape pairs validated by the user\nstudy comprise a novel, fine-grained benchmark that we publicly release to\nfoster research on text-to-shape coherence of text-conditioned 3D generative\nmodels. Benchmark available at\nhttps://cvlab-unibo.github.io/CrossCoherence-Web/.",
        "translated": ""
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2309.07915v1",
        "pub_date": "2023-09-14",
        "summary": "Starting from the resurgence of deep learning, vision-language models (VLMs)\nbenefiting from large language models (LLMs) have never been so popular.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images. The issue can\ntraced back to the architectural design of VLMs or pre-training data.\nSpecifically, the current VLMs primarily emphasize utilizing multi-modal data\nwith a single image some, rather than multi-modal prompts with interleaved\nmultiple images and text. Even though some newly proposed VLMs could handle\nuser prompts with multiple images, pre-training data does not provide more\nsophisticated multi-modal prompts than interleaved image and text crawled from\nthe web. We propose MMICL to address the issue by considering both the model\nand data perspectives. We introduce a well-designed architecture capable of\nseamlessly integrating visual and textual context in an interleaved manner and\nMIC dataset to reduce the gap between the training data and the complex user\nprompts in real-world applications, including: 1) multi-modal context with\ninterleaved images and text, 2) textual references for each image, and 3)\nmulti-image data with spatial, logical, or temporal relationships. Our\nexperiments confirm that MMICL achieves new stat-of-the-art zero-shot and\nfew-shot performance on a wide range of general vision-language tasks,\nespecially for complex reasoning benchmarks including MME and MMBench. Our\nanalysis demonstrates that MMICL effectively deals with the challenge of\ncomplex multi-modal prompt understanding. The experiments on ScienceQA-IMG also\nshow that MMICL successfully alleviates the issue of language bias in VLMs,\nwhich we believe is the reason behind the advanced performance of MMICL.",
        "translated": ""
    },
    {
        "title": "Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under Non-Uniform Motion",
        "url": "http://arxiv.org/abs/2309.08596v1",
        "pub_date": "2023-09-15",
        "summary": "Event cameras offer many advantages over standard cameras due to their\ndistinctive principle of operation: low power, low latency, high temporal\nresolution and high dynamic range. Nonetheless, the success of many downstream\nvisual applications also hinges on an efficient and effective scene\nrepresentation, where Neural Radiance Field (NeRF) is seen as the leading\ncandidate. Such promise and potential of event cameras and NeRF inspired recent\nworks to investigate on the reconstruction of NeRF from moving event cameras.\nHowever, these works are mainly limited in terms of the dependence on dense and\nlow-noise event streams, as well as generalization to arbitrary contrast\nthreshold values and camera speed profiles. In this work, we propose Robust\ne-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving\nevent cameras under various real-world conditions, especially from sparse and\nnoisy events generated under non-uniform motion. It consists of two key\ncomponents: a realistic event generation model that accounts for various\nintrinsic parameters (e.g. time-independent, asymmetric threshold and\nrefractory period) and non-idealities (e.g. pixel-to-pixel threshold\nvariation), as well as a complementary pair of normalized reconstruction losses\nthat can effectively generalize to arbitrary speed profiles and intrinsic\nparameter values without such prior knowledge. Experiments on real and novel\nrealistically simulated sequences verify our effectiveness. Our code, synthetic\ndataset and improved event simulator are public.",
        "translated": ""
    },
    {
        "title": "Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes",
        "url": "http://arxiv.org/abs/2309.08588v1",
        "pub_date": "2023-09-15",
        "summary": "We present an approach to estimating camera rotation in crowded, real-world\nscenes from handheld monocular video. While camera rotation estimation is a\nwell-studied problem, no previous methods exhibit both high accuracy and\nacceptable speed in this setting. Because the setting is not addressed well by\nother datasets, we provide a new dataset and benchmark, with high-accuracy,\nrigorously verified ground truth, on 17 video sequences. Methods developed for\nwide baseline stereo (e.g., 5-point methods) perform poorly on monocular video.\nOn the other hand, methods used in autonomous driving (e.g., SLAM) leverage\nspecific sensor setups, specific motion models, or local optimization\nstrategies (lagging batch processing) and do not generalize well to handheld\nvideo. Finally, for dynamic scenes, commonly used robustification techniques\nlike RANSAC require large numbers of iterations, and become prohibitively slow.\nWe introduce a novel generalization of the Hough transform on SO(3) to\nefficiently and robustly find the camera rotation most compatible with optical\nflow. Among comparably fast methods, ours reduces error by almost 50\\% over the\nnext best, and is more accurate than any method, irrespective of speed. This\nrepresents a strong new performance point for crowded scenes, an important\nsetting for computer vision. The code and the dataset are available at\nhttps://fabiendelattre.com/robust-rotation-estimation.",
        "translated": ""
    },
    {
        "title": "Replacing softmax with ReLU in Vision Transformers",
        "url": "http://arxiv.org/abs/2309.08586v1",
        "pub_date": "2023-09-15",
        "summary": "Previous research observed accuracy degradation when replacing the attention\nsoftmax with a point-wise activation such as ReLU. In the context of vision\ntransformers, we find that this degradation is mitigated when dividing by\nsequence length. Our experiments training small to large vision transformers on\nImageNet-21k indicate that ReLU-attention can approach or match the performance\nof softmax-attention in terms of scaling behavior as a function of compute.",
        "translated": ""
    },
    {
        "title": "Viewpoint Integration and Registration with Vision Language Foundation\n  Model for Image Change Understanding",
        "url": "http://arxiv.org/abs/2309.08585v1",
        "pub_date": "2023-09-15",
        "summary": "Recently, the development of pre-trained vision language foundation models\n(VLFMs) has led to remarkable performance in many tasks. However, these models\ntend to have strong single-image understanding capability but lack the ability\nto understand multiple images. Therefore, they cannot be directly applied to\ncope with image change understanding (ICU), which requires models to capture\nactual changes between multiple images and describe them in language. In this\npaper, we discover that existing VLFMs perform poorly when applied directly to\nICU because of the following problems: (1) VLFMs generally learn the global\nrepresentation of a single image, while ICU requires capturing nuances between\nmultiple images. (2) The ICU performance of VLFMs is significantly affected by\nviewpoint variations, which is caused by the altered relationships between\nobjects when viewpoint changes. To address these problems, we propose a\nViewpoint Integration and Registration method. Concretely, we introduce a fused\nadapter image encoder that fine-tunes pre-trained encoders by inserting\ndesigned trainable adapters and fused adapters, to effectively capture nuances\nbetween image pairs. Additionally, a viewpoint registration flow and a semantic\nemphasizing module are designed to reduce the performance degradation caused by\nviewpoint variations in the visual and semantic space, respectively.\nExperimental results on CLEVR-Change and Spot-the-Diff demonstrate that our\nmethod achieves state-of-the-art performance in all metrics.",
        "translated": ""
    },
    {
        "title": "The Impact of Different Backbone Architecture on Autonomous Vehicle\n  Dataset",
        "url": "http://arxiv.org/abs/2309.08564v1",
        "pub_date": "2023-09-15",
        "summary": "Object detection is a crucial component of autonomous driving, and many\ndetection applications have been developed to address this task. These\napplications often rely on backbone architectures, which extract representation\nfeatures from inputs to perform the object detection task. The quality of the\nfeatures extracted by the backbone architecture can have a significant impact\non the overall detection performance. Many researchers have focused on\ndeveloping new and improved backbone architectures to enhance the efficiency\nand accuracy of object detection applications. While these backbone\narchitectures have shown state-of-the-art performance on generic object\ndetection datasets like MS-COCO and PASCAL-VOC, evaluating their performance\nunder an autonomous driving environment has not been previously explored. To\naddress this, our study evaluates three well-known autonomous vehicle datasets,\nnamely KITTI, NuScenes, and BDD, to compare the performance of different\nbackbone architectures on object detection tasks.",
        "translated": ""
    },
    {
        "title": "General In-Hand Object Rotation with Vision and Touch",
        "url": "http://arxiv.org/abs/2309.09979v1",
        "pub_date": "2023-09-18",
        "summary": "We introduce RotateIt, a system that enables fingertip-based object rotation\nalong multiple axes by leveraging multimodal sensory inputs. Our system is\ntrained in simulation, where it has access to ground-truth object shapes and\nphysical properties. Then we distill it to operate on realistic yet noisy\nsimulated visuotactile and proprioceptive sensory inputs. These multimodal\ninputs are fused via a visuotactile transformer, enabling online inference of\nobject shapes and physical properties during deployment. We show significant\nperformance improvements over prior methods and the importance of visual and\ntactile sensing.",
        "translated": ""
    },
    {
        "title": "GEDepth: Ground Embedding for Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2309.09975v1",
        "pub_date": "2023-09-18",
        "summary": "Monocular depth estimation is an ill-posed problem as the same 2D image can\nbe projected from infinite 3D scenes. Although the leading algorithms in this\nfield have reported significant improvement, they are essentially geared to the\nparticular compound of pictorial observations and camera parameters (i.e.,\nintrinsics and extrinsics), strongly limiting their generalizability in\nreal-world scenarios. To cope with this challenge, this paper proposes a novel\nground embedding module to decouple camera parameters from pictorial cues, thus\npromoting the generalization capability. Given camera parameters, the proposed\nmodule generates the ground depth, which is stacked with the input image and\nreferenced in the final depth prediction. A ground attention is designed in the\nmodule to optimally combine ground depth with residual depth. Our ground\nembedding is highly flexible and lightweight, leading to a plug-in module that\nis amenable to be integrated into various depth estimation networks.\nExperiments reveal that our approach achieves the state-of-the-art results on\npopular benchmarks, and more importantly, renders significant generalization\nimprovement on a wide range of cross-domain tests.",
        "translated": ""
    },
    {
        "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models",
        "url": "http://arxiv.org/abs/2309.09958v1",
        "pub_date": "2023-09-18",
        "summary": "Visual instruction tuning has recently shown encouraging progress with\nopen-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However,\nmost existing studies of open-source LMM are performed using models with 13B\nparameters or smaller. In this paper we present an empirical study of scaling\nLLaVA up to 33B and 65B/70B, and share our findings from our explorations in\nimage resolution, data mixing and parameter-efficient training methods such as\nLoRA/QLoRA. These are evaluated by their impact on the multi-modal and language\ncapabilities when completing real-world tasks in the wild.\n  We find that scaling LMM consistently enhances model performance and improves\nlanguage capabilities, and performance of LoRA/QLoRA tuning of LMM are\ncomparable to the performance of full-model fine-tuning. Additionally, the\nstudy highlights the importance of higher image resolutions and mixing\nmultimodal-language data to improve LMM performance, and visual instruction\ntuning can sometimes improve LMM's pure language capability. We hope that this\nstudy makes state-of-the-art LMM research at a larger scale more accessible,\nthus helping establish stronger baselines for future research. Code and\ncheckpoints will be made public.",
        "translated": ""
    },
    {
        "title": "vSHARP: variable Splitting Half-quadratic ADMM algorithm for\n  Reconstruction of inverse-Problems",
        "url": "http://arxiv.org/abs/2309.09954v1",
        "pub_date": "2023-09-18",
        "summary": "Medical Imaging (MI) tasks, such as accelerated Parallel Magnetic Resonance\nImaging (MRI), often involve reconstructing an image from noisy or incomplete\nmeasurements. This amounts to solving ill-posed inverse problems, where a\nsatisfactory closed-form analytical solution is not available. Traditional\nmethods such as Compressed Sensing (CS) in MRI reconstruction can be\ntime-consuming or prone to obtaining low-fidelity images. Recently, a plethora\nof supervised and self-supervised Deep Learning (DL) approaches have\ndemonstrated superior performance in inverse-problem solving, surpassing\nconventional methods. In this study, we propose vSHARP (variable Splitting\nHalf-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel\nDL-based method for solving ill-posed inverse problems arising in MI. vSHARP\nutilizes the Half-Quadratic Variable Splitting method and employs the\nAlternating Direction Method of Multipliers (ADMM) to unroll the optimization\nprocess. For data consistency, vSHARP unrolls a differentiable gradient descent\nprocess in the image domain, while a DL-based denoiser, such as a U-Net\narchitecture, is applied to enhance image quality. vSHARP also employs a\ndilated-convolution DL-based model to predict the Lagrange multipliers for the\nADMM initialization. We evaluate the proposed model by applying it to the task\nof accelerated Parallel MRI Reconstruction on two distinct datasets. We present\na comparative analysis of our experimental results with state-of-the-art\napproaches, highlighting the superior performance of vSHARP.",
        "translated": ""
    },
    {
        "title": "End-to-End Learned Event- and Image-based Visual Odometry",
        "url": "http://arxiv.org/abs/2309.09947v1",
        "pub_date": "2023-09-18",
        "summary": "Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. While standard RGB cameras\nstruggle in low-light or high-speed motion, event-based cameras offer high\ndynamic range and low latency. However, seamlessly integrating asynchronous\nevent data with synchronous frames remains challenging. We introduce RAMP-VO,\nthe first end-to-end learned event- and image-based VO system. It leverages\nnovel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders that are\n8x faster and 20% more accurate than existing asynchronous encoders. RAMP-VO\nfurther employs a novel pose forecasting technique to predict future poses for\ninitialization. Despite being trained only in simulation, RAMP-VO outperforms\nimage- and event-based methods by 52% and 20%, respectively, on traditional,\nreal-world benchmarks as well as newly introduced Apollo and Malapert landing\nsequences, paving the way for robust and asynchronous VO in space.",
        "translated": ""
    },
    {
        "title": "Assessing the capacity of a denoising diffusion probabilistic model to\n  reproduce spatial context",
        "url": "http://arxiv.org/abs/2309.10817v1",
        "pub_date": "2023-09-19",
        "summary": "Diffusion models have emerged as a popular family of deep generative models\n(DGMs). In the literature, it has been claimed that one class of diffusion\nmodels -- denoising diffusion probabilistic models (DDPMs) -- demonstrate\nsuperior image synthesis performance as compared to generative adversarial\nnetworks (GANs). To date, these claims have been evaluated using either\nensemble-based methods designed for natural images, or conventional measures of\nimage quality such as structural similarity. However, there remains an\nimportant need to understand the extent to which DDPMs can reliably learn\nmedical imaging domain-relevant information, which is referred to as `spatial\ncontext' in this work. To address this, a systematic assessment of the ability\nof DDPMs to learn spatial context relevant to medical imaging applications is\nreported for the first time. A key aspect of the studies is the use of\nstochastic context models (SCMs) to produce training data. In this way, the\nability of the DDPMs to reliably reproduce spatial context can be\nquantitatively assessed by use of post-hoc image analyses. Error-rates in\nDDPM-generated ensembles are reported, and compared to those corresponding to a\nmodern GAN. The studies reveal new and important insights regarding the\ncapacity of DDPMs to learn spatial context. Notably, the results demonstrate\nthat DDPMs hold significant capacity for generating contextually correct images\nthat are `interpolated' between training samples, which may benefit\ndata-augmentation tasks in ways that GANs cannot.",
        "translated": ""
    },
    {
        "title": "PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes",
        "url": "http://arxiv.org/abs/2309.10815v1",
        "pub_date": "2023-09-19",
        "summary": "Training perception systems for self-driving cars requires substantial\nannotations. However, manual labeling in 2D images is highly labor-intensive.\nWhile existing datasets provide rich annotations for pre-recorded sequences,\nthey fall short in labeling rarely encountered viewpoints, potentially\nhampering the generalization ability for perception models. In this paper, we\npresent PanopticNeRF-360, a novel approach that combines coarse 3D annotations\nwith noisy 2D semantic cues to generate consistent panoptic labels and\nhigh-quality images from any viewpoint. Our key insight lies in exploiting the\ncomplementarity of 3D and 2D priors to mutually enhance geometry and semantics.\nSpecifically, we propose to leverage noisy semantic and instance labels in both\n3D and 2D spaces to guide geometry optimization. Simultaneously, the improved\ngeometry assists in filtering noise present in the 3D and 2D annotations by\nmerging them in 3D space via a learned semantic field. To further enhance\nappearance, we combine MLP and hash grids to yield hybrid scene features,\nstriking a balance between high-frequency appearance and predominantly\ncontiguous semantics. Our experiments demonstrate PanopticNeRF-360's\nstate-of-the-art performance over existing label transfer methods on the\nchallenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360\nenables omnidirectional rendering of high-fidelity, multi-view and\nspatiotemporally consistent appearance, semantic and instance labels. We make\nour code and data available at https://github.com/fuxiao0719/PanopticNeRF",
        "translated": ""
    },
    {
        "title": "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via\n  Partial Guidance",
        "url": "http://arxiv.org/abs/2309.10810v1",
        "pub_date": "2023-09-19",
        "summary": "Exploiting pre-trained diffusion models for restoration has recently become a\nfavored alternative to the traditional task-specific training approach.\nPrevious works have achieved noteworthy success by limiting the solution space\nusing explicit degradation models. However, these methods often fall short when\nfaced with complex degradations as they generally cannot be precisely modeled.\nIn this paper, we propose PGDiff by introducing partial guidance, a fresh\nperspective that is more adaptable to real-world degradations compared to\nexisting works. Rather than specifically defining the degradation process, our\napproach models the desired properties, such as image structure and color\nstatistics of high-quality images, and applies this guidance during the reverse\ndiffusion process. These properties are readily available and make no\nassumptions about the degradation process. When combined with a diffusion\nprior, this partial guidance can deliver appealing results across a range of\nrestoration tasks. Additionally, PGDiff can be extended to handle composite\ntasks by consolidating multiple high-quality image properties, achieved by\nintegrating the guidance from respective tasks. Experimental results\ndemonstrate that our method not only outperforms existing diffusion-prior-based\napproaches but also competes favorably with task-specific models.",
        "translated": ""
    },
    {
        "title": "Multi-Context Dual Hyper-Prior Neural Image Compression",
        "url": "http://arxiv.org/abs/2309.10799v1",
        "pub_date": "2023-09-19",
        "summary": "Transform and entropy models are the two core components in deep image\ncompression neural networks. Most existing learning-based image compression\nmethods utilize convolutional-based transform, which lacks the ability to model\nlong-range dependencies, primarily due to the limited receptive field of the\nconvolution operation. To address this limitation, we propose a\nTransformer-based nonlinear transform. This transform has the remarkable\nability to efficiently capture both local and global information from the input\nimage, leading to a more decorrelated latent representation. In addition, we\nintroduce a novel entropy model that incorporates two different hyperpriors to\nmodel cross-channel and spatial dependencies of the latent representation. To\nfurther improve the entropy model, we add a global context that leverages\ndistant relationships to predict the current latent more accurately. This\nglobal context employs a causal attention mechanism to extract long-range\ninformation in a content-dependent manner. Our experiments show that our\nproposed framework performs better than the state-of-the-art methods in terms\nof rate-distortion performance.",
        "translated": ""
    },
    {
        "title": "Multi-spectral Entropy Constrained Neural Compression of Solar Imagery",
        "url": "http://arxiv.org/abs/2309.10791v1",
        "pub_date": "2023-09-19",
        "summary": "Missions studying the dynamic behaviour of the Sun are defined to capture\nmulti-spectral images of the sun and transmit them to the ground station in a\ndaily basis. To make transmission efficient and feasible, image compression\nsystems need to be exploited. Recently successful end-to-end optimized neural\nnetwork-based image compression systems have shown great potential to be used\nin an ad-hoc manner. In this work we have proposed a transformer-based\nmulti-spectral neural image compressor to efficiently capture redundancies both\nintra/inter-wavelength. To unleash the locality of window-based self attention\nmechanism, we propose an inter-window aggregated token multi head self\nattention. Additionally to make the neural compressor autoencoder shift\ninvariant, a randomly shifted window attention mechanism is used which makes\nthe transformer blocks insensitive to translations in their input domain. We\ndemonstrate that the proposed approach not only outperforms the conventional\ncompression algorithms but also it is able to better decorrelates images along\nthe multiple wavelengths compared to single spectral compression.",
        "translated": ""
    },
    {
        "title": "A Large-scale Dataset for Audio-Language Representation Learning",
        "url": "http://arxiv.org/abs/2309.11500v1",
        "pub_date": "2023-09-20",
        "summary": "The AI community has made significant strides in developing powerful\nfoundation models, driven by large-scale multimodal datasets. However, in the\naudio representation learning community, the present audio-language datasets\nsuffer from limitations such as insufficient volume, simplistic content, and\narduous collection procedures. To tackle these challenges, we present an\ninnovative and automatic audio caption generation pipeline based on a series of\npublic tools or APIs, and construct a large-scale, high-quality, audio-language\ndataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. To\ndemonstrate the effectiveness of the proposed dataset, we train popular models\non our dataset and show performance improvement on various downstream tasks,\nnamely, audio-language retrieval, audio captioning, environment classification.\nIn addition, we establish a novel test set and provide a benchmark for\naudio-text tasks. The proposed dataset will be released at\nhttps://auto-acd.github.io/.",
        "translated": ""
    },
    {
        "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
        "url": "http://arxiv.org/abs/2309.11499v1",
        "pub_date": "2023-09-20",
        "summary": "This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.",
        "translated": ""
    },
    {
        "title": "FreeU: Free Lunch in Diffusion U-Net",
        "url": "http://arxiv.org/abs/2309.11497v1",
        "pub_date": "2023-09-20",
        "summary": "In this paper, we uncover the untapped potential of diffusion U-Net, which\nserves as a \"free lunch\" that substantially improves the generation quality on\nthe fly. We initially investigate the key contributions of the U-Net\narchitecture to the denoising process and identify that its main backbone\nprimarily contributes to denoising, whereas its skip connections mainly\nintroduce high-frequency features into the decoder module, causing the network\nto overlook the backbone semantics. Capitalizing on this discovery, we propose\na simple yet effective method-termed \"FreeU\" - that enhances generation quality\nwithout additional training or finetuning. Our key insight is to strategically\nre-weight the contributions sourced from the U-Net's skip connections and\nbackbone feature maps, to leverage the strengths of both components of the\nU-Net architecture. Promising results on image and video generation tasks\ndemonstrate that our FreeU can be readily integrated to existing diffusion\nmodels, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion,\nto improve the generation quality with only a few lines of code. All you need\nis to adjust two scaling factors during inference. Project page:\nhttps://chenyangsi.top/FreeU/.",
        "translated": ""
    },
    {
        "title": "Budget-Aware Pruning: Handling Multiple Domains with Less Parameters",
        "url": "http://arxiv.org/abs/2309.11464v1",
        "pub_date": "2023-09-20",
        "summary": "Deep learning has achieved state-of-the-art performance on several computer\nvision tasks and domains. Nevertheless, it still has a high computational cost\nand demands a significant amount of parameters. Such requirements hinder the\nuse in resource-limited environments and demand both software and hardware\noptimization. Another limitation is that deep models are usually specialized\ninto a single domain or task, requiring them to learn and store new parameters\nfor each new one. Multi-Domain Learning (MDL) attempts to solve this problem by\nlearning a single model that is capable of performing well in multiple domains.\nNevertheless, the models are usually larger than the baseline for a single\ndomain. This work tackles both of these problems: our objective is to prune\nmodels capable of handling multiple domains according to a user-defined budget,\nmaking them more computationally affordable while keeping a similar\nclassification performance. We achieve this by encouraging all domains to use a\nsimilar subset of filters from the baseline model, up to the amount defined by\nthe user's budget. Then, filters that are not used by any domain are pruned\nfrom the network. The proposed approach innovates by better adapting to\nresource-limited devices while, to our knowledge, being the only work that\nhandles multiple domains at test time with fewer parameters and lower\ncomputational complexity than the baseline model for a single domain.",
        "translated": ""
    },
    {
        "title": "Weight Averaging Improves Knowledge Distillation under Domain Shift",
        "url": "http://arxiv.org/abs/2309.11446v1",
        "pub_date": "2023-09-20",
        "summary": "Knowledge distillation (KD) is a powerful model compression technique broadly\nused in practical deep learning applications. It is focused on training a small\nstudent network to mimic a larger teacher network. While it is widely known\nthat KD can offer an improvement to student generalization in i.i.d setting,\nits performance under domain shift, i.e. the performance of student networks on\ndata from domains unseen during training, has received little attention in the\nliterature. In this paper we make a step towards bridging the research fields\nof knowledge distillation and domain generalization. We show that weight\naveraging techniques proposed in domain generalization literature, such as SWAD\nand SMA, also improve the performance of knowledge distillation under domain\nshift. In addition, we propose a simplistic weight averaging strategy that does\nnot require evaluation on validation data during training and show that it\nperforms on par with SWAD and SMA when applied to KD. We name our final\ndistillation approach Weight-Averaged Knowledge Distillation (WAKD).",
        "translated": ""
    },
    {
        "title": "Active Stereo Without Pattern Projector",
        "url": "http://arxiv.org/abs/2309.12315v1",
        "pub_date": "2023-09-21",
        "summary": "This paper proposes a novel framework integrating the principles of active\nstereo in standard passive camera systems without a physical pattern projector.\nWe virtually project a pattern over the left and right images according to the\nsparse measurements obtained from a depth sensor. Any such devices can be\nseamlessly plugged into our framework, allowing for the deployment of a virtual\nactive stereo setup in any possible environment, overcoming the limitation of\npattern projectors, such as limited working range or environmental conditions.\nExperiments on indoor/outdoor datasets, featuring both long and close-range,\nsupport the seamless effectiveness of our approach, boosting the accuracy of\nboth stereo algorithms and deep networks.",
        "translated": ""
    },
    {
        "title": "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight\n  Inheritance",
        "url": "http://arxiv.org/abs/2309.12314v1",
        "pub_date": "2023-09-21",
        "summary": "In this paper, we propose a novel cross-modal distillation method, called\nTinyCLIP, for large-scale language-image pre-trained models. The method\nintroduces two core techniques: affinity mimicking and weight inheritance.\nAffinity mimicking explores the interaction between modalities during\ndistillation, enabling student models to mimic teachers' behavior of learning\ncross-modal feature alignment in a visual-linguistic affinity space. Weight\ninheritance transmits the pre-trained weights from the teacher models to their\nstudent counterparts to improve distillation efficiency. Moreover, we extend\nthe method into a multi-stage progressive distillation to mitigate the loss of\ninformative weights during extreme compression. Comprehensive experiments\ndemonstrate the efficacy of TinyCLIP, showing that it can reduce the size of\nthe pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot\nperformance. While aiming for comparable performance, distillation with weight\ninheritance can speed up the training by 1.4 - 7.8 $\\times$ compared to\ntraining from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,\nachieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,\nsurpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%\nparameters. Finally, we demonstrate the good transferability of TinyCLIP in\nvarious downstream tasks. Code and models will be open-sourced at\nhttps://aka.ms/tinyclip.",
        "translated": ""
    },
    {
        "title": "ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals",
        "url": "http://arxiv.org/abs/2309.12312v1",
        "pub_date": "2023-09-21",
        "summary": "We present ForceSight, a system for text-guided mobile manipulation that\npredicts visual-force goals using a deep neural network. Given a single RGBD\nimage combined with a text prompt, ForceSight determines a target end-effector\npose in the camera frame (kinematic goal) and the associated forces (force\ngoal). Together, these two components form a visual-force goal. Prior work has\ndemonstrated that deep models outputting human-interpretable kinematic goals\ncan enable dexterous manipulation by real robots. Forces are critical to\nmanipulation, yet have typically been relegated to lower-level execution in\nthese systems. When deployed on a mobile manipulator equipped with an\neye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,\ndrawer opening, and object handovers with an 81% success rate in unseen\nenvironments with object instances that differed significantly from the\ntraining data. In a separate experiment, relying exclusively on visual servoing\nand ignoring force goals dropped the success rate from 90% to 45%,\ndemonstrating that force goals can significantly enhance performance. The\nappendix, videos, code, and trained models are available at\nhttps://force-sight.github.io/.",
        "translated": ""
    },
    {
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language\n  Model as an Agent",
        "url": "http://arxiv.org/abs/2309.12311v1",
        "pub_date": "2023-09-21",
        "summary": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .",
        "translated": ""
    },
    {
        "title": "TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive\n  Learning",
        "url": "http://arxiv.org/abs/2309.12306v1",
        "pub_date": "2023-09-21",
        "summary": "The goal of this work is Active Speaker Detection (ASD), a task to determine\nwhether a person is speaking or not in a series of video frames. Previous works\nhave dealt with the task by exploring network architectures while learning\neffective representations has been less explored. In this work, we propose\nTalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part\nof the full segments where a person on the screen is actually speaking. This\nencourages the model to learn effective representations through the natural\ncorrespondence of speech and facial movements. Our loss can be jointly\noptimized with the existing objectives for training ASD models without the need\nfor additional supervision or training data. The experiments demonstrate that\nour loss can be easily integrated into the existing ASD frameworks, improving\ntheir performance. Our method achieves state-of-the-art performances on\nAVA-ActiveSpeaker and ASW datasets.",
        "translated": ""
    },
    {
        "title": "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary\n  Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.13042v1",
        "pub_date": "2023-09-22",
        "summary": "We present MosaicFusion, a simple yet effective diffusion-based data\naugmentation approach for large vocabulary instance segmentation. Our method is\ntraining-free and does not rely on any label supervision. Two key designs\nenable us to employ an off-the-shelf text-to-image diffusion model as a useful\ndataset generator for object instances and mask annotations. First, we divide\nan image canvas into several regions and perform a single round of diffusion\nprocess to generate multiple instances simultaneously, conditioning on\ndifferent text prompts. Second, we obtain corresponding instance masks by\naggregating cross-attention maps associated with object prompts across layers\nand diffusion time steps, followed by simple thresholding and edge-aware\nrefinement processing. Without bells and whistles, our MosaicFusion can produce\na significant amount of synthetic labeled data for both rare and novel\ncategories. Experimental results on the challenging LVIS long-tailed and\nopen-vocabulary benchmarks demonstrate that MosaicFusion can significantly\nimprove the performance of existing instance segmentation models, especially\nfor rare and novel categories. Code will be released at\nhttps://github.com/Jiahao000/MosaicFusion.",
        "translated": ""
    },
    {
        "title": "Robotic Offline RL from Internet Videos via Value-Function Pre-Training",
        "url": "http://arxiv.org/abs/2309.13041v1",
        "pub_date": "2023-09-22",
        "summary": "Pre-training on Internet data has proven to be a key ingredient for broad\ngeneralization in many modern ML systems. What would it take to enable such\ncapabilities in robotic reinforcement learning (RL)? Offline RL methods, which\nlearn from datasets of robot experience, offer one way to leverage prior data\ninto the robotic learning pipeline. However, these methods have a \"type\nmismatch\" with video data (such as Ego4D), the largest prior datasets available\nfor robotics, since video offers observation-only experience without the action\nor reward annotations needed for RL methods. In this paper, we develop a system\nfor leveraging large-scale human video datasets in robotic offline RL, based\nentirely on learning value functions via temporal-difference learning. We show\nthat value learning on video datasets learns representations that are more\nconducive to downstream robotic offline RL than other approaches for learning\nfrom video data. Our system, called V-PTR, combines the benefits of\npre-training on video data with robotic offline RL approaches that train on\ndiverse robot data, resulting in value functions and policies for manipulation\ntasks that perform better, act robustly, and generalize broadly. On several\nmanipulation tasks on a real WidowX robot, our framework produces policies that\ngreatly improve over prior methods. Our video and additional details can be\nfound at https://dibyaghosh.com/vptr/",
        "translated": ""
    },
    {
        "title": "NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular\n  Objects with Neural Refractive-Reflective Fields",
        "url": "http://arxiv.org/abs/2309.13039v1",
        "pub_date": "2023-09-22",
        "summary": "Neural radiance fields (NeRF) have revolutionized the field of image-based\nview synthesis. However, NeRF uses straight rays and fails to deal with\ncomplicated light path changes caused by refraction and reflection. This\nprevents NeRF from successfully synthesizing transparent or specular objects,\nwhich are ubiquitous in real-world robotics and A/VR applications. In this\npaper, we introduce the refractive-reflective field. Taking the object\nsilhouette as input, we first utilize marching tetrahedra with a progressive\nencoding to reconstruct the geometry of non-Lambertian objects and then model\nrefraction and reflection effects of the object in a unified framework using\nFresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we\npropose a virtual cone supersampling technique. We benchmark our method on\ndifferent shapes, backgrounds and Fresnel terms on both real-world and\nsynthetic datasets. We also qualitatively and quantitatively benchmark the\nrendering results of various editing applications, including material editing,\nobject replacement/insertion, and environment illumination estimation. Codes\nand data are publicly available at https://github.com/dawning77/NeRRF.",
        "translated": ""
    },
    {
        "title": "Privacy Assessment on Reconstructed Images: Are Existing Evaluation\n  Metrics Faithful to Human Perception?",
        "url": "http://arxiv.org/abs/2309.13038v1",
        "pub_date": "2023-09-22",
        "summary": "Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used\nto evaluate model privacy risk under reconstruction attacks. Under these\nmetrics, reconstructed images that are determined to resemble the original one\ngenerally indicate more privacy leakage. Images determined as overall\ndissimilar, on the other hand, indicate higher robustness against attack.\nHowever, there is no guarantee that these metrics well reflect human opinions,\nwhich, as a judgement for model privacy leakage, are more trustworthy. In this\npaper, we comprehensively study the faithfulness of these hand-crafted metrics\nto human perception of privacy information from the reconstructed images. On 5\ndatasets ranging from natural images, faces, to fine-grained classes, we use 4\nexisting attack methods to reconstruct images from many different\nclassification models and, for each reconstructed image, we ask multiple human\nannotators to assess whether this image is recognizable. Our studies reveal\nthat the hand-crafted metrics only have a weak correlation with the human\nevaluation of privacy leakage and that even these metrics themselves often\ncontradict each other. These observations suggest risks of current metrics in\nthe community. To address this potential risk, we propose a learning-based\nmeasure called SemSim to evaluate the Semantic Similarity between the original\nand reconstructed images. SemSim is trained with a standard triplet loss, using\nan original image as an anchor, one of its recognizable reconstructed images as\na positive sample, and an unrecognizable one as a negative. By training on\nhuman annotations, SemSim exhibits a greater reflection of privacy leakage on\nthe semantic level. We show that SemSim has a significantly higher correlation\nwith human judgment compared with existing metrics. Moreover, this strong\ncorrelation generalizes to unseen datasets, models and attack methods.",
        "translated": ""
    },
    {
        "title": "Performance Analysis of UNet and Variants for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2309.13013v1",
        "pub_date": "2023-09-22",
        "summary": "Medical imaging plays a crucial role in modern healthcare by providing\nnon-invasive visualisation of internal structures and abnormalities, enabling\nearly disease detection, accurate diagnosis, and treatment planning. This study\naims to explore the application of deep learning models, particularly focusing\non the UNet architecture and its variants, in medical image segmentation. We\nseek to evaluate the performance of these models across various challenging\nmedical image segmentation tasks, addressing issues such as image\nnormalization, resizing, architecture choices, loss function design, and\nhyperparameter tuning. The findings reveal that the standard UNet, when\nextended with a deep network layer, is a proficient medical image segmentation\nmodel, while the Res-UNet and Attention Res-UNet architectures demonstrate\nsmoother convergence and superior performance, particularly when handling fine\nimage details. The study also addresses the challenge of high class imbalance\nthrough careful preprocessing and loss function definitions. We anticipate that\nthe results of this study will provide useful insights for researchers seeking\nto apply these models to new medical imaging problems and offer guidance and\nbest practices for their implementation.",
        "translated": ""
    },
    {
        "title": "Extreme Parkour with Legged Robots",
        "url": "http://arxiv.org/abs/2309.14341v1",
        "pub_date": "2023-09-25",
        "summary": "Humans can perform parkour by traversing obstacles in a highly dynamic\nfashion requiring precise eye-muscle coordination and movement. Getting robots\nto do the same task requires overcoming similar challenges. Classically, this\nis done by independently engineering perception, actuation, and control systems\nto very low tolerances. This restricts them to tightly controlled settings such\nas a predetermined obstacle course in labs. In contrast, humans are able to\nlearn parkour through practice without significantly changing their underlying\nbiology. In this paper, we take a similar approach to developing robot parkour\non a small low-cost robot with imprecise actuation and a single front-facing\ndepth camera for perception which is low-frequency, jittery, and prone to\nartifacts. We show how a single neural net policy operating directly from a\ncamera image, trained in simulation with large-scale RL, can overcome imprecise\nsensing and actuation to output highly precise control behavior end-to-end. We\nshow our robot can perform a high jump on obstacles 2x its height, long jump\nacross gaps 2x its length, do a handstand and run across tilted ramps, and\ngeneralize to novel obstacle courses with different physical properties.\nParkour videos at https://extreme-parkour.github.io/",
        "translated": ""
    },
    {
        "title": "Chop &amp; Learn: Recognizing and Generating Object-State Compositions",
        "url": "http://arxiv.org/abs/2309.14339v1",
        "pub_date": "2023-09-25",
        "summary": "Recognizing and generating object-state compositions has been a challenging\ntask, especially when generalizing to unseen compositions. In this paper, we\nstudy the task of cutting objects in different styles and the resulting object\nstate changes. We propose a new benchmark suite Chop &amp; Learn, to accommodate\nthe needs of learning objects and different cut styles using multiple\nviewpoints. We also propose a new task of Compositional Image Generation, which\ncan transfer learned cut styles to different objects, by generating novel\nobject-state images. Moreover, we also use the videos for Compositional Action\nRecognition, and show valuable uses of this dataset for multiple video tasks.\nProject website: https://chopnlearn.github.io.",
        "translated": ""
    },
    {
        "title": "3D Indoor Instance Segmentation in an Open-World",
        "url": "http://arxiv.org/abs/2309.14338v1",
        "pub_date": "2023-09-25",
        "summary": "Existing 3D instance segmentation methods typically assume that all semantic\nclasses to be segmented would be available during training and only seen\ncategories are segmented at inference. We argue that such a closed-world\nassumption is restrictive and explore for the first time 3D indoor instance\nsegmentation in an open-world setting, where the model is allowed to\ndistinguish a set of known classes as well as identify an unknown object as\nunknown and then later incrementally learning the semantic category of the\nunknown when the corresponding category labels are available. To this end, we\nintroduce an open-world 3D indoor instance segmentation method, where an\nauto-labeling scheme is employed to produce pseudo-labels during training and\ninduce separation to separate known and unknown category labels. We further\nimprove the pseudo-labels quality at inference by adjusting the unknown class\nprobability based on the objectness score distribution. We also introduce\ncarefully curated open-world splits leveraging realistic scenarios based on\ninherent object distribution, region-based indoor scene exploration and\nrandomness aspect of open-world classes. Extensive experiments reveal the\nefficacy of the proposed contributions leading to promising open-world 3D\ninstance segmentation performance.",
        "translated": ""
    },
    {
        "title": "UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human\n  Generation",
        "url": "http://arxiv.org/abs/2309.14335v1",
        "pub_date": "2023-09-25",
        "summary": "Human generation has achieved significant progress. Nonetheless, existing\nmethods still struggle to synthesize specific regions such as faces and hands.\nWe argue that the main reason is rooted in the training data. A holistic human\ndataset inevitably has insufficient and low-resolution information on local\nparts. Therefore, we propose to use multi-source datasets with various\nresolution images to jointly learn a high-resolution human generative model.\nHowever, multi-source data inherently a) contains different parts that do not\nspatially align into a coherent human, and b) comes with different scales. To\ntackle these challenges, we propose an end-to-end framework, UnitedHuman, that\nempowers continuous GAN with the ability to effectively utilize multi-source\ndata for high-resolution human generation. Specifically, 1) we design a\nMulti-Source Spatial Transformer that spatially aligns multi-source images to\nfull-body space with a human parametric model. 2) Next, a continuous GAN is\nproposed with global-structural guidance and CutMix consistency. Patches from\ndifferent datasets are then sampled and transformed to supervise the training\nof this scale-invariant generative model. Extensive experiments demonstrate\nthat our model jointly learned from multi-source data achieves superior quality\nthan those learned from a holistic dataset.",
        "translated": ""
    },
    {
        "title": "Noise-in, Bias-out: Balanced and Real-time MoCap Solving",
        "url": "http://arxiv.org/abs/2309.14330v1",
        "pub_date": "2023-09-25",
        "summary": "Real-time optical Motion Capture (MoCap) systems have not benefited from the\nadvances in modern data-driven modeling. In this work we apply machine learning\nto solve noisy unstructured marker estimates in real-time and deliver robust\nmarker-based MoCap even when using sparse affordable sensors. To achieve this\nwe focus on a number of challenges related to model training, namely the\nsourcing of training data and their long-tailed distribution. Leveraging\nrepresentation learning we design a technique for imbalanced regression that\nrequires no additional data or labels and improves the performance of our model\nin rare and challenging poses. By relying on a unified representation, we show\nthat training such a model is not bound to high-end MoCap training data\nacquisition, and exploit the advances in marker-less MoCap to acquire the\nnecessary data. Finally, we take a step towards richer and affordable MoCap by\nadapting a body model-based inverse kinematics solution to account for\nmeasurement and inference uncertainty, further improving performance and\nrobustness. Project page: https://moverseai.github.io/noise-tail",
        "translated": ""
    },
    {
        "title": "Generating Visual Scenes from Touch",
        "url": "http://arxiv.org/abs/2309.15117v1",
        "pub_date": "2023-09-26",
        "summary": "An emerging line of work has sought to generate plausible imagery from touch.\nExisting approaches, however, tackle only narrow aspects of the visuo-tactile\nsynthesis problem, and lag significantly behind the quality of cross-modal\nsynthesis methods in other domains. We draw on recent advances in latent\ndiffusion to create a model for synthesizing images from tactile signals (and\nvice versa) and apply it to a number of visuo-tactile synthesis tasks. Using\nthis model, we significantly outperform prior work on the tactile-driven\nstylization problem, i.e., manipulating an image to match a touch signal, and\nwe are the first to successfully generate images from touch without additional\nsources of information about the scene. We also successfully use our model to\naddress two novel synthesis problems: generating images that do not contain the\ntouch sensor or the hand holding it, and estimating an image's shading from its\nreflectance and touch.",
        "translated": ""
    },
    {
        "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced\n  Text-image Comprehension and Composition",
        "url": "http://arxiv.org/abs/2309.15112v1",
        "pub_date": "2023-09-26",
        "summary": "We propose InternLM-XComposer, a vision-language large model that enables\nadvanced image-text comprehension and composition. The innovative nature of our\nmodel is highlighted by three appealing properties: 1) Interleaved Text-Image\nComposition: InternLM-XComposer can effortlessly generate coherent and\ncontextual articles that seamlessly integrate images, providing a more engaging\nand immersive reading experience. Simply provide a title, and our system will\ngenerate the corresponding manuscript. It can intelligently identify the areas\nin the text where images would enhance the content and automatically insert the\nmost appropriate visual candidates. 2) Comprehension with Rich Multilingual\nKnowledge: The text-image comprehension is empowered by training on extensive\nmulti-modal multilingual concepts with carefully crafted strategies, resulting\nin a deep understanding of visual content. 3) State-of-the-art Performance: Our\nmodel consistently achieves state-of-the-art results across various mainstream\nbenchmarks for vision-language foundational models, including MME Benchmark,\nMMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).\nCollectively, InternLM-XComposer seamlessly blends advanced text-image\ncomprehension and composition, revolutionizing vision-language interaction and\noffering new insights and opportunities. The InternLM-XComposer models with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "translated": ""
    },
    {
        "title": "Doduo: Learning Dense Visual Correspondence from Unsupervised\n  Semantic-Aware Flow",
        "url": "http://arxiv.org/abs/2309.15110v1",
        "pub_date": "2023-09-26",
        "summary": "Dense visual correspondence plays a vital role in robotic perception. This\nwork focuses on establishing the dense correspondence between a pair of images\nthat captures dynamic scenes undergoing substantial transformations. We\nintroduce Doduo to learn general dense visual correspondence from in-the-wild\nimages and videos without ground truth supervision. Given a pair of images, it\nestimates the dense flow field encoding the displacement of each pixel in one\nimage to its corresponding pixel in the other image. Doduo uses flow-based\nwarping to acquire supervisory signals for the training. Incorporating semantic\npriors with self-supervised flow training, Doduo produces accurate dense\ncorrespondence robust to the dynamic changes of the scenes. Trained on an\nin-the-wild video dataset, Doduo illustrates superior performance on\npoint-level correspondence estimation over existing self-supervised\ncorrespondence learning baselines. We also apply Doduo to articulation\nestimation and zero-shot goal-conditioned manipulation, underlining its\npractical applications in robotics. Code and additional visualizations are\navailable at https://ut-austin-rpl.github.io/Doduo",
        "translated": ""
    },
    {
        "title": "DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal\n  Knowledge Distillation",
        "url": "http://arxiv.org/abs/2309.15109v1",
        "pub_date": "2023-09-26",
        "summary": "3D perception based on the representations learned from multi-camera\nbird's-eye-view (BEV) is trending as cameras are cost-effective for mass\nproduction in autonomous driving industry. However, there exists a distinct\nperformance gap between multi-camera BEV and LiDAR based 3D object detection.\nOne key reason is that LiDAR captures accurate depth and other geometry\nmeasurements, while it is notoriously challenging to infer such 3D information\nfrom merely image input. In this work, we propose to boost the representation\nlearning of a multi-camera BEV based student detector by training it to imitate\nthe features of a well-trained LiDAR based teacher detector. We propose\neffective balancing strategy to enforce the student to focus on learning the\ncrucial features from the teacher, and generalize knowledge transfer to\nmulti-scale layers with temporal fusion. We conduct extensive evaluations on\nmultiple representative models of multi-camera BEV. Experiments reveal that our\napproach renders significant improvement over the student models, leading to\nthe state-of-the-art performance on the popular benchmark nuScenes.",
        "translated": ""
    },
    {
        "title": "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2309.15103v1",
        "pub_date": "2023-09-26",
        "summary": "This work aims to learn a high-quality text-to-video (T2V) generative model\nby leveraging a pre-trained text-to-image (T2I) model as a basis. It is a\nhighly desirable yet challenging task to simultaneously a) accomplish the\nsynthesis of visually realistic and temporally coherent videos while b)\npreserving the strong creative generation nature of the pre-trained T2I model.\nTo this end, we propose LaVie, an integrated video generation framework that\noperates on cascaded video latent diffusion models, comprising a base T2V\nmodel, a temporal interpolation model, and a video super-resolution model. Our\nkey insights are two-fold: 1) We reveal that the incorporation of simple\ntemporal self-attentions, coupled with rotary positional encoding, adequately\ncaptures the temporal correlations inherent in video data. 2) Additionally, we\nvalidate that the process of joint image-video fine-tuning plays a pivotal role\nin producing high-quality and creative outcomes. To enhance the performance of\nLaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,\nconsisting of 25 million text-video pairs that prioritize quality, diversity,\nand aesthetic appeal. Extensive experiments demonstrate that LaVie achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nFurthermore, we showcase the versatility of pre-trained LaVie models in various\nlong video generation and personalized video synthesis applications.",
        "translated": ""
    },
    {
        "title": "SHACIRA: Scalable HAsh-grid Compression for Implicit Neural\n  Representations",
        "url": "http://arxiv.org/abs/2309.15848v1",
        "pub_date": "2023-09-27",
        "summary": "Implicit Neural Representations (INR) or neural fields have emerged as a\npopular framework to encode multimedia signals such as images and radiance\nfields while retaining high-quality. Recently, learnable feature grids proposed\nby Instant-NGP have allowed significant speed-up in the training as well as the\nsampling of INRs by replacing a large neural network with a multi-resolution\nlook-up table of feature vectors and a much smaller neural network. However,\nthese feature grids come at the expense of large memory consumption which can\nbe a bottleneck for storage and streaming applications. In this work, we\npropose SHACIRA, a simple yet effective task-agnostic framework for compressing\nsuch feature grids with no additional post-hoc pruning/quantization stages. We\nreparameterize feature grids with quantized latent weights and apply entropy\nregularization in the latent space to achieve high levels of compression across\nvarious domains. Quantitative and qualitative results on diverse datasets\nconsisting of images, videos, and radiance fields, show that our approach\noutperforms existing INR approaches without the need for any large datasets or\ndomain-specific heuristics. Our project page is available at\nhttp://shacira.github.io .",
        "translated": ""
    },
    {
        "title": "Exploiting the Signal-Leak Bias in Diffusion Models",
        "url": "http://arxiv.org/abs/2309.15842v1",
        "pub_date": "2023-09-27",
        "summary": "There is a bias in the inference pipeline of most diffusion models. This bias\narises from a signal leak whose distribution deviates from the noise\ndistribution, creating a discrepancy between training and inference processes.\nWe demonstrate that this signal-leak bias is particularly significant when\nmodels are tuned to a specific style, causing sub-optimal style matching.\nRecent research tries to avoid the signal leakage during training. We instead\nshow how we can exploit this signal-leak bias in existing diffusion models to\nallow more control over the generated images. This enables us to generate\nimages with more varied brightness, and images that better match a desired\nstyle or color. By modeling the distribution of the signal leak in the spatial\nfrequency and pixel domains, and including a signal leak in the initial latent,\nwe generate images that better match expected results without any additional\ntraining.",
        "translated": ""
    },
    {
        "title": "OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs",
        "url": "http://arxiv.org/abs/2309.15830v1",
        "pub_date": "2023-09-27",
        "summary": "We present a new method for generating realistic and view-consistent images\nwith fine geometry from 2D image collections. Our method proposes a hybrid\nexplicit-implicit representation called \\textbf{OrthoPlanes}, which encodes\nfine-grained 3D information in feature maps that can be efficiently generated\nby modifying 2D StyleGANs. Compared to previous representations, our method has\nbetter scalability and expressiveness with clear and explicit information. As a\nresult, our method can handle more challenging view-angles and synthesize\narticulated objects with high spatial degree of freedom. Experiments\ndemonstrate that our method achieves state-of-the-art results on FFHQ and SHHQ\ndatasets, both quantitatively and qualitatively. Project page:\n\\url{https://orthoplanes.github.io/}.",
        "translated": ""
    },
    {
        "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video\n  Generation",
        "url": "http://arxiv.org/abs/2309.15818v1",
        "pub_date": "2023-09-27",
        "summary": "Significant advancements have been achieved in the realm of large-scale\npre-trained text-to-video Diffusion Models (VDMs). However, previous methods\neither rely solely on pixel-based VDMs, which come with high computational\ncosts, or on latent-based VDMs, which often struggle with precise text-video\nalignment. In this paper, we are the first to propose a hybrid model, dubbed as\nShow-1, which marries pixel-based and latent-based VDMs for text-to-video\ngeneration. Our model first uses pixel-based VDMs to produce a low-resolution\nvideo of strong text-video correlation. After that, we propose a novel expert\ntranslation method that employs the latent-based VDMs to further upsample the\nlow-resolution video to high resolution. Compared to latent VDMs, Show-1 can\nproduce high-quality videos of precise text-video alignment; Compared to pixel\nVDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G\nvs 72G). We also validate our model on standard video generation benchmarks.\nOur code and model weights are publicly available at\n\\url{https://github.com/showlab/Show-1}.",
        "translated": ""
    },
    {
        "title": "Convolutional Networks with Oriented 1D Kernels",
        "url": "http://arxiv.org/abs/2309.15812v1",
        "pub_date": "2023-09-27",
        "summary": "In computer vision, 2D convolution is arguably the most important operation\nperformed by a ConvNet. Unsurprisingly, it has been the focus of intense\nsoftware and hardware optimization and enjoys highly efficient implementations.\nIn this work, we ask an intriguing question: can we make a ConvNet work without\n2D convolutions? Surprisingly, we find that the answer is yes -- we show that a\nConvNet consisting entirely of 1D convolutions can do just as well as 2D on\nImageNet classification. Specifically, we find that one key ingredient to a\nhigh-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are oriented\nnot just horizontally or vertically, but also at other angles. Our experiments\nshow that oriented 1D convolutions can not only replace 2D convolutions but\nalso augment existing architectures with large kernels, leading to improved\naccuracy with minimal FLOPs increase. A key contribution of this work is a\nhighly-optimized custom CUDA implementation of oriented 1D kernels, specialized\nto the depthwise convolution setting. Our benchmarks demonstrate that our\ncustom CUDA implementation almost perfectly realizes the theoretical advantage\nof 1D convolution: it is faster than a native horizontal convolution for any\narbitrary angle. Code is available at\nhttps://github.com/princeton-vl/Oriented1D.",
        "translated": ""
    },
    {
        "title": "Learning to Transform for Generalizable Instance-wise Invariance",
        "url": "http://arxiv.org/abs/2309.16672v1",
        "pub_date": "2023-09-28",
        "summary": "Computer vision research has long aimed to build systems that are robust to\nspatial transformations found in natural data. Traditionally, this is done\nusing data augmentation or hard-coding invariances into the architecture.\nHowever, too much or too little invariance can hurt, and the correct amount is\nunknown a priori and dependent on the instance. Ideally, the appropriate\ninvariance would be learned from data and inferred at test-time.\n  We treat invariance as a prediction problem. Given any image, we use a\nnormalizing flow to predict a distribution over transformations and average the\npredictions over them. Since this distribution only depends on the instance, we\ncan align instances before classifying them and generalize invariance across\nclasses. The same distribution can also be used to adapt to out-of-distribution\nposes. This normalizing flow is trained end-to-end and can learn a much larger\nrange of transformations than Augerino and InstaAug. When used as data\naugmentation, our method shows accuracy and robustness gains on CIFAR 10,\nCIFAR10-LT, and TinyImageNet.",
        "translated": ""
    },
    {
        "title": "Demystifying CLIP Data",
        "url": "http://arxiv.org/abs/2309.16671v1",
        "pub_date": "2023-09-28",
        "summary": "Contrastive Language-Image Pre-training (CLIP) is an approach that has\nadvanced research and applications in computer vision, fueling modern\nrecognition systems and generative models. We believe that the main ingredient\nto the success of CLIP is its data and not the model architecture or\npre-training objective. However, CLIP only provides very limited information\nabout its data and how it has been collected, leading to works that aim to\nreproduce CLIP's data by filtering with its model parameters. In this work, we\nintend to reveal CLIP's data curation approach and in our pursuit of making it\nopen to the community introduce Metadata-Curated Language-Image Pre-training\n(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's\nconcepts) and yields a balanced subset over the metadata distribution. Our\nexperimental study rigorously isolates the model and training settings,\nconcentrating solely on data. MetaCLIP applied to CommonCrawl with 400M\nimage-text data pairs outperforms CLIP's data on multiple standard benchmarks.\nIn zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,\nsurpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining\nthe same training budget, attains 72.4%. Our observations hold across various\nmodel sizes, exemplified by ViT-H achieving 80.5%, without any\nbells-and-whistles. Curation code and training data distribution on metadata is\nmade available at https://github.com/facebookresearch/MetaCLIP.",
        "translated": ""
    },
    {
        "title": "Decaf: Monocular Deformation Capture for Face and Hand Interactions",
        "url": "http://arxiv.org/abs/2309.16670v1",
        "pub_date": "2023-09-28",
        "summary": "Existing methods for 3D tracking from monocular RGB videos predominantly\nconsider articulated and rigid objects. Modelling dense non-rigid object\ndeformations in this setting remained largely unaddressed so far, although such\neffects can improve the realism of the downstream applications such as AR/VR\nand avatar communications. This is due to the severe ill-posedness of the\nmonocular view setting and the associated challenges. While it is possible to\nnaively track multiple non-rigid objects independently using 3D templates or\nparametric 3D models, such an approach would suffer from multiple artefacts in\nthe resulting 3D estimates such as depth ambiguity, unnatural intra-object\ncollisions and missing or implausible deformations. Hence, this paper\nintroduces the first method that addresses the fundamental challenges depicted\nabove and that allows tracking human hands interacting with human faces in 3D\nfrom single monocular RGB videos. We model hands as articulated objects\ninducing non-rigid face deformations during an active interaction. Our method\nrelies on a new hand-face motion and interaction capture dataset with realistic\nface deformations acquired with a markerless multi-view camera system. As a\npivotal step in its creation, we process the reconstructed raw 3D shapes with\nposition-based dynamics and an approach for non-uniform stiffness estimation of\nthe head tissues, which results in plausible annotations of the surface\ndeformations, hand-face contact regions and head-hand positions. At the core of\nour neural approach are a variational auto-encoder supplying the hand-face\ndepth prior and modules that guide the 3D tracking by estimating the contacts\nand the deformations. Our final 3D hand and face reconstructions are realistic\nand more plausible compared to several baselines applicable in our setting,\nboth quantitatively and qualitatively.\nhttps://vcai.mpi-inf.mpg.de/projects/Decaf",
        "translated": ""
    },
    {
        "title": "Training a Large Video Model on a Single Machine in a Day",
        "url": "http://arxiv.org/abs/2309.16669v1",
        "pub_date": "2023-09-28",
        "summary": "Videos are big, complex to pre-process, and slow to train on.\nState-of-the-art large-scale video models are trained on clusters of 32 or more\nGPUs for several days. As a consequence, academia largely ceded the training of\nlarge video models to industry. In this paper, we show how to still train a\nstate-of-the-art video model on a single machine with eight consumer-grade GPUs\nin a day. We identify three bottlenecks, IO, CPU, and GPU computation, and\noptimize each. The result is a highly efficient video training pipeline. For\ncomparable architectures, our pipeline achieves higher accuracies with\n$\\frac{1}{8}$ of the computation compared to prior work. Code is available at\nhttps://github.com/zhaoyue-zephyrus/AVION.",
        "translated": ""
    },
    {
        "title": "RealFill: Reference-Driven Generation for Authentic Image Completion",
        "url": "http://arxiv.org/abs/2309.16668v1",
        "pub_date": "2023-09-28",
        "summary": "Recent advances in generative imagery have brought forth outpainting and\ninpainting models that can produce high-quality, plausible image content in\nunknown regions, but the content these models hallucinate is necessarily\ninauthentic, since the models lack sufficient context about the true scene. In\nthis work, we propose RealFill, a novel generative approach for image\ncompletion that fills in missing regions of an image with the content that\nshould have been there. RealFill is a generative inpainting model that is\npersonalized using only a few reference images of a scene. These reference\nimages do not have to be aligned with the target image, and can be taken with\ndrastically varying viewpoints, lighting conditions, camera apertures, or image\nstyles. Once personalized, RealFill is able to complete a target image with\nvisually compelling contents that are faithful to the original scene. We\nevaluate RealFill on a new image completion benchmark that covers a set of\ndiverse and challenging scenarios, and find that it outperforms existing\napproaches by a large margin. See more results on our project page:\nhttps://realfill.github.io",
        "translated": ""
    },
    {
        "title": "Multi-task View Synthesis with Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.17450v1",
        "pub_date": "2023-09-29",
        "summary": "Multi-task visual learning is a critical aspect of computer vision. Current\nresearch, however, predominantly concentrates on the multi-task dense\nprediction setting, which overlooks the intrinsic 3D world and its multi-view\nconsistent structures, and lacks the capability for versatile imagination. In\nresponse to these limitations, we present a novel problem setting -- multi-task\nview synthesis (MTVS), which reinterprets multi-task prediction as a set of\nnovel-view synthesis tasks for multiple scene properties, including RGB. To\ntackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates\nboth multi-task and cross-view knowledge to simultaneously synthesize multiple\nscene properties. MuvieNeRF integrates two key modules, the Cross-Task\nAttention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient\nuse of information across multiple views and tasks. Extensive evaluation on\nboth synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable\nof simultaneously synthesizing different scene properties with promising visual\nquality, even outperforming conventional discriminative models in various\nsettings. Notably, we show that MuvieNeRF exhibits universal applicability\nacross a range of NeRF backbones. Our code is available at\nhttps://github.com/zsh2000/MuvieNeRF.",
        "translated": ""
    },
    {
        "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
        "url": "http://arxiv.org/abs/2309.17448v1",
        "pub_date": "2023-09-29",
        "summary": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and\nface motion capture with numerous applications. Despite encouraging progress,\ncurrent state-of-the-art methods still depend largely on confined training\ndatasets. In this work, we investigate scaling up EHPS towards the first\ngeneralist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the\nbackbone and training with up to 4.5M instances from diverse data sources. With\nbig data and the large model, SMPLer-X exhibits strong performance across\ndiverse test benchmarks and excellent transferability to even unseen\nenvironments. 1) For the data scaling, we perform a systematic investigation on\n32 EHPS datasets, encompassing a wide range of scenarios that a model trained\non any single dataset cannot handle. More importantly, capitalizing on insights\nobtained from the extensive benchmarking process, we optimize our training\nscheme and select datasets that lead to a significant leap in EHPS\ncapabilities. 2) For the model scaling, we take advantage of vision\ntransformers to study the scaling law of model sizes in EHPS. Moreover, our\nfinetuning strategy turn SMPLer-X into specialist models, allowing them to\nachieve further performance boosts. Notably, our foundation model SMPLer-X\nconsistently delivers state-of-the-art results on seven benchmarks such as\nAGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF\n(62.3 mm PVE without finetuning).",
        "translated": ""
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "url": "http://arxiv.org/abs/2309.17444v1",
        "pub_date": "2023-09-29",
        "summary": "Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion (e.g.,\neven lacking the ability to be prompted for objects moving from left to right).\nTo address these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.",
        "translated": ""
    },
    {
        "title": "FACTS: First Amplify Correlations and Then Slice to Discover Bias",
        "url": "http://arxiv.org/abs/2309.17430v1",
        "pub_date": "2023-09-29",
        "summary": "Computer vision datasets frequently contain spurious correlations between\ntask-relevant labels and (easy to learn) latent task-irrelevant attributes\n(e.g. context). Models trained on such datasets learn \"shortcuts\" and\nunderperform on bias-conflicting slices of data where the correlation does not\nhold. In this work, we study the problem of identifying such slices to inform\ndownstream bias mitigation strategies. We propose First Amplify Correlations\nand Then Slice to Discover Bias (FACTS), wherein we first amplify correlations\nto fit a simple bias-aligned hypothesis via strongly regularized empirical risk\nminimization. Next, we perform correlation-aware slicing via mixture modeling\nin bias-aligned feature space to discover underperforming data slices that\ncapture distinct correlations. Despite its simplicity, our method considerably\nimproves over prior work (by as much as 35% precision@10) in correlation bias\nidentification across a range of diverse evaluation settings. Our code is\navailable at: https://github.com/yvsriram/FACTS.",
        "translated": ""
    },
    {
        "title": "Classification of Potholes Based on Surface Area Using Pre-Trained\n  Models of Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2309.17426v1",
        "pub_date": "2023-09-29",
        "summary": "Potholes are fatal and can cause severe damage to vehicles as well as can\ncause deadly accidents. In South Asian countries, pavement distresses are the\nprimary cause due to poor subgrade conditions, lack of subsurface drainage, and\nexcessive rainfalls. The present research compares the performance of three\npre-trained Convolutional Neural Network (CNN) models, i.e., ResNet 50, ResNet\n18, and MobileNet. At first, pavement images are classified to find whether\nimages contain potholes, i.e., Potholes or Normal. Secondly, pavements images\nare classi-fied into three categories, i.e., Small Pothole, Large Pothole, and\nNormal. Pavement images are taken from 3.5 feet (waist height) and 2 feet.\nMobileNet v2 has an accuracy of 98% for detecting a pothole. The classification\nof images taken at the height of 2 feet has an accuracy value of 87.33%,\n88.67%, and 92% for classifying the large, small, and normal pavement,\nrespectively. Similarly, the classification of the images taken from full of\nwaist (FFW) height has an accuracy value of 98.67%, 98.67%, and 100%.",
        "translated": ""
    },
    {
        "title": "DREAM: Visual Decoding from Reversing Human Visual System",
        "url": "http://arxiv.org/abs/2310.02265v1",
        "pub_date": "2023-10-03",
        "summary": "In this work we present DREAM, an fMRI-to-image method for reconstructing\nviewed images from brain activities, grounded on fundamental knowledge of the\nhuman visual system. We craft reverse pathways that emulate the hierarchical\nand parallel nature of how humans perceive the visual world. These tailored\npathways are specialized to decipher semantics, color, and depth cues from fMRI\ndata, mirroring the forward pathways from visual stimuli to fMRI recordings. To\ndo so, two components mimic the inverse processes within the human visual\nsystem: the Reverse Visual Association Cortex (R-VAC) which reverses pathways\nof this brain region, extracting semantics from fMRI data; the Reverse Parallel\nPKM (R-PKM) component simultaneously predicting color and depth from fMRI\nsignals. The experiments indicate that our method outperforms the current\nstate-of-the-art models in terms of the consistency of appearance, structure,\nand semantics. Code will be made publicly available to facilitate further\nresearch in this field.",
        "translated": ""
    },
    {
        "title": "Generalizable Long-Horizon Manipulations with Large Language Models",
        "url": "http://arxiv.org/abs/2310.02264v1",
        "pub_date": "2023-10-03",
        "summary": "This work introduces a framework harnessing the capabilities of Large\nLanguage Models (LLMs) to generate primitive task conditions for generalizable\nlong-horizon manipulations with novel objects and unseen tasks. These task\nconditions serve as guides for the generation and adjustment of Dynamic\nMovement Primitives (DMP) trajectories for long-horizon task execution. We\nfurther create a challenging robotic manipulation task suite based on Pybullet\nfor long-horizon task evaluation. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of our framework on both\nfamiliar tasks involving new objects and novel but related tasks, highlighting\nthe potential of LLMs in enhancing robotic system versatility and adaptability.\nProject website: https://object814.github.io/Task-Condition-With-LLM/",
        "translated": ""
    },
    {
        "title": "RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and\n  Comfortable Autonomous Driving",
        "url": "http://arxiv.org/abs/2310.02262v1",
        "pub_date": "2023-10-03",
        "summary": "This paper addresses the growing demands for safety and comfort in\nintelligent robot systems, particularly autonomous vehicles, where road\nconditions play a pivotal role in overall driving performance. For example,\nreconstructing road surfaces helps to enhance the analysis and prediction of\nvehicle responses for motion planning and control systems. We introduce the\nRoad Surface Reconstruction Dataset (RSRD), a real-world, high-resolution, and\nhigh-precision dataset collected with a specialized platform in diverse driving\nconditions. It covers common road types containing approximately 16,000 pairs\nof stereo images, original point clouds, and ground-truth depth/disparity maps,\nwith accurate post-processing pipelines to ensure its quality. Based on RSRD,\nwe further build a comprehensive benchmark for recovering road profiles through\ndepth estimation and stereo matching. Preliminary evaluations with various\nstate-of-the-art methods reveal the effectiveness of our dataset and the\nchallenge of the task, underscoring substantial opportunities of RSRD as a\nvaluable resource for advancing techniques, e.g., multi-view stereo towards\nsafe autonomous driving. The dataset and demo videos are available at\nhttps://thu-rsxd.com/rsrd/",
        "translated": ""
    },
    {
        "title": "TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View\n  Radar Semantic Segmentation",
        "url": "http://arxiv.org/abs/2310.02260v1",
        "pub_date": "2023-10-03",
        "summary": "Scene understanding plays an essential role in enabling autonomous driving\nand maintaining high standards of performance and safety. To address this task,\ncameras and laser scanners (LiDARs) have been the most commonly used sensors,\nwith radars being less popular. Despite that, radars remain low-cost,\ninformation-dense, and fast-sensing techniques that are resistant to adverse\nweather conditions. While multiple works have been previously presented for\nradar-based scene semantic segmentation, the nature of the radar data still\nposes a challenge due to the inherent noise and sparsity, as well as the\ndisproportionate foreground and background. In this work, we propose a novel\napproach to the semantic segmentation of radar scenes using a multi-input\nfusion of radar data through a novel architecture and loss functions that are\ntailored to tackle the drawbacks of radar perception. Our novel architecture\nincludes an efficient attention block that adaptively captures important\nfeature information. Our method, TransRadar, outperforms state-of-the-art\nmethods on the CARRADA and RADIal datasets while having smaller model sizes.\nhttps://github.com/YahiDar/TransRadar",
        "translated": ""
    },
    {
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in\n  Visual Contexts",
        "url": "http://arxiv.org/abs/2310.02255v1",
        "pub_date": "2023-10-03",
        "summary": "Although Large Language Models (LLMs) and Large Multimodal Models (LMMs)\nexhibit impressive skills in various domains, their ability for mathematical\nreasoning within visual contexts has not been formally examined. Equipping LLMs\nand LMMs with this capability is vital for general-purpose AI assistants and\nshowcases promising potential in education, data analysis, and scientific\ndiscovery. To bridge this gap, we present MathVista, a benchmark designed to\namalgamate challenges from diverse mathematical and visual tasks. We first\ntaxonomize the key task types, reasoning skills, and visual contexts from the\nliterature to guide our selection from 28 existing math-focused and visual\nquestion answering datasets. Then, we construct three new datasets, IQTest,\nFunctionQA, and PaperQA, to accommodate for missing types of visual contexts.\nThe problems featured often require deep visual understanding beyond OCR or\nimage captioning, and compositional reasoning with rich domain-specific tools,\nthus posing a notable challenge to existing models. We conduct a comprehensive\nevaluation of 11 prominent open-source and proprietary foundation models (LLMs,\nLLMs augmented with tools, and LMMs), and early experiments with GPT-4V. The\nbest-performing model, Multimodal Bard, achieves only 58% of human performance\n(34.8% vs 60.3%), indicating ample room for further improvement. Given this\nsignificant gap, MathVista fuels future research in the development of\ngeneral-purpose AI agents capable of tackling mathematically intensive and\nvisually rich real-world tasks. Preliminary tests show that MathVista also\npresents challenges to GPT-4V, underscoring the benchmark's importance. The\nproject is available at https://mathvista.github.io/.",
        "translated": ""
    },
    {
        "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2310.03026v1",
        "pub_date": "2023-10-04",
        "summary": "Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc",
        "translated": ""
    },
    {
        "title": "Human-oriented Representation Learning for Robotic Manipulation",
        "url": "http://arxiv.org/abs/2310.03023v1",
        "pub_date": "2023-10-04",
        "summary": "Humans inherently possess generalizable visual representations that empower\nthem to efficiently explore and interact with the environments in manipulation\ntasks. We advocate that such a representation automatically arises from\nsimultaneously learning about multiple simple perceptual skills that are\ncritical for everyday scenarios (e.g., hand detection, state estimate, etc.)\nand is better suited for learning robot manipulation policies compared to\ncurrent state-of-the-art visual representations purely based on self-supervised\nobjectives. We formalize this idea through the lens of human-oriented\nmulti-task fine-tuning on top of pre-trained visual encoders, where each task\nis a perceptual skill tied to human-environment interactions. We introduce Task\nFusion Decoder as a plug-and-play embedding translator that utilizes the\nunderlying relationships among these perceptual skills to guide the\nrepresentation learning towards encoding meaningful structure for what's\nimportant for all perceptual skills, ultimately empowering learning of\ndownstream robotic manipulation tasks. Extensive experiments across a range of\nrobotic tasks and embodiments, in both simulations and real-world environments,\nshow that our Task Fusion Decoder consistently improves the representation of\nthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for\ndownstream manipulation policy-learning. Project page:\nhttps://sites.google.com/view/human-oriented-robot-learning",
        "translated": ""
    },
    {
        "title": "Consistent-1-to-3: Consistent Image to 3D View Synthesis via\n  Geometry-aware Diffusion Models",
        "url": "http://arxiv.org/abs/2310.03020v1",
        "pub_date": "2023-10-04",
        "summary": "Zero-shot novel view synthesis (NVS) from a single image is an essential\nproblem in 3D object understanding. While recent approaches that leverage\npre-trained generative models can synthesize high-quality novel views from\nin-the-wild inputs, they still struggle to maintain 3D consistency across\ndifferent views. In this paper, we present Consistent-1-to-3, which is a\ngenerative framework that significantly mitigate this issue. Specifically, we\ndecompose the NVS task into two stages: (i) transforming observed regions to a\nnovel view, and (ii) hallucinating unseen regions. We design a scene\nrepresentation transformer and view-conditioned diffusion model for performing\nthese two stages respectively. Inside the models, to enforce 3D consistency, we\npropose to employ epipolor-guided attention to incorporate geometry\nconstraints, and multi-view attention to better aggregate multi-view\ninformation. Finally, we design a hierarchy generation paradigm to generate\nlong sequences of consistent views, allowing a full 360 observation of the\nprovided object image. Qualitative and quantitative evaluation over multiple\ndatasets demonstrate the effectiveness of the proposed mechanisms against\nstate-of-the-art approaches. Our project page is at\nhttps://jianglongye.com/consistent123/",
        "translated": ""
    },
    {
        "title": "Efficient-3DiM: Learning a Generalizable Single-image Novel-view\n  Synthesizer in One Day",
        "url": "http://arxiv.org/abs/2310.03015v1",
        "pub_date": "2023-10-04",
        "summary": "The task of novel view synthesis aims to generate unseen perspectives of an\nobject or scene from a limited set of input images. Nevertheless, synthesizing\nnovel views from a single image still remains a significant challenge in the\nrealm of computer vision. Previous approaches tackle this problem by adopting\nmesh prediction, multi-plain image construction, or more advanced techniques\nsuch as neural radiance fields. Recently, a pre-trained diffusion model that is\nspecifically designed for 2D image synthesis has demonstrated its capability in\nproducing photorealistic novel views, if sufficiently optimized on a 3D\nfinetuning task. Although the fidelity and generalizability are greatly\nimproved, training such a powerful diffusion model requires a vast volume of\ntraining data and model parameters, resulting in a notoriously long time and\nhigh computational costs. To tackle this issue, we propose Efficient-3DiM, a\nsimple but effective framework to learn a single-image novel-view synthesizer.\nMotivated by our in-depth analysis of the inference process of diffusion\nmodels, we propose several pragmatic strategies to reduce the training overhead\nto a manageable scale, including a crafted timestep sampling strategy, a\nsuperior 3D feature extractor, and an enhanced training scheme. When combined,\nour framework is able to reduce the total training time from 10 days to less\nthan 1 day, significantly accelerating the training process under the same\ncomputational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive\nexperiments are conducted to demonstrate the efficiency and generalizability of\nour proposed method.",
        "translated": ""
    },
    {
        "title": "Towards Domain-Specific Features Disentanglement for Domain\n  Generalization",
        "url": "http://arxiv.org/abs/2310.03007v1",
        "pub_date": "2023-10-04",
        "summary": "Distributional shift between domains poses great challenges to modern machine\nlearning algorithms. The domain generalization (DG) signifies a popular line\ntargeting this issue, where these methods intend to uncover universal patterns\nacross disparate distributions. Noted, the crucial challenge behind DG is the\nexistence of irrelevant domain features, and most prior works overlook this\ninformation. Motivated by this, we propose a novel contrastive-based\ndisentanglement method CDDG, to effectively utilize the disentangled features\nto exploit the over-looked domain-specific features, and thus facilitating the\nextraction of the desired cross-domain category features for DG tasks.\nSpecifically, CDDG learns to decouple inherent mutually exclusive features by\nleveraging them in the latent space, thus making the learning discriminative.\nExtensive experiments conducted on various benchmark datasets demonstrate the\nsuperiority of our method compared to other state-of-the-art approaches.\nFurthermore, visualization evaluations confirm the potential of our method in\nachieving effective feature disentanglement.",
        "translated": ""
    },
    {
        "title": "Improved Baselines with Visual Instruction Tuning",
        "url": "http://arxiv.org/abs/2310.03744v1",
        "pub_date": "2023-10-05",
        "summary": "Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.",
        "translated": ""
    },
    {
        "title": "ContactGen: Generative Contact Modeling for Grasp Generation",
        "url": "http://arxiv.org/abs/2310.03740v1",
        "pub_date": "2023-10-05",
        "summary": "This paper presents a novel object-centric contact representation ContactGen\nfor hand-object interaction. The ContactGen comprises three components: a\ncontact map indicates the contact location, a part map represents the contact\nhand part, and a direction map tells the contact direction within each part.\nGiven an input object, we propose a conditional generative model to predict\nContactGen and adopt model-based optimization to predict diverse and\ngeometrically feasible grasps. Experimental results demonstrate our method can\ngenerate high-fidelity and diverse human grasps for various objects. Project\npage: https://stevenlsw.github.io/contactgen/",
        "translated": ""
    },
    {
        "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
        "url": "http://arxiv.org/abs/2310.03739v1",
        "pub_date": "2023-10-05",
        "summary": "Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.",
        "translated": ""
    },
    {
        "title": "Stylist: Style-Driven Feature Ranking for Robust Novelty Detection",
        "url": "http://arxiv.org/abs/2310.03738v1",
        "pub_date": "2023-10-05",
        "summary": "Novelty detection aims at finding samples that differ in some form from the\ndistribution of seen samples. But not all changes are created equal. Data can\nsuffer a multitude of distribution shifts, and we might want to detect only\nsome types of relevant changes. Similar to works in out-of-distribution\ngeneralization, we propose to use the formalization of separating into semantic\nor content changes, that are relevant to our task, and style changes, that are\nirrelevant. Within this formalization, we define the robust novelty detection\nas the task of finding semantic changes while being robust to style\ndistributional shifts. Leveraging pretrained, large-scale model\nrepresentations, we introduce Stylist, a novel method that focuses on dropping\nenvironment-biased features. First, we compute a per-feature score based on the\nfeature distribution distances between environments. Next, we show that our\nselection manages to remove features responsible for spurious correlations and\nimprove novelty detection performance. For evaluation, we adapt domain\ngeneralization datasets to our task and analyze the methods behaviors. We\nadditionally built a large synthetic dataset where we have control over the\nspurious correlations degree. We prove that our selection mechanism improves\nnovelty detection algorithms across multiple datasets, containing both\nstylistic and content shifts.",
        "translated": ""
    },
    {
        "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle\n  Consistency",
        "url": "http://arxiv.org/abs/2310.03734v1",
        "pub_date": "2023-10-05",
        "summary": "Current vision-language generative models rely on expansive corpora of paired\nimage-text data to attain optimal performance and generalization capabilities.\nHowever, automatically collecting such data (e.g. via large-scale web scraping)\nleads to low quality and poor image-text correlation, while human annotation is\nmore accurate but requires significant manual effort and expense. We introduce\n$\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage\n$\\textbf{T}$ext): an innovative training paradigm grounded in the concept of\ncycle consistency which allows vision-language training on unpaired image and\ntext data. ITIT is comprised of a joint image-text encoder with disjoint image\nand text decoders that enable bidirectional image-to-text and text-to-image\ngeneration in a single framework. During training, ITIT leverages a small set\nof paired image-text data to ensure its output matches the input reasonably\nwell in both directions. Simultaneously, the model is also trained on much\nlarger datasets containing only images or texts. This is achieved by enforcing\ncycle consistency between the original unpaired samples and the cycle-generated\ncounterparts. For instance, it generates a caption for a given input image and\nthen uses the caption to create an output image, and enforces similarity\nbetween the input and output images. Our experiments show that ITIT with\nunpaired datasets exhibits similar scaling behavior as using high-quality\npaired data. We demonstrate image generation and captioning performance on par\nwith state-of-the-art text-to-image and image-to-text models with orders of\nmagnitude fewer (only 3M) paired image-text data.",
        "translated": ""
    },
    {
        "title": "Alice Benchmarks: Connecting Real World Object Re-Identification with\n  the Synthetic",
        "url": "http://arxiv.org/abs/2310.04416v1",
        "pub_date": "2023-10-06",
        "summary": "For object re-identification (re-ID), learning from synthetic data has become\na promising strategy to cheaply acquire large-scale annotated datasets and\neffective models, with few privacy concerns. Many interesting research problems\narise from this strategy, e.g., how to reduce the domain gap between synthetic\nsource and real-world target. To facilitate developing more new approaches in\nlearning from synthetic data, we introduce the Alice benchmarks, large-scale\ndatasets providing benchmarks as well as evaluation protocols to the research\ncommunity. Within the Alice benchmarks, two object re-ID tasks are offered:\nperson and vehicle re-ID. We collected and annotated two challenging real-world\ntarget datasets: AlicePerson and AliceVehicle, captured under various\nilluminations, image resolutions, etc. As an important feature of our real\ntarget, the clusterability of its training set is not manually guaranteed to\nmake it closer to a real domain adaptation test scenario. Correspondingly, we\nreuse existing PersonX and VehicleX as synthetic source domains. The primary\ngoal is to train models from synthetic data that can work effectively in the\nreal world. In this paper, we detail the settings of Alice benchmarks, provide\nan analysis of existing commonly-used domain adaptation methods, and discuss\nsome interesting future directions. An online server will be set up for the\ncommunity to evaluate methods conveniently and fairly.",
        "translated": ""
    },
    {
        "title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis",
        "url": "http://arxiv.org/abs/2310.04414v1",
        "pub_date": "2023-10-06",
        "summary": "Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.",
        "translated": ""
    },
    {
        "title": "FedConv: Enhancing Convolutional Neural Networks for Handling Data\n  Heterogeneity in Federated Learning",
        "url": "http://arxiv.org/abs/2310.04412v1",
        "pub_date": "2023-10-06",
        "summary": "Federated learning (FL) is an emerging paradigm in machine learning, where a\nshared model is collaboratively learned using data from multiple devices to\nmitigate the risk of data leakage. While recent studies posit that Vision\nTransformer (ViT) outperforms Convolutional Neural Networks (CNNs) in\naddressing data heterogeneity in FL, the specific architectural components that\nunderpin this advantage have yet to be elucidated. In this paper, we\nsystematically investigate the impact of different architectural elements, such\nas activation functions and normalization layers, on the performance within\nheterogeneous FL. Through rigorous empirical analyses, we are able to offer the\nfirst-of-its-kind general guidance on micro-architecture design principles for\nheterogeneous FL.\n  Intriguingly, our findings indicate that with strategic architectural\nmodifications, pure CNNs can achieve a level of robustness that either matches\nor even exceeds that of ViTs when handling heterogeneous data clients in FL.\nAdditionally, our approach is compatible with existing FL techniques and\ndelivers state-of-the-art solutions across a broad spectrum of FL benchmarks.\nThe code is publicly available at https://github.com/UCSC-VLAA/FedConv",
        "translated": ""
    },
    {
        "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in\n  Language Models",
        "url": "http://arxiv.org/abs/2310.04406v1",
        "pub_date": "2023-10-06",
        "summary": "While large language models (LLMs) have demonstrated impressive performance\non a range of decision-making tasks, they rely on simple acting processes and\nfall short of broad deployment as autonomous agents. We introduce LATS\n(Language Agent Tree Search), a general framework that synergizes the\ncapabilities of LLMs in planning, acting, and reasoning. Drawing inspiration\nfrom Monte Carlo tree search in model-based reinforcement learning, LATS\nemploys LLMs as agents, value functions, and optimizers, repurposing their\nlatent strengths for enhanced decision-making. What is crucial in this method\nis the use of an environment for external feedback, which offers a more\ndeliberate and adaptive problem-solving mechanism that moves beyond the\nlimitations of existing techniques. Our experimental evaluation across diverse\ndomains, such as programming, HotPotQA, and WebShop, illustrates the\napplicability of LATS for both reasoning and acting. In particular, LATS\nachieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of\n75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness\nand generality of our method.",
        "translated": ""
    },
    {
        "title": "Latent Consistency Models: Synthesizing High-Resolution Images with\n  Few-Step Inference",
        "url": "http://arxiv.org/abs/2310.04378v1",
        "pub_date": "2023-10-06",
        "summary": "Latent Diffusion models (LDMs) have achieved remarkable results in\nsynthesizing high-resolution images. However, the iterative sampling process is\ncomputationally intensive and leads to slow generation. Inspired by Consistency\nModels (song et al.), we propose Latent Consistency Models (LCMs), enabling\nswift inference with minimal steps on any pre-trained LDMs, including Stable\nDiffusion (rombach et al). Viewing the guided reverse diffusion process as\nsolving an augmented probability flow ODE (PF-ODE), LCMs are designed to\ndirectly predict the solution of such ODE in latent space, mitigating the need\nfor numerous iterations and allowing rapid, high-fidelity sampling. Efficiently\ndistilled from pre-trained classifier-free guided diffusion models, a\nhigh-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.\nFurthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method\nthat is tailored for fine-tuning LCMs on customized image datasets. Evaluation\non the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve\nstate-of-the-art text-to-image generation performance with few-step inference.\nProject Page: https://latent-consistency-models.github.io/",
        "translated": ""
    },
    {
        "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video\n  editing",
        "url": "http://arxiv.org/abs/2310.05922v1",
        "pub_date": "2023-10-09",
        "summary": "Text-to-video editing aims to edit the visual appearance of a source video\nconditional on textual prompts. A major challenge in this task is to ensure\nthat all frames in the edited video are visually consistent. Most recent works\napply advanced text-to-image diffusion models to this task by inflating 2D\nspatial attention in the U-Net into spatio-temporal attention. Although\ntemporal context can be added through spatio-temporal attention, it may\nintroduce some irrelevant information for each patch and therefore cause\ninconsistency in the edited video. In this paper, for the first time, we\nintroduce optical flow into the attention module in the diffusion model's U-Net\nto address the inconsistency issue for text-to-video editing. Our method,\nFLATTEN, enforces the patches on the same flow path across different frames to\nattend to each other in the attention module, thus improving the visual\nconsistency in the edited videos. Additionally, our method is training-free and\ncan be seamlessly integrated into any diffusion-based text-to-video editing\nmethods and improve their visual consistency. Experiment results on existing\ntext-to-video editing benchmarks show that our proposed method achieves the new\nstate-of-the-art performance. In particular, our method excels in maintaining\nthe visual consistency in the edited videos.",
        "translated": ""
    },
    {
        "title": "SimPLR: A Simple and Plain Transformer for Object Detection and\n  Segmentation",
        "url": "http://arxiv.org/abs/2310.05920v1",
        "pub_date": "2023-10-09",
        "summary": "The ability to detect objects in images at varying scales has played a\npivotal role in the design of modern object detectors. Despite considerable\nprogress in removing handcrafted components using transformers, multi-scale\nfeature maps remain a key factor for their empirical success, even with a plain\nbackbone like the Vision Transformer (ViT). In this paper, we show that this\nreliance on feature pyramids is unnecessary and a transformer-based detector\nwith scale-aware attention enables the plain detector `SimPLR' whose backbone\nand detection head both operate on single-scale features. The plain\narchitecture allows SimPLR to effectively take advantages of self-supervised\nlearning and scaling approaches with ViTs, yielding strong performance compared\nto multi-scale counterparts. We demonstrate through our experiments that when\nscaling to larger backbones, SimPLR indicates better performance than\nend-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while\nconsistently being faster. The code will be released.",
        "translated": ""
    },
    {
        "title": "Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input",
        "url": "http://arxiv.org/abs/2310.05917v1",
        "pub_date": "2023-10-09",
        "summary": "Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.",
        "translated": ""
    },
    {
        "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition",
        "url": "http://arxiv.org/abs/2310.05916v1",
        "pub_date": "2023-10-09",
        "summary": "We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.",
        "translated": ""
    },
    {
        "title": "Streaming Anchor Loss: Augmenting Supervision with Temporal Significance",
        "url": "http://arxiv.org/abs/2310.05886v1",
        "pub_date": "2023-10-09",
        "summary": "Streaming neural network models for fast frame-wise responses to various\nspeech and sensory signals are widely adopted on resource-constrained\nplatforms. Hence, increasing the learning capacity of such streaming models\n(i.e., by adding more parameters) to improve the predictive power may not be\nviable for real-world tasks. In this work, we propose a new loss, Streaming\nAnchor Loss (SAL), to better utilize the given learning capacity by encouraging\nthe model to learn more from essential frames. More specifically, our SAL and\nits focal variations dynamically modulate the frame-wise cross entropy loss\nbased on the importance of the corresponding frames so that a higher loss\npenalty is assigned for frames within the temporal proximity of semantically\ncritical events. Therefore, our loss ensures that the model training focuses on\npredicting the relatively rare but task-relevant frames. Experimental results\nwith standard lightweight convolutional and recurrent streaming networks on\nthree different speech based detection tasks demonstrate that SAL enables the\nmodel to learn the overall task more effectively with improved accuracy and\nlatency, without any additional data, model parameters, or architectural\nchanges.",
        "translated": ""
    },
    {
        "title": "AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description",
        "url": "http://arxiv.org/abs/2310.06838v1",
        "pub_date": "2023-10-10",
        "summary": "Audio Description (AD) is the task of generating descriptions of visual\ncontent, at suitable time intervals, for the benefit of visually impaired\naudiences. For movies, this presents notable challenges -- AD must occur only\nduring existing pauses in dialogue, should refer to characters by name, and\nought to aid understanding of the storyline as a whole. To this end, we develop\na new model for automatically generating movie AD, given CLIP visual features\nof the frames, the cast list, and the temporal locations of the speech;\naddressing all three of the 'who', 'when', and 'what' questions: (i) who -- we\nintroduce a character bank consisting of the character's name, the actor that\nplayed the part, and a CLIP feature of their face, for the principal cast of\neach movie, and demonstrate how this can be used to improve naming in the\ngenerated AD; (ii) when -- we investigate several models for determining\nwhether an AD should be generated for a time interval or not, based on the\nvisual content of the interval and its neighbours; and (iii) what -- we\nimplement a new vision-language model for this task, that can ingest the\nproposals from the character bank, whilst conditioning on the visual features\nusing cross-attention, and demonstrate how this improves over previous\narchitectures for AD text generation in an apples-to-apples comparison.",
        "translated": ""
    },
    {
        "title": "What Does Stable Diffusion Know about the 3D Scene?",
        "url": "http://arxiv.org/abs/2310.06836v1",
        "pub_date": "2023-10-10",
        "summary": "Recent advances in generative models like Stable Diffusion enable the\ngeneration of highly photo-realistic images. Our objective in this paper is to\nprobe the diffusion network to determine to what extent it 'understands'\ndifferent properties of the 3D scene depicted in an image. To this end, we make\nthe following contributions: (i) We introduce a protocol to evaluate whether a\nnetwork models a number of physical 'properties' of the 3D scene by probing for\nexplicit features that represent these properties. The probes are applied on\ndatasets of real images with annotations for the property. (ii) We apply this\nprotocol to properties covering scene geometry, scene material, support\nrelations, lighting, and view dependent measures. (iii) We find that Stable\nDiffusion is good at a number of properties including scene geometry, support\nrelations, shadows and depth, but less performant for occlusion. (iv) We also\napply the probes to other models trained at large-scale, including DINO and\nCLIP, and find their performance inferior to that of Stable Diffusion.",
        "translated": ""
    },
    {
        "title": "NECO: NEural Collapse Based Out-of-distribution detection",
        "url": "http://arxiv.org/abs/2310.06823v1",
        "pub_date": "2023-10-10",
        "summary": "Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.",
        "translated": ""
    },
    {
        "title": "Neural Bounding",
        "url": "http://arxiv.org/abs/2310.06822v1",
        "pub_date": "2023-10-10",
        "summary": "Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free and empty. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods.",
        "translated": ""
    },
    {
        "title": "Uni3D: Exploring Unified 3D Representation at Scale",
        "url": "http://arxiv.org/abs/2310.06773v1",
        "pub_date": "2023-10-10",
        "summary": "Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.",
        "translated": ""
    },
    {
        "title": "PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection",
        "url": "http://arxiv.org/abs/2310.07716v1",
        "pub_date": "2023-10-11",
        "summary": "Object anomaly detection is an important problem in the field of machine\nvision and has seen remarkable progress recently. However, two significant\nchallenges hinder its research and application. First, existing datasets lack\ncomprehensive visual information from various pose angles. They usually have an\nunrealistic assumption that the anomaly-free training dataset is pose-aligned,\nand the testing samples have the same pose as the training data. However, in\npractice, anomaly may exist in any regions on a object, the training and query\nsamples may have different poses, calling for the study on pose-agnostic\nanomaly detection. Second, the absence of a consensus on experimental protocols\nfor pose-agnostic anomaly detection leads to unfair comparisons of different\nmethods, hindering the research on pose-agnostic anomaly detection. To address\nthese issues, we develop Multi-pose Anomaly Detection (MAD) dataset and\nPose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to\naddress the pose-agnostic anomaly detection problem. Specifically, we build MAD\nusing 20 complex-shaped LEGO toys including 4K views with various poses, and\nhigh-quality and diverse 3D anomalies in both simulated and real environments.\nAdditionally, we propose a novel method OmniposeAD, trained using MAD,\nspecifically designed for pose-agnostic anomaly detection. Through\ncomprehensive evaluations, we demonstrate the relevance of our dataset and\nmethod. Furthermore, we provide an open-source benchmark library, including\ndataset and baseline methods that cover 8 anomaly detection paradigms, to\nfacilitate future research and application in this domain. Code, data, and\nmodels are publicly available at https://github.com/EricLee0224/PAD.",
        "translated": ""
    },
    {
        "title": "MatFormer: Nested Transformer for Elastic Inference",
        "url": "http://arxiv.org/abs/2310.07707v1",
        "pub_date": "2023-10-11",
        "summary": "Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, &amp; ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n&amp; encoders), modalities (language &amp; vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.",
        "translated": ""
    },
    {
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
        "url": "http://arxiv.org/abs/2310.07704v1",
        "pub_date": "2023-10-11",
        "summary": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret",
        "translated": ""
    },
    {
        "title": "ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2310.07702v1",
        "pub_date": "2023-10-11",
        "summary": "In this work, we investigate the capability of generating images from\npre-trained diffusion models at much higher resolutions than the training image\nsizes. In addition, the generated images should have arbitrary image aspect\nratios. When generating images directly at a higher resolution, 1024 x 1024,\nwith the pre-trained Stable Diffusion using training images of resolution 512 x\n512, we observe persistent problems of object repetition and unreasonable\nobject structures. Existing works for higher-resolution generation, such as\nattention-based and joint-diffusion approaches, cannot well address these\nissues. As a new perspective, we examine the structural components of the U-Net\nin diffusion models and identify the crucial cause as the limited perception\nfield of convolutional kernels. Based on this key observation, we propose a\nsimple yet effective re-dilation that can dynamically adjust the convolutional\nperception field during inference. We further propose the dispersed convolution\nand noise-damped classifier-free guidance, which can enable\nultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our\napproach does not require any training or optimization. Extensive experiments\ndemonstrate that our approach can address the repetition issue well and achieve\nstate-of-the-art performance on higher-resolution image synthesis, especially\nin texture details. Our work also suggests that a pre-trained diffusion model\ntrained on low-resolution images can be directly used for high-resolution\nvisual generation without further tuning, which may provide insights for future\nresearch on ultra-high-resolution image and video synthesis.",
        "translated": ""
    },
    {
        "title": "From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched\n  Captions",
        "url": "http://arxiv.org/abs/2310.07699v1",
        "pub_date": "2023-10-11",
        "summary": "Web-crawled datasets are pivotal to the success of pre-training\nvision-language models, exemplified by CLIP. However, web-crawled AltTexts can\nbe noisy and potentially irrelevant to images, thereby undermining the crucial\nimage-text alignment. Existing methods for rewriting captions using large\nlanguage models (LLMs) have shown promise on small, curated datasets like CC3M\nand CC12M. Nevertheless, their efficacy on massive web-captured captions is\nconstrained by the inherent noise and randomness in such data. In this study,\nwe address this limitation by focusing on two key aspects: data quality and\ndata variety. Unlike recent LLM rewriting techniques, we emphasize exploiting\nvisual concepts and their integration into the captions to improve data\nquality. For data variety, we propose a novel mixed training scheme that\noptimally leverages AltTexts alongside newly generated Visual-enriched Captions\n(VeC). We use CLIP as one example and adapt the method for CLIP training on\nlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive\nevaluation of VeCLIP across small, medium, and large scales of raw data. Our\nresults show significant advantages in image-text alignment and overall model\nperformance, underscoring the effectiveness of VeCLIP in improving CLIP\ntraining. For example, VeCLIP achieves a remarkable over 20% improvement in\nCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,\nwe also achieve a notable over 3% improvement while using only 14% of the data\nemployed in the vanilla CLIP and 11% in ALIGN.",
        "translated": ""
    },
    {
        "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos\n  Possible Today?",
        "url": "http://arxiv.org/abs/2310.08587v1",
        "pub_date": "2023-10-12",
        "summary": "Rendering scenes observed in a monocular video from novel viewpoints is a\nchallenging problem. For static scenes the community has studied both\nscene-specific optimization techniques, which optimize on every test scene, and\ngeneralized techniques, which only run a deep net forward pass on a test scene.\nIn contrast, for dynamic scenes, scene-specific optimization techniques exist,\nbut, to our best knowledge, there is currently no generalized method for\ndynamic novel view synthesis from a given monocular video. To answer whether\ngeneralized dynamic novel view synthesis from monocular videos is possible\ntoday, we establish an analysis framework based on existing techniques and work\ntoward the generalized approach. We find a pseudo-generalized process without\nscene-specific appearance optimization is possible, but geometrically and\ntemporally consistent depth estimates are needed. Despite no scene-specific\nappearance optimization, the pseudo-generalized approach improves upon some\nscene-specific methods.",
        "translated": ""
    },
    {
        "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
        "url": "http://arxiv.org/abs/2310.08588v1",
        "pub_date": "2023-10-12",
        "summary": "Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.",
        "translated": ""
    },
    {
        "title": "Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic\n  Scenes",
        "url": "http://arxiv.org/abs/2310.08585v1",
        "pub_date": "2023-10-12",
        "summary": "This paper aims to tackle the challenge of dynamic view synthesis from\nmulti-view videos. The key observation is that while previous grid-based\nmethods offer consistent rendering, they fall short in capturing appearance\ndetails of a complex dynamic scene, a domain where multi-view image-based\nrendering methods demonstrate the opposite properties. To combine the best of\ntwo worlds, we introduce Im4D, a hybrid scene representation that consists of a\ngrid-based geometry representation and a multi-view image-based appearance\nrepresentation. Specifically, the dynamic geometry is encoded as a 4D density\nfunction composed of spatiotemporal feature planes and a small MLP network,\nwhich globally models the scene structure and facilitates the rendering\nconsistency. We represent the scene appearance by the original multi-view\nvideos and a network that learns to predict the color of a 3D point from image\nfeatures, instead of memorizing detailed appearance totally with networks,\nthereby naturally making the learning of networks easier. Our method is\nevaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,\nNHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D\nexhibits state-of-the-art performance in rendering quality and can be trained\nefficiently, while realizing real-time rendering with a speed of 79.8 FPS for\n512x512 images, on a single RTX 3090 GPU.",
        "translated": ""
    },
    {
        "title": "PonderV2: Pave the Way for 3D Foundataion Model with A Universal\n  Pre-training Paradigm",
        "url": "http://arxiv.org/abs/2310.08586v1",
        "pub_date": "2023-10-12",
        "summary": "In contrast to numerous NLP and 2D computer vision foundational models, the\nlearning of a robust and highly generalized 3D foundational model poses\nconsiderably greater challenges. This is primarily due to the inherent data\nvariability and the diversity of downstream tasks. In this paper, we introduce\na comprehensive 3D pre-training framework designed to facilitate the\nacquisition of efficient 3D representations, thereby establishing a pathway to\n3D foundational models. Motivated by the fact that informative 3D features\nshould be able to encode rich geometry and appearance cues that can be utilized\nto render realistic images, we propose a novel universal paradigm to learn\npoint cloud representations by differentiable neural rendering, serving as a\nbridge between 3D and 2D worlds. We train a point cloud encoder within a\ndevised volumetric neural renderer by comparing the rendered images with the\nreal images. Notably, our approach demonstrates the seamless integration of the\nlearned 3D encoder into diverse downstream tasks. These tasks encompass not\nonly high-level challenges such as 3D detection and segmentation but also\nlow-level objectives like 3D reconstruction and image synthesis, spanning both\nindoor and outdoor scenarios. Besides, we also illustrate the capability of\npre-training a 2D backbone using the proposed universal methodology, surpassing\nconventional pre-training methods by a large margin. For the first time,\n\\sexyname achieves state-of-the-art performance on 11 indoor and outdoor\nbenchmarks. The consistent improvements in various settings imply the\neffectiveness of the proposed method. Code and models will be made available at\nhttps://github.com/Pointcept/Pointcept.",
        "translated": ""
    },
    {
        "title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long\n  unlabelled video",
        "url": "http://arxiv.org/abs/2310.08584v1",
        "pub_date": "2023-10-12",
        "summary": "Self-supervised learning has unlocked the potential of scaling up pretraining\nto billions of images, since annotation is unnecessary. But are we making the\nbest use of data? How more economical can we be? In this work, we attempt to\nanswer this question by making two contributions. First, we investigate\nfirst-person videos and introduce a \"Walking Tours\" dataset. These videos are\nhigh-resolution, hours-long, captured in a single uninterrupted take, depicting\na large number of objects and actions with natural scene transitions. They are\nunlabeled and uncurated, thus realistic for self-supervision and comparable\nwith human learning.\n  Second, we introduce a novel self-supervised image pretraining method\ntailored for learning from continuous videos. Existing methods typically adapt\nimage-based pretraining approaches to incorporate more frames. Instead, we\nadvocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\nleads to attention maps that Discover and tRAck objects over time in an\nend-to-end manner, using transformer cross-attention. We derive multiple views\nfrom the tracks and use them in a classical self-supervised distillation loss.\nUsing our novel approach, a single Walking Tours video remarkably becomes a\nstrong competitor to ImageNet for several image and video downstream tasks.",
        "translated": ""
    },
    {
        "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
        "url": "http://arxiv.org/abs/2310.09291v1",
        "pub_date": "2023-10-13",
        "summary": "Given an image and a target modification (e.g an image of the Eiffel tower\nand the text \"without people and at night-time\"), Compositional Image Retrieval\n(CIR) aims to retrieve the relevant target image in a database. While\nsupervised approaches rely on annotating triplets that is costly (i.e. query\nimage, textual modification, and target image), recent research sidesteps this\nneed by using large-scale vision-language models (VLMs), performing Zero-Shot\nCIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require\ntraining task-specific, customized models over large amounts of image-text\npairs. In this work, we propose to tackle CIR in a training-free manner via our\nCompositional Image Retrieval through Vision-by-Language (CIReVL), a simple,\nyet human-understandable and scalable pipeline that effectively recombines\nlarge-scale VLMs with large language models (LLMs). By captioning the reference\nimage using a pre-trained generative VLM and asking a LLM to recompose the\ncaption based on the textual target modification for subsequent retrieval via\ne.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we\nfind competitive, in-part state-of-the-art performance - improving over\nsupervised methods. Moreover, the modularity of CIReVL offers simple\nscalability without re-training, allowing us to both investigate scaling laws\nand bottlenecks for ZS-CIR while easily scaling up to in parts more than double\nof previously reported results. Finally, we show that CIReVL makes CIR\nhuman-understandable by composing image and text in a modular fashion in the\nlanguage domain, thereby making it intervenable, allowing to post-hoc re-align\nfailure cases. Code will be released upon acceptance.",
        "translated": ""
    },
    {
        "title": "An Unbiased Look at Datasets for Visuo-Motor Pre-Training",
        "url": "http://arxiv.org/abs/2310.09289v1",
        "pub_date": "2023-10-13",
        "summary": "Visual representation learning hold great promise for robotics, but is\nseverely hampered by the scarcity and homogeneity of robotics datasets. Recent\nworks address this problem by pre-training visual representations on\nlarge-scale but out-of-domain data (e.g., videos of egocentric interactions)\nand then transferring them to target robotics tasks. While the field is heavily\nfocused on developing better pre-training algorithms, we find that dataset\nchoice is just as important to this paradigm's success. After all, the\nrepresentation can only learn the structures or priors present in the\npre-training dataset. To this end, we flip the focus on algorithms, and instead\nconduct a dataset centric analysis of robotic pre-training. Our findings call\ninto question some common wisdom in the field. We observe that traditional\nvision datasets (like ImageNet, Kinetics and 100 Days of Hands) are\nsurprisingly competitive options for visuo-motor representation learning, and\nthat the pre-training dataset's image distribution matters more than its size.\nFinally, we show that common simulation benchmarks are not a reliable proxy for\nreal world performance and that simple regularization strategies can\ndramatically improve real world policy learning.\nhttps://data4robotics.github.io",
        "translated": ""
    },
    {
        "title": "SAIR: Learning Semantic-aware Implicit Representation",
        "url": "http://arxiv.org/abs/2310.09285v1",
        "pub_date": "2023-10-13",
        "summary": "Implicit representation of an image can map arbitrary coordinates in the\ncontinuous domain to their corresponding color values, presenting a powerful\ncapability for image reconstruction. Nevertheless, existing implicit\nrepresentation approaches only focus on building continuous appearance mapping,\nignoring the continuities of the semantic information across pixels. As a\nresult, they can hardly achieve desired reconstruction results when the\nsemantic information within input images is corrupted, for example, a large\nregion misses. To address the issue, we propose to learn semantic-aware\nimplicit representation (SAIR), that is, we make the implicit representation of\neach pixel rely on both its appearance and semantic information (\\eg, which\nobject does the pixel belong to). To this end, we propose a framework with two\nmodules: (1) building a semantic implicit representation (SIR) for a corrupted\nimage whose large regions miss. Given an arbitrary coordinate in the continuous\ndomain, we can obtain its respective text-aligned embedding indicating the\nobject the pixel belongs. (2) building an appearance implicit representation\n(AIR) based on the SIR. Given an arbitrary coordinate in the continuous domain,\nwe can reconstruct its color whether or not the pixel is missed in the input.\nWe validate the novel semantic-aware implicit representation method on the\nimage inpainting task, and the extensive experiments demonstrate that our\nmethod surpasses state-of-the-art approaches by a significant margin.",
        "translated": ""
    },
    {
        "title": "Transformer-based Multimodal Change Detection with Multitask Consistency\n  Constraints",
        "url": "http://arxiv.org/abs/2310.09276v1",
        "pub_date": "2023-10-13",
        "summary": "Change detection plays a fundamental role in Earth observation for analyzing\ntemporal iterations over time. However, recent studies have largely neglected\nthe utilization of multimodal data that presents significant practical and\ntechnical advantages compared to single-modal approaches. This research focuses\non leveraging digital surface model (DSM) data and aerial images captured at\ndifferent times for detecting change beyond 2D. We observe that the current\nchange detection methods struggle with the multitask conflicts between semantic\nand height change detection tasks. To address this challenge, we propose an\nefficient Transformer-based network that learns shared representation between\ncross-dimensional inputs through cross-attention. It adopts a consistency\nconstraint to establish the multimodal relationship, which involves obtaining\npseudo change through height change thresholding and minimizing the difference\nbetween semantic and pseudo change within their overlapping regions. A\nDSM-to-image multimodal dataset encompassing three cities in the Netherlands\nwas constructed. It lays a new foundation for beyond-2D change detection from\ncross-dimensional inputs. Compared to five state-of-the-art change detection\nmethods, our model demonstrates consistent multitask superiority in terms of\nsemantic and height change detection. Furthermore, the consistency strategy can\nbe seamlessly adapted to the other methods, yielding promising improvements.",
        "translated": ""
    },
    {
        "title": "Understanding and Modeling the Effects of Task and Context on Drivers'\n  Gaze Allocation",
        "url": "http://arxiv.org/abs/2310.09275v1",
        "pub_date": "2023-10-13",
        "summary": "Understanding what drivers look at is important for many applications,\nincluding driver training, monitoring, and assistance, as well as self-driving.\nTraditionally, factors affecting human visual attention have been divided into\nbottom-up (involuntary attraction to salient regions) and top-down (task- and\ncontext-driven). Although both play a role in drivers' gaze allocation, most of\nthe existing modeling approaches apply techniques developed for bottom-up\nsaliency and do not consider task and context influences explicitly. Likewise,\ncommon driving attention benchmarks lack relevant task and context annotations.\nTherefore, to enable analysis and modeling of these factors for drivers' gaze\nprediction, we propose the following: 1) address some shortcomings of the\npopular DR(eye)VE dataset and extend it with per-frame annotations for driving\ntask and context; 2) benchmark a number of baseline and SOTA models for\nsaliency and driver gaze prediction and analyze them w.r.t. the new\nannotations; and finally, 3) a novel model that modulates drivers' gaze\nprediction with explicit action and context information, and as a result\nsignificantly improves SOTA performance on DR(eye)VE overall (by 24\\% KLD and\n89\\% NSS) and on a subset of action and safety-critical intersection scenarios\n(by 10--30\\% KLD). Extended annotations, code for model and evaluation will be\nmade publicly available.",
        "translated": ""
    },
    {
        "title": "DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis",
        "url": "http://arxiv.org/abs/2310.11449v1",
        "pub_date": "2023-10-17",
        "summary": "Generating controllable and photorealistic digital human avatars is a\nlong-standing and important problem in Vision and Graphics. Recent methods have\nshown great progress in terms of either photorealism or inference speed while\nthe combination of the two desired properties still remains unsolved. To this\nend, we propose a novel method, called DELIFFAS, which parameterizes the\nappearance of the human as a surface light field that is attached to a\ncontrollable and deforming human mesh model. At the core, we represent the\nlight field around the human with a deformable two-surface parameterization,\nwhich enables fast and accurate inference of the human appearance. This allows\nperceptual supervision on the full image compared to previous approaches that\ncould only supervise individual pixels or small patches due to their slow\nruntime. Our carefully designed human representation and supervision strategy\nleads to state-of-the-art synthesis results and inference time. The video\nresults and code are available at\nhttps://vcai.mpi-inf.mpg.de/projects/DELIFFAS.",
        "translated": ""
    },
    {
        "title": "4K4D: Real-Time 4D View Synthesis at 4K Resolution",
        "url": "http://arxiv.org/abs/2310.11448v1",
        "pub_date": "2023-10-17",
        "summary": "This paper targets high-fidelity and real-time view synthesis of dynamic 3D\nscenes at 4K resolution. Recently, some methods on dynamic view synthesis have\nshown impressive rendering quality. However, their speed is still limited when\nrendering high-resolution images. To overcome this problem, we propose 4K4D, a\n4D point cloud representation that supports hardware rasterization and enables\nunprecedented rendering speed. Our representation is built on a 4D feature grid\nso that the points are naturally regularized and can be robustly optimized. In\naddition, we design a novel hybrid appearance model that significantly boosts\nthe rendering quality while preserving efficiency. Moreover, we develop a\ndifferentiable depth peeling algorithm to effectively learn the proposed model\nfrom RGB videos. Experiments show that our representation can be rendered at\nover 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the\nENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x\nfaster than previous methods and achieves the state-of-the-art rendering\nquality. We will release the code for reproducibility.",
        "translated": ""
    },
    {
        "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",
        "url": "http://arxiv.org/abs/2310.11441v1",
        "pub_date": "2023-10-17",
        "summary": "We present Set-of-Mark (SoM), a new visual prompting method, to unleash the\nvisual grounding abilities of large multimodal models (LMMs), such as GPT-4V.\nAs illustrated in Fig. 1 (right), we employ off-the-shelf interactive\nsegmentation models, such as SAM, to partition an image into regions at\ndifferent levels of granularity, and overlay these regions with a set of marks\ne.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can\nanswer the questions that require visual grounding. We perform a comprehensive\nempirical study to validate the effectiveness of SoM on a wide range of\nfine-grained vision and multimodal tasks. For example, our experiments show\nthat GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring\nsegmentation model on RefCOCOg in a zero-shot setting.",
        "translated": ""
    },
    {
        "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models",
        "url": "http://arxiv.org/abs/2310.11440v1",
        "pub_date": "2023-10-17",
        "summary": "The vision and language generative models have been overgrown in recent\nyears. For video generation, various open-sourced models and public-available\nservices are released for generating high-visual quality videos. However, these\nmethods often use a few academic metrics, for example, FVD or IS, to evaluate\nthe performance. We argue that it is hard to judge the large conditional\ngenerative models from the simple metrics since these models are often trained\non very large datasets with multi-aspect abilities. Thus, we propose a new\nframework and pipeline to exhaustively evaluate the performance of the\ngenerated videos. To achieve this, we first conduct a new prompt list for\ntext-to-video generation by analyzing the real-world prompt list with the help\nof the large language model. Then, we evaluate the state-of-the-art video\ngenerative models on our carefully designed benchmarks, in terms of visual\nqualities, content qualities, motion qualities, and text-caption alignment with\naround 18 objective metrics. To obtain the final leaderboard of the models, we\nalso fit a series of coefficients to align the objective metrics to the users'\nopinions. Based on the proposed opinion alignment method, our final score shows\na higher correlation than simply averaging the metrics, showing the\neffectiveness of the proposed evaluation method.",
        "translated": ""
    },
    {
        "title": "Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching",
        "url": "http://arxiv.org/abs/2310.11420v1",
        "pub_date": "2023-10-17",
        "summary": "We propose a novel unsupervised learning approach for non-rigid 3D shape\nmatching. Our approach improves upon recent state-of-the art deep functional\nmap methods and can be applied to a broad range of different challenging\nscenarios. Previous deep functional map methods mainly focus on feature\nextraction and aim exclusively at obtaining more expressive features for\nfunctional map computation. However, the importance of the functional map\ncomputation itself is often neglected and the relationship between the\nfunctional map and point-wise map is underexplored. In this paper, we\nsystematically investigate the coupling relationship between the functional map\nfrom the functional map solver and the point-wise map based on feature\nsimilarity. To this end, we propose a self-adaptive functional map solver to\nadjust the functional map regularisation for different shape matching\nscenarios, together with a vertex-wise contrastive loss to obtain more\ndiscriminative features. Using different challenging datasets (including\nnon-isometry, topological noise and partiality), we demonstrate that our method\nsubstantially outperforms previous state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum\n  Computing",
        "url": "http://arxiv.org/abs/2310.12153v1",
        "pub_date": "2023-10-18",
        "summary": "Adiabatic quantum computing (AQC) is a promising quantum computing approach\nfor discrete and often NP-hard optimization problems. Current AQCs allow to\nimplement problems of research interest, which has sparked the development of\nquantum representations for many machine learning and computer vision tasks.\nDespite requiring multiple measurements from the noisy AQC, current approaches\nonly utilize the best measurement, discarding information contained in the\nremaining ones. In this work, we explore the potential of using this\ninformation for probabilistic balanced k-means clustering. Instead of\ndiscarding non-optimal solutions, we propose to use them to compute calibrated\nposterior probabilities with little additional compute cost. This allows us to\nidentify ambiguous solutions and data points, which we demonstrate on a D-Wave\nAQC on synthetic and real data.",
        "translated": ""
    },
    {
        "title": "Learning from Rich Semantics and Coarse Locations for Long-tailed Object\n  Detection",
        "url": "http://arxiv.org/abs/2310.12152v1",
        "pub_date": "2023-10-18",
        "summary": "Long-tailed object detection (LTOD) aims to handle the extreme data imbalance\nin real-world datasets, where many tail classes have scarce instances. One\npopular strategy is to explore extra data with image-level labels, yet it\nproduces limited results due to (1) semantic ambiguity -- an image-level label\nonly captures a salient part of the image, ignoring the remaining rich\nsemantics within the image; and (2) location sensitivity -- the label highly\ndepends on the locations and crops of the original image, which may change\nafter data transformations like random cropping. To remedy this, we propose\nRichSem, a simple but effective method, which is robust to learn rich semantics\nfrom coarse locations without the need of accurate bounding boxes. RichSem\nleverages rich semantics from images, which are then served as additional soft\nsupervision for training detectors. Specifically, we add a semantic branch to\nour detector to learn these soft semantics and enhance feature representations\nfor long-tailed object detection. The semantic branch is only used for training\nand is removed during inference. RichSem achieves consistent improvements on\nboth overall and rare-category of LVIS under different backbones and detectors.\nOur method achieves state-of-the-art performance without requiring complex\ntraining and testing procedures. Moreover, we show the effectiveness of our\nmethod on other long-tailed datasets with additional experiments. Code is\navailable at \\url{https://github.com/MengLcool/RichSem}.",
        "translated": ""
    },
    {
        "title": "Object-aware Inversion and Reassembly for Image Editing",
        "url": "http://arxiv.org/abs/2310.12149v1",
        "pub_date": "2023-10-18",
        "summary": "By comparing the original and target prompts in editing task, we can obtain\nnumerous editing pairs, each comprising an object and its corresponding editing\ntarget. To allow editability while maintaining fidelity to the input image,\nexisting editing methods typically involve a fixed number of inversion steps\nthat project the whole input image to its noisier latent representation,\nfollowed by a denoising process guided by the target prompt. However, we find\nthat the optimal number of inversion steps for achieving ideal editing results\nvaries significantly among different editing pairs, owing to varying editing\ndifficulties. Therefore, the current literature, which relies on a fixed number\nof inversion steps, produces sub-optimal generation quality, especially when\nhandling multiple editing pairs in a natural image. To this end, we propose a\nnew image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR),\nto enable object-level fine-grained editing. Specifically, we design a new\nsearch metric, which determines the optimal inversion steps for each editing\npair, by jointly considering the editability of the target and the fidelity of\nthe non-editing region. We use our search metric to find the optimal inversion\nstep for each editing pair when editing an image. We then edit these editing\npairs separately to avoid concept mismatch. Subsequently, we propose an\nadditional reassembly step to seamlessly integrate the respective editing\nresults and the non-editing region to obtain the final edited image. To\nsystematically evaluate the effectiveness of our method, we collect two\ndatasets for benchmarking single- and multi-object editing, respectively.\nExperiments demonstrate that our method achieves superior performance in\nediting object shapes, colors, materials, categories, etc., especially in\nmulti-object editing scenarios.",
        "translated": ""
    },
    {
        "title": "InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot\n  Interactions",
        "url": "http://arxiv.org/abs/2310.12147v1",
        "pub_date": "2023-10-18",
        "summary": "Ambiguity is ubiquitous in human communication. Previous approaches in\nHuman-Robot Interaction (HRI) have often relied on predefined interaction\ntemplates, leading to reduced performance in realistic and open-ended\nscenarios. To address these issues, we present a large-scale dataset, \\invig,\nfor interactive visual grounding under language ambiguity. Our dataset\ncomprises over 520K images accompanied by open-ended goal-oriented\ndisambiguation dialogues, encompassing millions of object instances and\ncorresponding question-answer pairs. Leveraging the \\invig dataset, we conduct\nextensive studies and propose a set of baseline solutions for end-to-end\ninteractive visual disambiguation and grounding, achieving a 45.6\\% success\nrate during validation. To the best of our knowledge, the \\invig dataset is the\nfirst large-scale dataset for resolving open-ended interactive visual\ngrounding, presenting a practical yet highly challenging benchmark for\nambiguity-aware HRI. Codes and datasets are available at:\n\\href{https://openivg.github.io}{https://openivg.github.io}.",
        "translated": ""
    },
    {
        "title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning",
        "url": "http://arxiv.org/abs/2310.12128v1",
        "pub_date": "2023-10-18",
        "summary": "Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows,\nconnection lines, etc.). Existing state-of-the-art T2I models often fail at\ndiagram generation because they lack fine-grained object layout control when\nmany objects are densely connected via complex relations such as arrows/lines\nand also often fail to render comprehensible text labels. To address this gap,\nwe present DiagrammerGPT, a novel two-stage text-to-diagram generation\nframework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)\nto generate more accurate open-domain, open-platform diagrams. In the first\nstage, we use LLMs to generate and iteratively refine 'diagram plans' (in a\nplanner-auditor feedback loop) which describe all the entities (objects and\ntext labels), their relationships (arrows or lines), and their bounding box\nlayouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a\ntext label rendering module to generate diagrams following the diagram plans.\nTo benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nquantitatively and qualitatively that our DiagrammerGPT framework produces more\naccurate diagrams, outperforming existing T2I models. We also provide\ncomprehensive analysis including open-domain diagram generation, vector graphic\ndiagram generation in different platforms, human-in-the-loop diagram plan\nediting, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our\nwork can inspire further research on diagram generation via T2I models and\nLLMs.",
        "translated": ""
    },
    {
        "title": "Putting the Object Back into Video Object Segmentation",
        "url": "http://arxiv.org/abs/2310.12982v1",
        "pub_date": "2023-10-19",
        "summary": "We present Cutie, a video object segmentation (VOS) network with object-level\nmemory reading, which puts the object representation from memory back into the\nvideo object segmentation result. Recent works on VOS employ bottom-up\npixel-level memory reading which struggles due to matching noise, especially in\nthe presence of distractors, resulting in lower performance in more challenging\ndata. In contrast, Cutie performs top-down object-level memory reading by\nadapting a small set of object queries for restructuring and interacting with\nthe bottom-up pixel features iteratively with a query-based object transformer\n(qt, hence Cutie). The object queries act as a high-level summary of the target\nobject, while high-resolution feature maps are retained for accurate\nsegmentation. Together with foreground-background masked attention, Cutie\ncleanly separates the semantics of the foreground object from the background.\nOn the challenging MOSE dataset, Cutie improves by 8.7 J&amp;F over XMem with a\nsimilar running time and improves by 4.2 J&amp;F over DeAOT while running three\ntimes as fast. Code is available at: https://hkchengrex.github.io/Cutie",
        "translated": ""
    },
    {
        "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
        "url": "http://arxiv.org/abs/2310.12978v1",
        "pub_date": "2023-10-19",
        "summary": "This work targets a novel text-driven whole-body motion generation task,\nwhich takes a given textual description as input and aims at generating\nhigh-quality, diverse, and coherent facial expressions, hand gestures, and body\nmotions simultaneously. Previous works on text-driven motion generation tasks\nmainly have two limitations: they ignore the key role of fine-grained hand and\nface controlling in vivid whole-body motion generation, and lack a good\nalignment between text and motion. To address such limitations, we propose a\nText-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which\nis the first attempt to our knowledge towards applicable holistic motion\ngeneration in this research area. To tackle this challenging task, our solution\nincludes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and\na Hierarchical-GPT for fine-grained body and hand motion reconstruction and\ngeneration with two structured codebooks; and (2) a pre-trained\ntext-motion-alignment model to help generated motion align with the input\ntextual description explicitly. Comprehensive experiments verify that our model\nhas significant advantages in both the quality of generated motions and their\nalignment with text.",
        "translated": ""
    },
    {
        "title": "On the Hidden Waves of Image",
        "url": "http://arxiv.org/abs/2310.12976v1",
        "pub_date": "2023-10-19",
        "summary": "In this paper, we introduce an intriguing phenomenon-the successful\nreconstruction of images using a set of one-way wave equations with hidden and\nlearnable speeds. Each individual image corresponds to a solution with a unique\ninitial condition, which can be computed from the original image using a visual\nencoder (e.g., a convolutional neural network). Furthermore, the solution for\neach image exhibits two noteworthy mathematical properties: (a) it can be\ndecomposed into a collection of special solutions of the same one-way wave\nequations that are first-order autoregressive, with shared coefficient matrices\nfor autoregression, and (b) the product of these coefficient matrices forms a\ndiagonal matrix with the speeds of the wave equations as its diagonal elements.\nWe term this phenomenon hidden waves, as it reveals that, although the speeds\nof the set of wave equations and autoregressive coefficient matrices are\nlatent, they are both learnable and shared across images. This represents a\nmathematical invariance across images, providing a new mathematical perspective\nto understand images.",
        "translated": ""
    },
    {
        "title": "Variational Inference for SDEs Driven by Fractional Noise",
        "url": "http://arxiv.org/abs/2310.12975v1",
        "pub_date": "2023-10-19",
        "summary": "We present a novel variational framework for performing inference in (neural)\nstochastic differential equations (SDEs) driven by Markov-approximate\nfractional Brownian motion (fBM). SDEs offer a versatile tool for modeling\nreal-world continuous-time dynamic systems with inherent noise and randomness.\nCombining SDEs with the powerful inference capabilities of variational methods,\nenables the learning of representative function distributions through\nstochastic gradient descent. However, conventional SDEs typically assume the\nunderlying noise to follow a Brownian motion (BM), which hinders their ability\nto capture long-term dependencies. In contrast, fractional Brownian motion\n(fBM) extends BM to encompass non-Markovian dynamics, but existing methods for\ninferring fBM parameters are either computationally demanding or statistically\ninefficient. In this paper, building upon the Markov approximation of fBM, we\nderive the evidence lower bound essential for efficient variational inference\nof posterior path measures, drawing from the well-established field of\nstochastic analysis. Additionally, we provide a closed-form expression to\ndetermine optimal approximation coefficients. Furthermore, we propose the use\nof neural networks to learn the drift, diffusion and control terms within our\nvariational posterior, leading to the variational training of neural-SDEs. In\nthis framework, we also optimize the Hurst index, governing the nature of our\nfractional noise. Beyond validation on synthetic data, we contribute a novel\narchitecture for variational latent video prediction,-an approach that, to the\nbest of our knowledge, enables the first variational neural-SDE application to\nvideo perception.",
        "translated": ""
    },
    {
        "title": "FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects",
        "url": "http://arxiv.org/abs/2310.12974v1",
        "pub_date": "2023-10-19",
        "summary": "In this work, we address the challenging task of 3D object recognition\nwithout the reliance on real-world 3D labeled data. Our goal is to predict the\n3D shape, size, and 6D pose of objects within a single RGB-D image, operating\nat the category level and eliminating the need for CAD models during inference.\nWhile existing self-supervised methods have made strides in this field, they\noften suffer from inefficiencies arising from non-end-to-end processing,\nreliance on separate models for different object categories, and slow surface\nextraction during the training of implicit reconstruction models; thus\nhindering both the speed and real-world applicability of the 3D recognition\nprocess. Our proposed method leverages a multi-stage training pipeline,\ndesigned to efficiently transfer synthetic performance to the real-world\ndomain. This approach is achieved through a combination of 2D and 3D supervised\nlosses during the synthetic domain training, followed by the incorporation of\n2D supervised and 3D self-supervised losses on real-world data in two\nadditional learning stages. By adopting this comprehensive strategy, our method\nsuccessfully overcomes the aforementioned limitations and outperforms existing\nself-supervised 6D pose and size estimation baselines on the NOCS test-set with\na 16.4% absolute improvement in mAP for 6D pose estimation while running in\nnear real-time at 5 Hz.",
        "translated": ""
    },
    {
        "title": "Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias\n  in Face-Recognition Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2310.13674v1",
        "pub_date": "2023-10-20",
        "summary": "Convolutional neural network (CNN), as an important model in artificial\nintelligence, has been widely used and studied in different disciplines. The\ncomputational mechanisms of CNNs are still not fully revealed due to the their\ncomplex nature. In this study, we focused on 4 extensively studied CNNs\n(AlexNet, VGG11, VGG13, and VGG16) which has been analyzed as human-like models\nby neuroscientists with ample evidence. We trained these CNNs to emotion\nvalence classification task by transfer learning. Comparing their performance\nwith human data, the data unveiled that these CNNs would partly perform as\nhuman does. We then update the object-based AlexNet using self-attention\nmechanism based on neuroscience and behavioral data. The updated FE-AlexNet\noutperformed all the other tested CNNs and closely resembles human perception.\nThe results further unveil the computational mechanisms of these CNNs.\nMoreover, this study offers a new paradigm to better understand and improve CNN\nperformance via human data.",
        "translated": ""
    },
    {
        "title": "ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot\n  Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2310.13670v1",
        "pub_date": "2023-10-20",
        "summary": "Novel view synthesis has recently made significant progress with the advent\nof Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to\nachieve this task from only a few images by introducing a new loss function for\nunknown viewpoints with no input images. The loss function assumes that a\npre-trained feature extractor should output the same feature even if input\nimages are captured at different viewpoints since the images contain the same\nobject. However, while that assumption is ideal, in reality, it is known that\nas viewpoints continuously change, also feature vectors continuously change.\nThus, the assumption can harm training. To avoid this harmful training, we\npropose ManifoldNeRF, a method for supervising feature vectors at unknown\nviewpoints using interpolated features from neighboring known viewpoints. Since\nthe method provides appropriate supervision for each unknown viewpoint by the\ninterpolated features, the volume representation is learned better than\nDietNeRF. Experimental results show that the proposed method performs better\nthan others in a complex scene. We also experimented with several subsets of\nviewpoints from a set of viewpoints and identified an effective set of\nviewpoints for real environments. This provided a basic policy of viewpoint\npatterns for real-world application. The code is available at\nhttps://github.com/haganelego/ManifoldNeRF_BMVC2023",
        "translated": ""
    },
    {
        "title": "Deep-Learning-based Change Detection with Spaceborne Hyperspectral\n  PRISMA data",
        "url": "http://arxiv.org/abs/2310.13627v1",
        "pub_date": "2023-10-20",
        "summary": "Change detection (CD) methods have been applied to optical data for decades,\nwhile the use of hyperspectral data with a fine spectral resolution has been\nrarely explored. CD is applied in several sectors, such as environmental\nmonitoring and disaster management. Thanks to the PRecursore IperSpettrale\ndella Missione operativA (PRISMA), hyperspectral-from-space CD is now possible.\nIn this work, we apply standard and deep-learning (DL) CD methods to different\ntargets, from natural to urban areas. We propose a pipeline starting from\ncoregistration, followed by CD with a full-spectrum algorithm and by a DL\nnetwork developed for optical data. We find that changes in vegetation and\nbuilt environments are well captured. The spectral information is valuable to\nidentify subtle changes and the DL methods are less affected by noise compared\nto the statistical method, but atmospheric effects and the lack of reliable\nground truth represent a major challenge to hyperspectral CD.",
        "translated": ""
    },
    {
        "title": "What you see is what you get: Experience ranking with deep neural\n  dataset-to-dataset similarity for topological localisation",
        "url": "http://arxiv.org/abs/2310.13622v1",
        "pub_date": "2023-10-20",
        "summary": "Recalling the most relevant visual memories for localisation or understanding\na priori the likely outcome of localisation effort against a particular visual\nmemory is useful for efficient and robust visual navigation. Solutions to this\nproblem should be divorced from performance appraisal against ground truth - as\nthis is not available at run-time - and should ideally be based on\ngeneralisable environmental observations. For this, we propose applying the\nrecently developed Visual DNA as a highly scalable tool for comparing datasets\nof images - in this work, sequences of map and live experiences. In the case of\nlocalisation, important dataset differences impacting performance are modes of\nappearance change, including weather, lighting, and season. Specifically, for\nany deep architecture which is used for place recognition by matching feature\nvolumes at a particular layer, we use distribution measures to compare\nneuron-wise activation statistics between live images and multiple previously\nrecorded past experiences, with a potentially large seasonal (winter/summer) or\ntime of day (day/night) shift. We find that differences in these statistics\ncorrelate to performance when localising using a past experience with the same\nappearance gap. We validate our approach over the Nordland cross-season dataset\nas well as data from Oxford's University Parks with lighting and mild seasonal\nchange, showing excellent ability of our system to rank actual localisation\nperformance across candidate experiences.",
        "translated": ""
    },
    {
        "title": "Semi-supervised multimodal coreference resolution in image narrations",
        "url": "http://arxiv.org/abs/2310.13619v1",
        "pub_date": "2023-10-20",
        "summary": "In this paper, we study multimodal coreference resolution, specifically where\na longer descriptive text, i.e., a narration is paired with an image. This\nposes significant challenges due to fine-grained image-text alignment, inherent\nambiguity present in narrative language, and unavailability of large annotated\ntraining sets. To tackle these challenges, we present a data efficient\nsemi-supervised approach that utilizes image-narration pairs to resolve\ncoreferences and narrative grounding in a multimodal context. Our approach\nincorporates losses for both labeled and unlabeled data within a cross-modal\nframework. Our evaluation shows that the proposed approach outperforms strong\nbaselines both quantitatively and qualitatively, for the tasks of coreference\nresolution and narrative grounding.",
        "translated": ""
    },
    {
        "title": "RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions",
        "url": "http://arxiv.org/abs/2310.15171v1",
        "pub_date": "2023-10-23",
        "summary": "Depth estimation from monocular images is pivotal for real-world visual\nperception systems. While current learning-based depth estimation models train\nand test on meticulously curated data, they often overlook out-of-distribution\n(OoD) situations. Yet, in practical settings -- especially safety-critical ones\nlike autonomous driving -- common corruptions can arise. Addressing this\noversight, we introduce a comprehensive robustness test suite, RoboDepth,\nencompassing 18 corruptions spanning three categories: i) weather and lighting\nconditions; ii) sensor failures and movement; and iii) data processing\nanomalies. We subsequently benchmark 42 depth estimation models across indoor\nand outdoor scenes to assess their resilience to these corruptions. Our\nfindings underscore that, in the absence of a dedicated robustness evaluation\nframework, many leading depth estimation models may be susceptible to typical\ncorruptions. We delve into design considerations for crafting more robust depth\nestimation models, touching upon pre-training, augmentation, modality, model\ncapacity, and learning paradigms. We anticipate our benchmark will establish a\nfoundational platform for advancing robust OoD depth estimation.",
        "translated": ""
    },
    {
        "title": "FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling",
        "url": "http://arxiv.org/abs/2310.15169v1",
        "pub_date": "2023-10-23",
        "summary": "With the availability of large-scale video datasets and the advances of\ndiffusion models, text-driven video generation has achieved substantial\nprogress. However, existing video generation models are typically trained on a\nlimited number of frames, resulting in the inability to generate high-fidelity\nlong videos during inference. Furthermore, these models only support\nsingle-text conditions, whereas real-life scenarios often require multi-text\nconditions as the video content changes over time. To tackle these challenges,\nthis study explores the potential of extending the text-driven capability to\ngenerate longer videos conditioned on multiple texts. 1) We first analyze the\nimpact of initial noise in video diffusion models. Then building upon the\nobservation of noise, we propose FreeNoise, a tuning-free and time-efficient\nparadigm to enhance the generative capabilities of pretrained video diffusion\nmodels while preserving content consistency. Specifically, instead of\ninitializing noises for all frames, we reschedule a sequence of noises for\nlong-range correlation and perform temporal attention over them by window-based\nfunction. 2) Additionally, we design a novel motion injection method to support\nthe generation of videos conditioned on multiple text prompts. Extensive\nexperiments validate the superiority of our paradigm in extending the\ngenerative capabilities of video diffusion models. It is noteworthy that\ncompared with the previous best-performing method which brought about 255%\nextra time cost, our method incurs only negligible time cost of approximately\n17%. Generated video samples are available at our website:\nhttp://haonanqiu.com/projects/FreeNoise.html.",
        "translated": ""
    },
    {
        "title": "Ghost on the Shell: An Expressive Representation of General 3D Shapes",
        "url": "http://arxiv.org/abs/2310.15168v1",
        "pub_date": "2023-10-23",
        "summary": "The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Visual Reasoning Coordinators",
        "url": "http://arxiv.org/abs/2310.15166v1",
        "pub_date": "2023-10-23",
        "summary": "Visual reasoning requires multimodal perception and commonsense cognition of\nthe world. Recently, multiple vision-language models (VLMs) have been proposed\nwith excellent commonsense reasoning ability in various domains. However, how\nto harness the collective power of these complementary VLMs is rarely explored.\nExisting methods like ensemble still struggle to aggregate these models with\nthe desired higher-order communications. In this work, we propose Cola, a novel\nparadigm that coordinates multiple VLMs for visual reasoning. Our key insight\nis that a large language model (LLM) can efficiently coordinate multiple VLMs\nby facilitating natural language communication that leverages their distinct\nand complementary capabilities. Extensive experiments demonstrate that our\ninstruction tuning variant, Cola-FT, achieves state-of-the-art performance on\nvisual question answering (VQA), outside knowledge VQA, visual entailment, and\nvisual spatial reasoning tasks. Moreover, we show that our in-context learning\nvariant, Cola-Zero, exhibits competitive performance in zero and few-shot\nsettings, without finetuning. Through systematic ablation studies and\nvisualizations, we validate that a coordinator LLM indeed comprehends the\ninstruction prompts as well as the separate functionalities of VLMs; it then\ncoordinates them to enable impressive visual reasoning capabilities.",
        "translated": ""
    },
    {
        "title": "Handling Data Heterogeneity via Architectural Design for Federated\n  Visual Recognition",
        "url": "http://arxiv.org/abs/2310.15165v1",
        "pub_date": "2023-10-23",
        "summary": "Federated Learning (FL) is a promising research paradigm that enables the\ncollaborative training of machine learning models among various parties without\nthe need for sensitive information exchange. Nonetheless, retaining data in\nindividual clients introduces fundamental challenges to achieving performance\non par with centrally trained models. Our study provides an extensive review of\nfederated learning applied to visual recognition. It underscores the critical\nrole of thoughtful architectural design choices in achieving optimal\nperformance, a factor often neglected in the FL literature. Many existing FL\nsolutions are tested on shallow or simple networks, which may not accurately\nreflect real-world applications. This practice restricts the transferability of\nresearch findings to large-scale visual recognition models. Through an in-depth\nanalysis of diverse cutting-edge architectures such as convolutional neural\nnetworks, transformers, and MLP-mixers, we experimentally demonstrate that\narchitectural choices can substantially enhance FL systems' performance,\nparticularly when handling heterogeneous data. We study 19 visual recognition\nmodels from five different architectural families on four challenging FL\ndatasets. We also re-investigate the inferior performance of convolution-based\narchitectures in the FL setting and analyze the influence of normalization\nlayers on the FL performance. Our findings emphasize the importance of\narchitectural design for computer vision tasks in practical scenarios,\neffectively narrowing the performance gap between federated and centralized\nlearning. Our source code is available at\nhttps://github.com/sarapieri/fed_het.git.",
        "translated": ""
    },
    {
        "title": "Synthetic Data as Validation",
        "url": "http://arxiv.org/abs/2310.16052v1",
        "pub_date": "2023-10-24",
        "summary": "This study leverages synthetic data as a validation set to reduce overfitting\nand ease the selection of the best model in AI development. While synthetic\ndata have been used for augmenting the training set, we find that synthetic\ndata can also significantly diversify the validation set, offering marked\nadvantages in domains like healthcare, where data are typically limited,\nsensitive, and from out-domain sources (i.e., hospitals). In this study, we\nillustrate the effectiveness of synthetic data for early cancer detection in\ncomputed tomography (CT) volumes, where synthetic tumors are generated and\nsuperimposed onto healthy organs, thereby creating an extensive dataset for\nrigorous validation. Using synthetic data as validation can improve AI\nrobustness in both in-domain and out-domain test sets. Furthermore, we\nestablish a new continual learning framework that continuously trains AI models\non a stream of out-domain data with synthetic tumors. The AI model trained and\nvalidated in dynamically expanding synthetic data can consistently outperform\nmodels trained and validated exclusively on real-world data. Specifically, the\nDSC score for liver tumor segmentation improves from 26.7% (95% CI:\n22.6%-30.9%) to 34.5% (30.8%-38.2%) when evaluated on an in-domain dataset and\nfrom 31.1% (26.0%-36.2%) to 35.4% (32.1%-38.7%) on an out-domain dataset.\nImportantly, the performance gain is particularly significant in identifying\nvery tiny liver tumors (radius &lt; 5mm) in CT volumes, with Sensitivity improving\nfrom 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain\ndataset, justifying the efficacy in early detection of cancer. The application\nof synthetic data, from both training and validation perspectives, underlines a\npromising avenue to enhance AI robustness when dealing with data from varying\ndomains.",
        "translated": ""
    },
    {
        "title": "From Posterior Sampling to Meaningful Diversity in Image Restoration",
        "url": "http://arxiv.org/abs/2310.16047v1",
        "pub_date": "2023-10-24",
        "summary": "Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR",
        "translated": ""
    },
    {
        "title": "Woodpecker: Hallucination Correction for Multimodal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2310.16045v1",
        "pub_date": "2023-10-24",
        "summary": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal\nLarge Language Models (MLLMs), referring to the phenomenon that the generated\ntext is inconsistent with the image content. In order to mitigate\nhallucinations, existing studies mainly resort to an instruction-tuning manner\nthat requires retraining the models with specific data. In this paper, we pave\na different way, introducing a training-free method named Woodpecker. Like a\nwoodpecker heals trees, it picks out and corrects hallucinations from the\ngenerated text. Concretely, Woodpecker consists of five stages: key concept\nextraction, question formulation, visual knowledge validation, visual claim\ngeneration, and hallucination correction. Implemented in a post-remedy manner,\nWoodpecker can easily serve different MLLMs, while being interpretable by\naccessing intermediate outputs of the five stages. We evaluate Woodpecker both\nquantitatively and qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement\nin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released\nat https://github.com/BradyFU/Woodpecker.",
        "translated": ""
    },
    {
        "title": "Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark",
        "url": "http://arxiv.org/abs/2310.16044v1",
        "pub_date": "2023-10-24",
        "summary": "We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering\nBenchmark. Recent advances in inverse rendering have enabled a wide range of\nreal-world applications in 3D content generation, moving rapidly from research\nand commercial use cases to consumer devices. While the results continue to\nimprove, there is no real-world benchmark that can quantitatively assess and\ncompare the performance of various inverse rendering methods. Existing\nreal-world datasets typically only consist of the shape and multi-view images\nof objects, which are not sufficient for evaluating the quality of material\nrecovery and object relighting. Methods capable of recovering material and\nlighting often resort to synthetic data for quantitative evaluation, which on\nthe other hand does not guarantee generalization to complex real-world\nenvironments. We introduce a new dataset of real-world objects captured under a\nvariety of natural scenes with ground-truth 3D scans, multi-view images, and\nenvironment lighting. Using this dataset, we establish the first comprehensive\nreal-world evaluation benchmark for object inverse rendering tasks from\nin-the-wild scenes, and compare the performance of various existing methods.\nAll data, code, and models can be accessed at https://stanfordorb.github.io/.",
        "translated": ""
    },
    {
        "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
        "url": "http://arxiv.org/abs/2310.16035v1",
        "pub_date": "2023-10-24",
        "summary": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.",
        "translated": ""
    },
    {
        "title": "SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous\n  Manipulation",
        "url": "http://arxiv.org/abs/2310.16838v1",
        "pub_date": "2023-10-25",
        "summary": "Humans excel at transferring manipulation skills across diverse object\nshapes, poses, and appearances due to their understanding of semantic\ncorrespondences between different instances. To endow robots with a similar\nhigh-level understanding, we develop a Distilled Feature Field (DFF) for 3D\nscenes, leveraging large 2D vision models to distill semantic features from\nmultiview images. While current research demonstrates advanced performance in\nreconstructing DFFs from dense views, the development of learning a DFF from\nsparse views is relatively nascent, despite its prevalence in numerous\nmanipulation tasks with fixed cameras. In this work, we introduce SparseDFF, a\nnovel method for acquiring view-consistent 3D DFFs from sparse RGBD\nobservations, enabling one-shot learning of dexterous manipulations that are\ntransferable to novel scenes. Specifically, we map the image features to the 3D\npoint cloud, allowing for propagation across the 3D space to establish a dense\nfeature field. At the core of SparseDFF is a lightweight feature refinement\nnetwork, optimized with a contrastive loss between pairwise views after\nback-projecting the image features onto the 3D point cloud. Additionally, we\nimplement a point-pruning mechanism to augment feature continuity within each\nlocal neighborhood. By establishing coherent feature fields on both source and\ntarget scenes, we devise an energy function that facilitates the minimization\nof feature discrepancies w.r.t. the end-effector parameters between the\ndemonstration and the target manipulation. We evaluate our approach using a\ndexterous hand, mastering real-world manipulations on both rigid and deformable\nobjects, and showcase robust generalization in the face of object and\nscene-context variations.",
        "translated": ""
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "url": "http://arxiv.org/abs/2310.16836v1",
        "pub_date": "2023-10-25",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
        "translated": ""
    },
    {
        "title": "Proposal-Contrastive Pretraining for Object Detection from Fewer Data",
        "url": "http://arxiv.org/abs/2310.16835v1",
        "pub_date": "2023-10-25",
        "summary": "The use of pretrained deep neural networks represents an attractive way to\nachieve strong results with few data available. When specialized in dense\nproblems such as object detection, learning local rather than global\ninformation in images has proven to be more efficient. However, for\nunsupervised pretraining, the popular contrastive learning requires a large\nbatch size and, therefore, a lot of resources. To address this problem, we are\ninterested in transformer-based object detectors that have recently gained\ntraction in the community with good performance and with the particularity of\ngenerating many diverse object proposals.\n  In this work, we present Proposal Selection Contrast (ProSeCo), a novel\nunsupervised overall pretraining approach that leverages this property. ProSeCo\nuses the large number of object proposals generated by the detector for\ncontrastive learning, which allows the use of a smaller batch size, combined\nwith object-level features to learn local information in the images. To improve\nthe effectiveness of the contrastive loss, we introduce the object location\ninformation in the selection of positive examples to take into account multiple\noverlapping object proposals. When reusing pretrained backbone, we advocate for\nconsistency in learning local information between the backbone and the\ndetection head.\n  We show that our method outperforms state of the art in unsupervised\npretraining for object detection on standard and novel benchmarks in learning\nwith fewer data.",
        "translated": ""
    },
    {
        "title": "LightSpeed: Light and Fast Neural Light Fields on Mobile Devices",
        "url": "http://arxiv.org/abs/2310.16832v1",
        "pub_date": "2023-10-25",
        "summary": "Real-time novel-view image synthesis on mobile devices is prohibitive due to\nthe limited computational power and storage. Using volumetric rendering\nmethods, such as NeRF and its derivatives, on mobile devices is not suitable\ndue to the high computational cost of volumetric rendering. On the other hand,\nrecent advances in neural light field representations have shown promising\nreal-time view synthesis results on mobile devices. Neural light field methods\nlearn a direct mapping from a ray representation to the pixel color. The\ncurrent choice of ray representation is either stratified ray sampling or\nPl\\\"{u}cker coordinates, overlooking the classic light slab (two-plane)\nrepresentation, the preferred representation to interpolate between light field\nviews. In this work, we find that using the light slab representation is an\nefficient representation for learning a neural light field. More importantly,\nit is a lower-dimensional ray representation enabling us to learn the 4D ray\nspace using feature grids which are significantly faster to train and render.\nAlthough mostly designed for frontal views, we show that the light-slab\nrepresentation can be further extended to non-frontal scenes using a\ndivide-and-conquer strategy. Our method offers superior rendering quality\ncompared to previous light field methods and achieves a significantly improved\ntrade-off between rendering quality and speed.",
        "translated": ""
    },
    {
        "title": "PERF: Panoramic Neural Radiance Field from a Single Panorama",
        "url": "http://arxiv.org/abs/2310.16831v1",
        "pub_date": "2023-10-25",
        "summary": "Neural Radiance Field (NeRF) has achieved substantial progress in novel view\nsynthesis given multi-view images. Recently, some works have attempted to train\na NeRF from a single image with 3D priors. They mainly focus on a limited field\nof view and there are few invisible occlusions, which greatly limits their\nscalability to real-world 360-degree panoramic scenarios with large-size\nocclusions. In this paper, we present PERF, a 360-degree novel view synthesis\nframework that trains a panoramic neural radiance field from a single panorama.\nNotably, PERF allows 3D roaming in a complex scene without expensive and\ntedious image collection. To achieve this goal, we propose a novel\ncollaborative RGBD inpainting method and a progressive inpainting-and-erasing\nmethod to lift up a 360-degree 2D scene to a 3D scene. Specifically, we first\npredict a panoramic depth map as initialization given a single panorama, and\nreconstruct visible 3D regions with volume rendering. Then we introduce a\ncollaborative RGBD inpainting approach into a NeRF for completing RGB images\nand depth maps from random views, which is derived from an RGB Stable Diffusion\nmodel and a monocular depth estimator. Finally, we introduce an\ninpainting-and-erasing strategy to avoid inconsistent geometry between a\nnewly-sampled view and reference views. The two components are integrated into\nthe learning of NeRFs in a unified optimization framework and achieve promising\nresults. Extensive experiments on Replica and a new dataset PERF-in-the-wild\ndemonstrate the superiority of our PERF over state-of-the-art methods. Our PERF\ncan be widely used for real-world applications, such as panorama-to-3D,\ntext-to-3D, and 3D scene stylization applications. Project page and code are\navailable at https://perf-project.github.io/.",
        "translated": ""
    },
    {
        "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of\n  General Knowledge Transfer between Any Pretrained Model",
        "url": "http://arxiv.org/abs/2310.17653v1",
        "pub_date": "2023-10-26",
        "summary": "Training deep networks requires various design decisions regarding for\ninstance their architecture, data augmentation, or optimization. In this work,\nwe find these training variations to result in networks learning unique feature\nsets from the data. Using public model libraries comprising thousands of models\ntrained on canonical datasets like ImageNet, we observe that for arbitrary\npairings of pretrained models, one model extracts significant data context\nunavailable in the other -- independent of overall performance. Given any\narbitrary pairing of pretrained models and no external rankings (such as\nseparate test sets, e.g. due to data privacy), we investigate if it is possible\nto transfer such \"complementary\" knowledge from one model to another without\nperformance degradation -- a task made particularly difficult as additional\nknowledge can be contained in stronger, equiperformant or weaker models. Yet\nfacilitating robust transfer in scenarios agnostic to pretrained model pairings\nwould unlock auxiliary gains and knowledge fusion from any model repository\nwithout restrictions on model and problem specifics - including from weaker,\nlower-performance models. This work therefore provides an initial, in-depth\nexploration on the viability of such general-purpose knowledge transfer. Across\nlarge-scale experiments, we first reveal the shortcomings of standard knowledge\ndistillation techniques, and then propose a much more general extension through\ndata partitioning for successful transfer between nearly all pretrained models,\nwhich we show can also be done unsupervised. Finally, we assess both the\nscalability and impact of fundamental model properties on successful\nmodel-agnostic knowledge transfer.",
        "translated": ""
    },
    {
        "title": "A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised\n  Video Anomaly Detection",
        "url": "http://arxiv.org/abs/2310.17650v1",
        "pub_date": "2023-10-26",
        "summary": "Detection of anomalous events in videos is an important problem in\napplications such as surveillance. Video anomaly detection (VAD) is\nwell-studied in the one-class classification (OCC) and weakly supervised (WS)\nsettings. However, fully unsupervised (US) video anomaly detection methods,\nwhich learn a complete system without any annotation or human supervision, have\nnot been explored in depth. This is because the lack of any ground truth\nannotations significantly increases the magnitude of the VAD challenge. To\naddress this challenge, we propose a simple-but-effective two-stage\npseudo-label generation framework that produces segment-level (normal/anomaly)\npseudo-labels, which can be further used to train a segment-level anomaly\ndetector in a supervised manner. The proposed coarse-to-fine pseudo-label\n(C2FPL) generator employs carefully-designed hierarchical divisive clustering\nand statistical hypothesis testing to identify anomalous video segments from a\nset of completely unlabeled videos. The trained anomaly detector can be\ndirectly applied on segments of an unseen test video to obtain segment-level,\nand subsequently, frame-level anomaly predictions. Extensive studies on two\nlarge-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that\nthe proposed unsupervised approach achieves superior performance compared to\nall existing OCC and US methods , while yielding comparable performance to the\nstate-of-the-art WS methods.",
        "translated": ""
    },
    {
        "title": "6-DoF Stability Field via Diffusion Models",
        "url": "http://arxiv.org/abs/2310.17649v1",
        "pub_date": "2023-10-26",
        "summary": "A core capability for robot manipulation is reasoning over where and how to\nstably place objects in cluttered environments. Traditionally, robots have\nrelied on object-specific, hand-crafted heuristics in order to perform such\nreasoning, with limited generalizability beyond a small number of object\ninstances and object interaction patterns. Recent approaches instead learn\nnotions of physical interaction, namely motion prediction, but require\nsupervision in the form of labeled object information or come at the cost of\nhigh sample complexity, and do not directly reason over stability or object\nplacement. We present 6-DoFusion, a generative model capable of generating 3D\nposes of an object that produces a stable configuration of a given scene.\nUnderlying 6-DoFusion is a diffusion model that incrementally refines a\nrandomly initialized SE(3) pose to generate a sample from a learned,\ncontext-dependent distribution over stable poses. We evaluate our model on\ndifferent object placement and stacking tasks, demonstrating its ability to\nconstruct stable scenes that involve novel object classes as well as to improve\nthe accuracy of state-of-the-art 3D pose estimation methods.",
        "translated": ""
    },
    {
        "title": "Defending Against Transfer Attacks From Public Models",
        "url": "http://arxiv.org/abs/2310.17645v1",
        "pub_date": "2023-10-26",
        "summary": "Adversarial attacks have been a looming and unaddressed threat in the\nindustry. However, through a decade-long history of the robustness evaluation\nliterature, we have learned that mounting a strong or optimal attack is\nchallenging. It requires both machine learning and domain expertise. In other\nwords, the white-box threat model, religiously assumed by a large majority of\nthe past literature, is unrealistic. In this paper, we propose a new practical\nthreat model where the adversary relies on transfer attacks through publicly\navailable surrogate models. We argue that this setting will become the most\nprevalent for security-sensitive applications in the future. We evaluate the\ntransfer attacks in this setting and propose a specialized defense method based\non a game-theoretic perspective. The defenses are evaluated under 24 public\nmodels and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and\nImageNet). Under this threat model, our defense, PubDef, outperforms the\nstate-of-the-art white-box adversarial training by a large margin with almost\nno loss in the normal accuracy. For instance, on ImageNet, our defense achieves\n62% accuracy under the strongest transfer attack vs only 36% of the best\nadversarially trained model. Its accuracy when not under attack is only 2%\nlower than that of an undefended model (78% vs 80%). We release our code at\nhttps://github.com/wagner-group/pubdef.",
        "translated": ""
    },
    {
        "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free\n  Deep Learning Studies: A Case Study on NLP",
        "url": "http://arxiv.org/abs/2310.17644v1",
        "pub_date": "2023-10-26",
        "summary": "Reproducibility in scientific work has been becoming increasingly important\nin research communities such as machine learning, natural language processing,\nand computer vision communities due to the rapid development of the research\ndomains supported by recent advances in deep learning. In this work, we present\na significantly upgraded version of torchdistill, a modular-driven coding-free\ndeep learning framework significantly upgraded from the initial release, which\nsupports only image classification and object detection tasks for reproducible\nknowledge distillation experiments. To demonstrate that the upgraded framework\ncan support more tasks with third-party libraries, we reproduce the GLUE\nbenchmark results of BERT models using a script based on the upgraded\ntorchdistill, harmonizing with various Hugging Face libraries. All the 27\nfine-tuned BERT models and configurations to reproduce the results are\npublished at Hugging Face, and the model weights have already been widely used\nin research communities. We also reimplement popular small-sized models and new\nknowledge distillation methods and perform additional experiments for computer\nvision tasks.",
        "translated": ""
    },
    {
        "title": "Image Clustering Conditioned on Text Criteria",
        "url": "http://arxiv.org/abs/2310.18297v1",
        "pub_date": "2023-10-27",
        "summary": "Classical clustering methods do not provide users with direct control of the\nclustering results, and the clustering results may not be consistent with the\nrelevant criterion that a user has in mind. In this work, we present a new\nmethodology for performing image clustering based on user-specified text\ncriteria by leveraging modern vision-language models and large language models.\nWe call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and\nit represents a different paradigm of image clustering. IC$|$TC requires a\nminimal and practical degree of human intervention and grants the user\nsignificant control over the clustering results in return. Our experiments show\nthat IC$|$TC can effectively cluster images with various criteria, such as\nhuman action, physical location, or the person's mood, while significantly\noutperforming baselines.",
        "translated": ""
    },
    {
        "title": "Always Clear Days: Degradation Type and Severity Aware All-In-One\n  Adverse Weather Removal",
        "url": "http://arxiv.org/abs/2310.18293v1",
        "pub_date": "2023-10-27",
        "summary": "All-in-one adverse weather removal is an emerging topic on image restoration,\nwhich aims to restore multiple weather degradation in an unified model, and the\nchallenging are twofold. First, discovering and handling the property of\nmulti-domain in target distribution formed by multiple weather conditions.\nSecond, design efficient and effective operations for different degradation\ntypes. To address this problem, most prior works focus on the multi-domain\ncaused by weather type. Inspired by inter\\&amp;intra-domain adaptation literature,\nwe observed that not only weather type but also weather severity introduce\nmulti-domain within each weather type domain, which is ignored by previous\nmethods, and further limit their performance. To this end, we proposed a\ndegradation type and severity aware model, called \\textbf{UtilityIR}, for blind\nall-in-one bad weather image restoration. To extract weather information from\nsingle image, we proposed a novel Marginal Quality Ranking Loss (MQRL) and\nutilized Contrastive Loss (CL) to guide weather severity and type extraction,\nand leverage a bag of novel techniques such as Multi-Head Cross Attention\n(MHCA) and Local-Global Adaptive Instance Normalization (LG-AdaIN) to\nefficiently restore spatial varying weather degradation. The proposed method\ncan significantly outperform the SOTA methods subjectively and objectively on\ndifferent weather restoration tasks with a large margin, and enjoy less model\nparameters. Proposed method even can restore \\textbf{unseen} domain combined\nmultiple degradation images, and modulating restoration level. Implementation\ncode will be available at\n{https://github.com/fordevoted/UtilityIR}{\\textit{this repository}}",
        "translated": ""
    },
    {
        "title": "Heterogeneous Federated Learning with Group-Aware Prompt Tuning",
        "url": "http://arxiv.org/abs/2310.18285v1",
        "pub_date": "2023-10-27",
        "summary": "Transformers have achieved remarkable success in various machine-learning\ntasks, prompting their widespread adoption. In this paper, we explore their\napplication in the context of federated learning (FL), with a particular focus\non heterogeneous scenarios where individual clients possess diverse local\ndatasets. To meet the computational and communication demands of FL, we\nleverage pre-trained Transformers and use an efficient prompt-tuning strategy.\nOur strategy introduces the concept of learning both shared and group prompts,\nenabling the acquisition of universal knowledge and group-specific knowledge\nsimultaneously. Additionally, a prompt selection module assigns personalized\ngroup prompts to each input, aligning the global model with the data\ndistribution of each client. This approach allows us to train a single global\nmodel that can automatically adapt to various local client data distributions\nwithout requiring local fine-tuning. In this way, our proposed method\neffectively bridges the gap between global and personalized local models in\nFederated Learning and surpasses alternative approaches that lack the\ncapability to adapt to previously unseen clients. The effectiveness of our\napproach is rigorously validated through extensive experimentation and ablation\nstudies.",
        "translated": ""
    },
    {
        "title": "FOUND: Foot Optimization with Uncertain Normals for Surface Deformation\n  Using Synthetic Data",
        "url": "http://arxiv.org/abs/2310.18279v1",
        "pub_date": "2023-10-27",
        "summary": "Surface reconstruction from multi-view images is a challenging task, with\nsolutions often requiring a large number of sampled images with high overlap.\nWe seek to develop a method for few-view reconstruction, for the case of the\nhuman foot. To solve this task, we must extract rich geometric cues from RGB\nimages, before carefully fusing them into a final 3D object. Our FOUND approach\ntackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of\n50,000 photorealistic foot images, paired with ground truth surface normals and\nkeypoints; (ii) an uncertainty-aware surface normal predictor trained on our\nsynthetic dataset; (iii) an optimization scheme for fitting a generative foot\nmodel to a series of images; and (iv) a benchmark dataset of calibrated images\nand high resolution ground truth geometry. We show that our normal predictor\noutperforms all off-the-shelf equivalents significantly on real images, and our\noptimization scheme outperforms state-of-the-art photogrammetry pipelines,\nespecially for a few-view setting. We release our synthetic dataset and\nbaseline 3D scans to the research community.",
        "translated": ""
    },
    {
        "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
        "url": "http://arxiv.org/abs/2310.18274v1",
        "pub_date": "2023-10-27",
        "summary": "Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.",
        "translated": ""
    },
    {
        "title": "There Are No Data Like More Data- Datasets for Deep Learning in Earth\n  Observation",
        "url": "http://arxiv.org/abs/2310.19231v1",
        "pub_date": "2023-10-30",
        "summary": "Carefully curated and annotated datasets are the foundation of machine\nlearning, with particularly data-hungry deep neural networks forming the core\nof what is often called Artificial Intelligence (AI). Due to the massive\nsuccess of deep learning applied to Earth Observation (EO) problems, the focus\nof the community has been largely on the development of ever-more sophisticated\ndeep neural network architectures and training strategies largely ignoring the\noverall importance of datasets. For that purpose, numerous task-specific\ndatasets have been created that were largely ignored by previously published\nreview articles on AI for Earth observation. With this article, we want to\nchange the perspective and put machine learning datasets dedicated to Earth\nobservation data and applications into the spotlight. Based on a review of the\nhistorical developments, currently available resources are described and a\nperspective for future developments is formed. We hope to contribute to an\nunderstanding that the nature of our data is what distinguishes the Earth\nobservation community from many other communities that apply deep learning\ntechniques to image data, and that a detailed understanding of EO data\npeculiarities is among the core competencies of our discipline.",
        "translated": ""
    },
    {
        "title": "CHAMMI: A benchmark for channel-adaptive models in microscopy imaging",
        "url": "http://arxiv.org/abs/2310.19224v1",
        "pub_date": "2023-10-30",
        "summary": "Most neural networks assume that input images have a fixed number of channels\n(three for RGB images). However, there are many settings where the number of\nchannels may vary, such as microscopy images where the number of channels\nchanges depending on instruments and experimental goals. Yet, there has not\nbeen a systemic attempt to create and evaluate neural networks that are\ninvariant to the number and type of channels. As a result, trained models\nremain specific to individual studies and are hardly reusable for other\nmicroscopy settings. In this paper, we present a benchmark for investigating\nchannel-adaptive models in microscopy imaging, which consists of 1) a dataset\nof varied-channel single-cell images, and 2) a biologically relevant evaluation\nframework. In addition, we adapted several existing techniques to create\nchannel-adaptive models and compared their performance on this benchmark to\nfixed-channel, baseline models. We find that channel-adaptive models can\ngeneralize better to out-of-domain tasks and can be computationally efficient.\nWe contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an\nevaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate\nobjective comparisons in future research and applications.",
        "translated": ""
    },
    {
        "title": "Modular Anti-noise Deep Learning Network for Robotic Grasp Detection\n  Based on RGB Images",
        "url": "http://arxiv.org/abs/2310.19223v1",
        "pub_date": "2023-10-30",
        "summary": "While traditional methods relies on depth sensors, the current trend leans\ntowards utilizing cost-effective RGB images, despite their absence of depth\ncues. This paper introduces an interesting approach to detect grasping pose\nfrom a single RGB image. To this end, we propose a modular learning network\naugmented with grasp detection and semantic segmentation, tailored for robots\nequipped with parallel-plate grippers. Our network not only identifies\ngraspable objects but also fuses prior grasp analyses with semantic\nsegmentation, thereby boosting grasp detection precision. Significantly, our\ndesign exhibits resilience, adeptly handling blurred and noisy visuals. Key\ncontributions encompass a trainable network for grasp detection from RGB\nimages, a modular design facilitating feasible grasp implementation, and an\narchitecture robust against common image distortions. We demonstrate the\nfeasibility and accuracy of our proposed approach through practical experiments\nand evaluations.",
        "translated": ""
    },
    {
        "title": "Generalized Category Discovery with Clustering Assignment Consistency",
        "url": "http://arxiv.org/abs/2310.19210v1",
        "pub_date": "2023-10-30",
        "summary": "Generalized category discovery (GCD) is a recently proposed open-world task.\nGiven a set of images consisting of labeled and unlabeled instances, the goal\nof GCD is to automatically cluster the unlabeled samples using information\ntransferred from the labeled dataset. The unlabeled dataset comprises both\nknown and novel classes. The main challenge is that unlabeled novel class\nsamples and unlabeled known class samples are mixed together in the unlabeled\ndataset. To address the GCD without knowing the class number of unlabeled\ndataset, we propose a co-training-based framework that encourages clustering\nconsistency. Specifically, we first introduce weak and strong augmentation\ntransformations to generate two sufficiently different views for the same\nsample. Then, based on the co-training assumption, we propose a consistency\nrepresentation learning strategy, which encourages consistency between\nfeature-prototype similarity and clustering assignment. Finally, we use the\ndiscriminative embeddings learned from the semi-supervised representation\nlearning process to construct an original sparse network and use a community\ndetection method to obtain the clustering results and the number of categories\nsimultaneously. Extensive experiments show that our method achieves\nstate-of-the-art performance on three generic benchmarks and three fine-grained\nvisual recognition datasets. Especially in the ImageNet-100 data set, our\nmethod significantly exceeds the best baseline by 15.5\\% and 7.0\\% on the\n\\texttt{Novel} and \\texttt{All} classes, respectively.",
        "translated": ""
    },
    {
        "title": "3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets",
        "url": "http://arxiv.org/abs/2310.19188v1",
        "pub_date": "2023-10-29",
        "summary": "We present 3DMiner -- a pipeline for mining 3D shapes from challenging\nlarge-scale unannotated image datasets. Unlike other unsupervised 3D\nreconstruction methods, we assume that, within a large-enough dataset, there\nmust exist images of objects with similar shapes but varying backgrounds,\ntextures, and viewpoints. Our approach leverages the recent advances in\nlearning self-supervised image representations to cluster images with\ngeometrically similar shapes and find common image correspondences between\nthem. We then exploit these correspondences to obtain rough camera estimates as\ninitialization for bundle-adjustment. Finally, for every image cluster, we\napply a progressive bundle-adjusting reconstruction method to learn a neural\noccupancy field representing the underlying shape. We show that this procedure\nis robust to several types of errors introduced in previous steps (e.g., wrong\ncamera poses, images containing dissimilar shapes, etc.), allowing us to obtain\nshape and pose annotations for images in-the-wild. When using images from Pix3D\nchairs, our method is capable of producing significantly better results than\nstate-of-the-art unsupervised 3D reconstruction techniques, both quantitatively\nand qualitatively. Furthermore, we show how 3DMiner can be applied to\nin-the-wild data by reconstructing shapes present in images from the LAION-5B\ndataset. Project Page: https://ttchengab.github.io/3dminerOfficial",
        "translated": ""
    },
    {
        "title": "FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance\n  Fields by Analyzing and Enhancing Fourier PlenOctrees",
        "url": "http://arxiv.org/abs/2310.20710v1",
        "pub_date": "2023-10-31",
        "summary": "Fourier PlenOctrees have shown to be an efficient representation for\nreal-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many\nadvantages, this method suffers from artifacts introduced by the involved\ncompression when combining it with recent state-of-the-art techniques for\ntraining the static per-frame NeRF models. In this paper, we perform an\nin-depth analysis of these artifacts and leverage the resulting insights to\npropose an improved representation. In particular, we present a novel density\nencoding that adapts the Fourier-based compression to the characteristics of\nthe transfer function used by the underlying volume rendering procedure and\nleads to a substantial reduction of artifacts in the dynamic model.\nFurthermore, we show an augmentation of the training data that relaxes the\nperiodicity assumption of the compression. We demonstrate the effectiveness of\nour enhanced Fourier PlenOctrees in the scope of quantitative and qualitative\nevaluations on synthetic and real-world scenes.",
        "translated": ""
    },
    {
        "title": "DDAM-PS: Diligent Domain Adaptive Mixer for Person Search",
        "url": "http://arxiv.org/abs/2310.20706v1",
        "pub_date": "2023-10-31",
        "summary": "Person search (PS) is a challenging computer vision problem where the\nobjective is to achieve joint optimization for pedestrian detection and\nre-identification (ReID). Although previous advancements have shown promising\nperformance in the field under fully and weakly supervised learning fashion,\nthere exists a major gap in investigating the domain adaptation ability of PS\nmodels. In this paper, we propose a diligent domain adaptive mixer (DDAM) for\nperson search (DDAP-PS) framework that aims to bridge a gap to improve\nknowledge transfer from the labeled source domain to the unlabeled target\ndomain. Specifically, we introduce a novel DDAM module that generates moderate\nmixed-domain representations by combining source and target domain\nrepresentations. The proposed DDAM module encourages domain mixing to minimize\nthe distance between the two extreme domains, thereby enhancing the ReID task.\nTo achieve this, we introduce two bridge losses and a disparity loss. The\nobjective of the two bridge losses is to guide the moderate mixed-domain\nrepresentations to maintain an appropriate distance from both the source and\ntarget domain representations. The disparity loss aims to prevent the moderate\nmixed-domain representations from being biased towards either the source or\ntarget domains, thereby avoiding overfitting. Furthermore, we address the\nconflict between the two subtasks, localization and ReID, during domain\nadaptation. To handle this cross-task conflict, we forcefully decouple the\nnorm-aware embedding, which aids in better learning of the moderate\nmixed-domain representation. We conduct experiments to validate the\neffectiveness of our proposed method. Our approach demonstrates favorable\nperformance on the challenging PRW and CUHK-SYSU datasets. Our source code is\npublicly available at \\url{https://github.com/mustansarfiaz/DDAM-PS}.",
        "translated": ""
    },
    {
        "title": "Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked\n  Autoencoders",
        "url": "http://arxiv.org/abs/2310.20704v1",
        "pub_date": "2023-10-31",
        "summary": "Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite\ntheir success, ViTs lack inductive biases, which can make it difficult to train\nthem with limited data. To address this challenge, prior studies suggest\ntraining ViTs with self-supervised learning (SSL) and fine-tuning sequentially.\nHowever, we observe that jointly optimizing ViTs for the primary task and a\nSelf-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the\namount of training data is limited. We explore the appropriate SSL tasks that\ncan be optimized alongside the primary task, the training schemes for these\ntasks, and the data scale at which they can be most effective. Our findings\nreveal that SSAT is a powerful technique that enables ViTs to leverage the\nunique characteristics of both the self-supervised and primary tasks, achieving\nbetter performance than typical ViTs pre-training with SSL and fine-tuning\nsequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT\nsignificantly improves ViT performance while reducing carbon footprint. We also\nconfirm the effectiveness of SSAT in the video domain for deepfake detection,\nshowcasing its generalizability. Our code is available at\nhttps://github.com/dominickrei/Limited-data-vits.",
        "translated": ""
    },
    {
        "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and\n  Prediction",
        "url": "http://arxiv.org/abs/2310.20700v1",
        "pub_date": "2023-10-31",
        "summary": "Recently video generation has achieved substantial progress with realistic\nresults. Nevertheless, existing AI-generated videos are usually very short\nclips (\"shot-level\") depicting a single scene. To deliver a coherent long video\n(\"story-level\"), it is desirable to have creative transition and prediction\neffects across different clips. This paper presents a short-to-long video\ndiffusion model, SEINE, that focuses on generative transition and prediction.\nThe goal is to generate high-quality long videos with smooth and creative\ntransitions between scenes and varying lengths of shot-level videos.\nSpecifically, we propose a random-mask video diffusion model to automatically\ngenerate transitions based on textual descriptions. By providing the images of\ndifferent scenes as inputs, combined with text-based control, our model\ngenerates transition videos that ensure coherence and visual quality.\nFurthermore, the model can be readily extended to various tasks such as\nimage-to-video animation and autoregressive video prediction. To conduct a\ncomprehensive evaluation of this new generative task, we propose three\nassessing criteria for smooth and creative transition: temporal consistency,\nsemantic similarity, and video-text semantic alignment. Extensive experiments\nvalidate the effectiveness of our approach over existing methods for generative\ntransition and prediction, enabling the creation of story-level long videos.\nProject page: https://vchitect.github.io/SEINE-project/ .",
        "translated": ""
    },
    {
        "title": "HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception",
        "url": "http://arxiv.org/abs/2310.20695v1",
        "pub_date": "2023-10-31",
        "summary": "Model pre-training is essential in human-centric perception. In this paper,\nwe first introduce masked image modeling (MIM) as a pre-training approach for\nthis task. Upon revisiting the MIM training strategy, we reveal that human\nstructure priors offer significant potential. Motivated by this insight, we\nfurther incorporate an intuitive human structure prior - human parts - into\npre-training. Specifically, we employ this prior to guide the mask sampling\nprocess. Image patches, corresponding to human part regions, have high priority\nto be masked out. This encourages the model to concentrate more on body\nstructure information during pre-training, yielding substantial benefits across\na range of human-centric perception tasks. To further capture human\ncharacteristics, we propose a structure-invariant alignment loss that enforces\ndifferent masked views, guided by the human part prior, to be closely aligned\nfor the same image. We term the entire method as HAP. HAP simply uses a plain\nViT as the encoder yet establishes new state-of-the-art performance on 11\nhuman-centric benchmarks, and on-par result on one dataset. For example, HAP\nachieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K\nfor pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose\nestimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.",
        "translated": ""
    },
    {
        "title": "What User Behaviors Make the Differences During the Process of Visual\n  Analytics?",
        "url": "http://arxiv.org/abs/2311.00690v1",
        "pub_date": "2023-11-01",
        "summary": "The understanding of visual analytics process can benefit visualization\nresearchers from multiple aspects, including improving visual designs and\ndeveloping advanced interaction functions. However, the log files of user\nbehaviors are still hard to analyze due to the complexity of sensemaking and\nour lack of knowledge on the related user behaviors. This work presents a study\non a comprehensive data collection of user behaviors, and our analysis approach\nwith time-series classification methods. We have chosen a classical\nvisualization application, Covid-19 data analysis, with common analysis tasks\ncovering geo-spatial, time-series and multi-attributes. Our user study collects\nuser behaviors on a diverse set of visualization tasks with two comparable\nsystems, desktop and immersive visualizations. We summarize the classification\nresults with three time-series machine learning algorithms at two scales, and\nexplore the influences of behavior features. Our results reveal that user\nbehaviors can be distinguished during the process of visual analytics and there\nis a potentially strong association between the physical behaviors of users and\nthe visualization tasks they perform. We also demonstrate the usage of our\nmodels by interpreting open sessions of visual analytics, which provides an\nautomatic way to study sensemaking without tedious manual annotations.",
        "translated": ""
    },
    {
        "title": "Collaboration in Immersive Environments: Challenges and Solutions",
        "url": "http://arxiv.org/abs/2311.00689v1",
        "pub_date": "2023-11-01",
        "summary": "Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in\nall engineering fields in order to avoid the use of physical prototypes, to\ntrain in high-risk situations, and to interpret real or simulated results. In\norder to complete a shared task or assign tasks to the agents in such immersive\nenvironments, collaboration or Shared Cooperative Activities are a necessity.\nCollaboration in immersive environments is an emerging field of research that\naims to study and enhance the ways in which people interact and work together\nin Virtual and Augmented Reality settings. Collaboration in immersive\nenvironments is a complex process that involves different factors such as\ncommunication, coordination, and social presence. This paper provides an\noverview of the current state of research on collaboration in immersive\nenvironments. It discusses the different types of immersive environments,\nincluding VR and AR, and the different forms of collaboration that can occur in\nthese environments. The paper also highlights the challenges and limitations of\ncollaboration in immersive environments, such as the lack of physical cues,\ncost and usability and the need for further research in this area. Overall,\ncollaboration in immersive environments is a promising field with a wide range\nof potential applications, from education to industry, and it can benefit both\nindividuals and groups by enhancing their ability to work together effectively.",
        "translated": ""
    },
    {
        "title": "ProcSim: Proxy-based Confidence for Robust Similarity Learning",
        "url": "http://arxiv.org/abs/2311.00668v1",
        "pub_date": "2023-11-01",
        "summary": "Deep Metric Learning (DML) methods aim at learning an embedding space in\nwhich distances are closely related to the inherent semantic similarity of the\ninputs. Previous studies have shown that popular benchmark datasets often\ncontain numerous wrong labels, and DML methods are susceptible to them.\nIntending to study the effect of realistic noise, we create an ontology of the\nclasses in a dataset and use it to simulate semantically coherent labeling\nmistakes. To train robust DML models, we propose ProcSim, a simple framework\nthat assigns a confidence score to each sample using the normalized distance to\nits class representative. The experimental results show that the proposed\nmethod achieves state-of-the-art performance on the DML benchmark datasets\ninjected with uniform and the proposed semantically coherent noise.",
        "translated": ""
    },
    {
        "title": "TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining\n  and Object Detection in Rain",
        "url": "http://arxiv.org/abs/2311.00660v1",
        "pub_date": "2023-11-01",
        "summary": "Rain generation algorithms have the potential to improve the generalization\nof deraining methods and scene understanding in rainy conditions. However, in\npractice, they produce artifacts and distortions and struggle to control the\namount of rain generated due to a lack of proper constraints. In this paper, we\npropose an unpaired image-to-image translation framework for generating\nrealistic rainy images. We first introduce a Triangular Probability Similarity\n(TPS) constraint to guide the generated images toward clear and rainy images in\nthe discriminator manifold, thereby minimizing artifacts and distortions during\nrain generation. Unlike conventional contrastive learning approaches, which\nindiscriminately push negative samples away from the anchors, we propose a\nSemantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing\nforce of negative samples based on the semantic similarity between the clear\nand the rainy images and the feature similarity between the anchor and the\nnegative samples. Experiments demonstrate realistic rain generation with\nminimal artifacts and distortions, which benefits image deraining and object\ndetection in rain. Furthermore, the method can be used to generate realistic\nsnowy and night images, underscoring its potential for broader applicability.\nCode is available at https://github.com/ShenZheng2000/TPSeNCE.",
        "translated": ""
    },
    {
        "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface",
        "url": "http://arxiv.org/abs/2311.00618v1",
        "pub_date": "2023-11-01",
        "summary": "We demonstrate text as a strong cross-modal interface. Rather than relying on\ndeep embeddings to connect image and language as the interface representation,\nour approach represents an image as text, from which we enjoy the\ninterpretability and flexibility inherent to natural language. We employ an\nautoencoder that uses a pre-trained text-to-image diffusion model for decoding.\nThe encoder is trained to transform an input image into text, which is then fed\ninto the fixed text-to-image diffusion decoder to reconstruct the original\ninput -- a process we term De-Diffusion. Experiments validate both the\nprecision and comprehensiveness of De-Diffusion text representing images, such\nthat it can be readily ingested by off-the-shelf text-to-image tools and LLMs\nfor diverse multi-modal tasks. For example, a single De-Diffusion model can\ngeneralize to provide transferable prompts for different text-to-image tools,\nand also achieves a new state of the art on open-ended vision-language tasks by\nsimply prompting large language models with few-shot examples.",
        "translated": ""
    },
    {
        "title": "Idempotent Generative Network",
        "url": "http://arxiv.org/abs/2311.01462v1",
        "pub_date": "2023-11-02",
        "summary": "We propose a new approach for generative modeling based on training a neural\nnetwork to be idempotent. An idempotent operator is one that can be applied\nsequentially without changing the result beyond the initial application, namely\n$f(f(z))=f(z)$. The proposed model $f$ is trained to map a source distribution\n(e.g, Gaussian noise) to a target distribution (e.g. realistic images) using\nthe following objectives: (1) Instances from the target distribution should map\nto themselves, namely $f(x)=x$. We define the target manifold as the set of all\ninstances that $f$ maps to themselves. (2) Instances that form the source\ndistribution should map onto the defined target manifold. This is achieved by\noptimizing the idempotence term, $f(f(z))=f(z)$ which encourages the range of\n$f(z)$ to be on the target manifold. Under ideal assumptions such a process\nprovably converges to the target distribution. This strategy results in a model\ncapable of generating an output in one step, maintaining a consistent latent\nspace, while also allowing sequential applications for refinement.\nAdditionally, we find that by processing inputs from both target and source\ndistributions, the model adeptly projects corrupted or modified data back to\nthe target manifold. This work is a first step towards a ``global projector''\nthat enables projecting any input into a target data distribution.",
        "translated": ""
    },
    {
        "title": "Align Your Prompts: Test-Time Prompting with Distribution Alignment for\n  Zero-Shot Generalization",
        "url": "http://arxiv.org/abs/2311.01459v1",
        "pub_date": "2023-11-02",
        "summary": "The promising zero-shot generalization of vision-language models such as CLIP\nhas led to their adoption using prompt learning for numerous downstream tasks.\nPrevious works have shown test-time prompt tuning using entropy minimization to\nadapt text prompts for unseen domains. While effective, this overlooks the key\ncause for performance degradation to unseen domains -- distribution shift. In\nthis work, we explicitly handle this problem by aligning the\nout-of-distribution (OOD) test sample statistics to those of the source data\nusing prompt tuning. We use a single test sample to adapt multi-modal prompts\nat test time by minimizing the feature distribution shift to bridge the gap in\nthe test domain. Evaluating against the domain generalization benchmark, our\nmethod improves zero-shot top- 1 accuracy beyond existing prompt-learning\ntechniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset\ngeneralization with unseen categories across 10 datasets, our method improves\nconsistently across all datasets compared to the existing state-of-the-art. Our\nsource code and models are available at\nhttps://jameelhassan.github.io/promptalign.",
        "translated": ""
    },
    {
        "title": "Detecting Deepfakes Without Seeing Any",
        "url": "http://arxiv.org/abs/2311.01458v1",
        "pub_date": "2023-11-02",
        "summary": "Deepfake attacks, malicious manipulation of media containing people, are a\nserious concern for society. Conventional deepfake detection methods train\nsupervised classifiers to distinguish real media from previously encountered\ndeepfakes. Such techniques can only detect deepfakes similar to those\npreviously seen, but not zero-day (previously unseen) attack types. As current\ndeepfake generation techniques are changing at a breathtaking pace, new attack\ntypes are proposed frequently, making this a major issue. Our main observations\nare that: i) in many effective deepfake attacks, the fake media must be\naccompanied by false facts i.e. claims about the identity, speech, motion, or\nappearance of the person. For instance, when impersonating Obama, the attacker\nexplicitly or implicitly claims that the fake media show Obama; ii) current\ngenerative techniques cannot perfectly synthesize the false facts claimed by\nthe attacker. We therefore introduce the concept of \"fact checking\", adapted\nfrom fake news detection, for detecting zero-day deepfake attacks. Fact\nchecking verifies that the claimed facts (e.g. identity is Obama), agree with\nthe observed media (e.g. is the face really Obama's?), and thus can\ndifferentiate between real and fake media. Consequently, we introduce FACTOR, a\npractical recipe for deepfake fact checking and demonstrate its power in\ncritical attack settings: face swapping and audio-visual synthesis. Although it\nis training-free, relies exclusively on off-the-shelf features, is very easy to\nimplement, and does not see any deepfakes, it achieves better than\nstate-of-the-art accuracy.",
        "translated": ""
    },
    {
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation",
        "url": "http://arxiv.org/abs/2311.01455v1",
        "pub_date": "2023-11-02",
        "summary": "We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.",
        "translated": ""
    },
    {
        "title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and\n  Generation",
        "url": "http://arxiv.org/abs/2311.01448v1",
        "pub_date": "2023-11-02",
        "summary": "LiDAR provides accurate geometric measurements of the 3D world.\nUnfortunately, dense LiDARs are very expensive and the point clouds captured by\nlow-beam LiDAR are often sparse. To address these issues, we present\nUltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR\ngeneration, and LiDAR manipulation. The crux of UltraLiDAR is a compact,\ndiscrete representation that encodes the point cloud's geometric structure, is\nrobust to noise, and is easy to manipulate. We show that by aligning the\nrepresentation of a sparse point cloud to that of a dense point cloud, we can\ndensify the sparse point clouds as if they were captured by a real high-density\nLiDAR, drastically reducing the cost. Furthermore, by learning a prior over the\ndiscrete codebook, we can generate diverse, realistic LiDAR point clouds for\nself-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense\nLiDAR completion and LiDAR generation. Experiments show that densifying\nreal-world point clouds with our approach can significantly improve the\nperformance of downstream perception systems. Compared to prior art on LiDAR\ngeneration, our approach generates much more realistic point clouds. According\nto A/B test, over 98.5\\% of the time human participants prefer our results over\nthose of previous methods.",
        "translated": ""
    },
    {
        "title": "EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via\n  Self-Supervision",
        "url": "http://arxiv.org/abs/2311.02077v1",
        "pub_date": "2023-11-03",
        "summary": "We present EmerNeRF, a simple yet powerful approach for learning\nspatial-temporal representations of dynamic driving scenes. Grounded in neural\nfields, EmerNeRF simultaneously captures scene geometry, appearance, motion,\nand semantics via self-bootstrapping. EmerNeRF hinges upon two core components:\nFirst, it stratifies scenes into static and dynamic fields. This decomposition\nemerges purely from self-supervision, enabling our model to learn from general,\nin-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field\nfrom the dynamic field and uses this flow field to further aggregate\nmulti-frame features, amplifying the rendering precision of dynamic objects.\nCoupling these three fields (static, dynamic, and flow) enables EmerNeRF to\nrepresent highly-dynamic scenes self-sufficiently, without relying on ground\ntruth object annotations or pre-trained models for dynamic object segmentation\nor optical flow estimation. Our method achieves state-of-the-art performance in\nsensor simulation, significantly outperforming previous methods when\nreconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In\naddition, to bolster EmerNeRF's semantic generalization, we lift 2D visual\nfoundation model features into 4D space-time and address a general positional\nbias in modern Transformers, significantly boosting 3D perception performance\n(e.g., 37.50% relative improvement in occupancy prediction accuracy on\naverage). Finally, we construct a diverse and challenging 120-sequence dataset\nto benchmark neural fields under extreme and highly-dynamic settings.",
        "translated": ""
    },
    {
        "title": "Learning Historical Status Prompt for Accurate and Robust Visual\n  Tracking",
        "url": "http://arxiv.org/abs/2311.02072v1",
        "pub_date": "2023-11-03",
        "summary": "Most trackers perform template and search region similarity matching to find\nthe most similar object to the template during tracking. However, they struggle\nto make prediction when the target appearance changes due to the limited\nhistorical information introduced by roughly cropping the current search region\nbased on the predicted result of previous frame. In this paper, we identify\nthat the central impediment to improving the performance of existing trackers\nis the incapacity to integrate abundant and effective historical information.\nTo address this issue, we propose a Historical Information Prompter (HIP) to\nenhance the provision of historical information. We also build HIPTrack upon\nHIP module. HIP is a plug-and-play module that make full use of search region\nfeatures to introduce historical appearance information. It also incorporates\nhistorical position information by constructing refined mask of the target. HIP\nis a lightweight module to generate historical information prompts. By\nintegrating historical information prompts, HIPTrack significantly enhances the\ntracking performance without the need to retrain the backbone. Experimental\nresults demonstrate that our method outperforms all state-of-the-art approaches\non LaSOT, LaSOT ext, GOT10k and NfS. Futhermore, HIP module exhibits strong\ngenerality and can be seamlessly integrated into trackers to improve tracking\nperformance. The source code and models will be released for further research.",
        "translated": ""
    },
    {
        "title": "LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery",
        "url": "http://arxiv.org/abs/2311.02058v1",
        "pub_date": "2023-11-03",
        "summary": "We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.",
        "translated": ""
    },
    {
        "title": "Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via\n  Automatic Label Generation",
        "url": "http://arxiv.org/abs/2311.02044v1",
        "pub_date": "2023-11-03",
        "summary": "This research work seeks to explore and identify strategies that can\ndetermine road topology information in 2D and 3D under highly dynamic urban\ndriving scenarios. To facilitate this exploration, we introduce a substantial\ndataset comprising nearly one million automatically labeled data frames. A key\ncontribution of our research lies in developing an automatic label-generation\nprocess and an occlusion handling strategy. This strategy is designed to model\na wide range of occlusion scenarios, from mild disruptions to severe blockages.\nFurthermore, we present a comprehensive ablation study wherein multiple\ncenterline detection methods are developed and evaluated. This analysis not\nonly benchmarks the performance of various approaches but also provides\nvaluable insights into the interpretability of these methods. Finally, we\ndemonstrate the practicality of our methods and assess their adaptability\nacross different sensor configurations, highlighting their versatility and\nrelevance in real-world scenarios. Our dataset and experimental models are\npublicly available.",
        "translated": ""
    },
    {
        "title": "VQPy: An Object-Oriented Approach to Modern Video Analytics",
        "url": "http://arxiv.org/abs/2311.01623v1",
        "pub_date": "2023-11-03",
        "summary": "Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.",
        "translated": ""
    },
    {
        "title": "Exploitation-Guided Exploration for Semantic Embodied Navigation",
        "url": "http://arxiv.org/abs/2311.03357v1",
        "pub_date": "2023-11-06",
        "summary": "In the recent progress in embodied navigation and sim-to-robot transfer,\nmodular policies have emerged as a de facto framework. However, there is more\nto compositionality beyond the decomposition of the learning load into modular\ncomponents. In this work, we investigate a principled way to syntactically\ncombine these components. Particularly, we propose Exploitation-Guided\nExploration (XGX) where separate modules for exploration and exploitation come\ntogether in a novel and intuitive manner. We configure the exploitation module\nto take over in the deterministic final steps of navigation i.e. when the goal\nbecomes visible. Crucially, an exploitation module teacher-forces the\nexploration module and continues driving an overridden policy optimization.\nXGX, with effective decomposition and novel guidance, improves the\nstate-of-the-art performance on the challenging object navigation task from 70%\nto 73%. Along with better accuracy, through targeted analysis, we show that XGX\nis also more efficient at goal-conditioned exploration. Finally, we show\nsim-to-real transfer to robot hardware and XGX performs over two-fold better\nthan the best baseline from simulation benchmarking. Project page:\nxgxvisnav.github.io",
        "translated": ""
    },
    {
        "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img\n  Synthesis",
        "url": "http://arxiv.org/abs/2311.03355v1",
        "pub_date": "2023-11-06",
        "summary": "We propose SegGen, a highly-effective training data generation method for\nimage segmentation, which pushes the performance limits of state-of-the-art\nsegmentation models to a significant extent. SegGen designs and integrates two\ndata generation strategies: MaskSyn and ImgSyn. (i) MaskSyn synthesizes new\nmask-image pairs via our proposed text-to-mask generation model and\nmask-to-image generation model, greatly improving the diversity in segmentation\nmasks for model supervision; (ii) ImgSyn synthesizes new images based on\nexisting masks using the mask-to-image generation model, strongly improving\nimage diversity for model inputs. On the highly competitive ADE20K and COCO\nbenchmarks, our data generation method markedly improves the performance of\nstate-of-the-art segmentation models in semantic segmentation, panoptic\nsegmentation, and instance segmentation. Notably, in terms of the ADE20K mIoU,\nMask2Former R50 is largely boosted from 47.2 to 49.9 (+2.7); Mask2Former Swin-L\nis also significantly increased from 56.1 to 57.4 (+1.3). These promising\nresults strongly suggest the effectiveness of our SegGen even when abundant\nhuman-annotated training data is utilized. Moreover, training with our\nsynthetic data makes the segmentation models more robust towards unseen\ndomains. Project website: https://seggenerator.github.io",
        "translated": ""
    },
    {
        "title": "GLaMM: Pixel Grounding Large Multimodal Model",
        "url": "http://arxiv.org/abs/2311.03356v1",
        "pub_date": "2023-11-06",
        "summary": "Large Multimodal Models (LMMs) extend Large Language Models to the vision\ndomain. Initial efforts towards LMMs used holistic images and text prompts to\ngenerate ungrounded textual responses. Very recently, region-level LMMs have\nbeen used to generate visually grounded responses. However, they are limited to\nonly referring a single object category at a time, require users to specify the\nregions in inputs, or cannot offer dense pixel-wise object grounding. In this\nwork, we present Grounding LMM (GLaMM), the first model that can generate\nnatural language responses seamlessly intertwined with corresponding object\nsegmentation masks. GLaMM not only grounds objects appearing in the\nconversations but is flexible enough to accept both textual and optional visual\nprompts (region of interest) as input. This empowers users to interact with the\nmodel at various levels of granularity, both in textual and visual domains. Due\nto the lack of standard benchmarks for the novel setting of generating visually\ngrounded detailed conversations, we introduce a comprehensive evaluation\nprotocol with our curated grounded conversations. Our proposed Grounded\nConversation Generation (GCG) task requires densely grounded concepts in\nnatural scenes at a large-scale. To this end, we propose a densely annotated\nGrounding-anything Dataset (GranD) using our proposed automated annotation\npipeline that encompasses 7.5M unique concepts grounded in a total of 810M\nregions available with segmentation masks. Besides GCG, GLaMM also performs\neffectively on several downstream tasks e.g., referring expression\nsegmentation, image and region-level captioning and vision-language\nconversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.",
        "translated": ""
    },
    {
        "title": "CoVLM: Composing Visual Entities and Relationships in Large Language\n  Models Via Communicative Decoding",
        "url": "http://arxiv.org/abs/2311.03354v1",
        "pub_date": "2023-11-06",
        "summary": "A remarkable ability of human beings resides in compositional reasoning,\ni.e., the capacity to make \"infinite use of finite means\". However, current\nlarge vision-language foundation models (VLMs) fall short of such compositional\nabilities due to their \"bag-of-words\" behaviors and inability to construct\nwords that correctly represent visual entities and the relations among the\nentities. To this end, we propose CoVLM, which can guide the LLM to explicitly\ncompose visual entities and relationships among the text and dynamically\ncommunicate with the vision encoder and detection network to achieve\nvision-language communicative decoding. Specifically, we first devise a set of\nnovel communication tokens for the LLM, for dynamic communication between the\nvisual detection system and the language system. A communication token is\ngenerated by the LLM following a visual entity or a relation, to inform the\ndetection network to propose regions that are relevant to the sentence\ngenerated so far. The proposed regions-of-interests (ROIs) are then fed back\ninto the LLM for better language generation contingent on the relevant regions.\nThe LLM is thus able to compose the visual entities and relationships through\nthe communication tokens. The vision-to-language and language-to-vision\ncommunication are iteratively performed until the entire sentence is generated.\nOur framework seamlessly bridges the gap between visual perception and LLMs and\noutperforms previous VLMs by a large margin on compositional reasoning\nbenchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on\nARO top-1 accuracy). We also achieve state-of-the-art performances on\ntraditional vision-language tasks such as referring expression comprehension\nand visual question answering.",
        "translated": ""
    },
    {
        "title": "Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion",
        "url": "http://arxiv.org/abs/2311.03352v1",
        "pub_date": "2023-11-06",
        "summary": "In this paper, we highlight a problem of evaluation metrics adopted in the\nopen-vocabulary segmentation. That is, the evaluation process still heavily\nrelies on closed-set metrics on zero-shot or cross-dataset pipelines without\nconsidering the similarity between predicted and ground truth categories. To\ntackle this issue, we first survey eleven similarity measurements between two\ncategorical words using WordNet linguistics statistics, text embedding, and\nlanguage models by comprehensive quantitative analysis and user study. Built\nupon those explored measurements, we designed novel evaluation metrics, namely\nOpen mIoU, Open AP, and Open PQ, tailored for three open-vocabulary\nsegmentation tasks. We benchmarked the proposed evaluation metrics on 12\nopen-vocabulary methods of three segmentation tasks. Even though the relative\nsubjectivity of similarity distance, we demonstrate that our metrics can still\nwell evaluate the open ability of the existing open-vocabulary segmentation\nmethods. We hope that our work can bring with the community new thinking about\nhow to evaluate the open ability of models. The evaluation code is released in\ngithub.",
        "translated": ""
    },
    {
        "title": "OtterHD: A High-Resolution Multi-modality Model",
        "url": "http://arxiv.org/abs/2311.04219v1",
        "pub_date": "2023-11-07",
        "summary": "In this paper, we present OtterHD-8B, an innovative multimodal model evolved\nfrom Fuyu-8B, specifically engineered to interpret high-resolution visual\ninputs with granular precision. Unlike conventional models that are constrained\nby fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible\ninput dimensions, ensuring its versatility across various inference\nrequirements. Alongside this model, we introduce MagnifierBench, an evaluation\nframework designed to scrutinize models' ability to discern minute details and\nspatial relationships of small objects. Our comparative analysis reveals that\nwhile current leading models falter on this benchmark, OtterHD-8B, particularly\nwhen directly processing high-resolution inputs, outperforms its counterparts\nby a substantial margin. The findings illuminate the structural variances in\nvisual information processing among different models and the influence that the\nvision encoders' pre-training resolution disparities have on model\neffectiveness within such benchmarks. Our study highlights the critical role of\nflexibility and high-resolution input capabilities in large multimodal models\nand also exemplifies the potential inherent in the Fuyu architecture's\nsimplicity for handling complex visual data.",
        "translated": ""
    },
    {
        "title": "Towards Garment Sewing Pattern Reconstruction from a Single Image",
        "url": "http://arxiv.org/abs/2311.04218v1",
        "pub_date": "2023-11-07",
        "summary": "Garment sewing pattern represents the intrinsic rest shape of a garment, and\nis the core for many applications like fashion design, virtual try-on, and\ndigital avatars. In this work, we explore the challenging problem of recovering\ngarment sewing patterns from daily photos for augmenting these applications. To\nsolve the problem, we first synthesize a versatile dataset, named SewFactory,\nwhich consists of around 1M images and ground-truth sewing patterns for model\ntraining and quantitative evaluation. SewFactory covers a wide range of human\nposes, body shapes, and sewing patterns, and possesses realistic appearances\nthanks to the proposed human texture synthesis network. Then, we propose a\ntwo-level Transformer network called Sewformer, which significantly improves\nthe sewing pattern prediction performance. Extensive experiments demonstrate\nthat the proposed framework is effective in recovering sewing patterns and well\ngeneralizes to casually-taken human photos. Code, dataset, and pre-trained\nmodels are available at: https://sewformer.github.io.",
        "translated": ""
    },
    {
        "title": "Video Instance Matting",
        "url": "http://arxiv.org/abs/2311.04212v1",
        "pub_date": "2023-11-07",
        "summary": "Conventional video matting outputs one alpha matte for all instances\nappearing in a video frame so that individual instances are not distinguished.\nWhile video instance segmentation provides time-consistent instance masks,\nresults are unsatisfactory for matting applications, especially due to applied\nbinarization. To remedy this deficiency, we propose Video Instance\nMatting~(VIM), that is, estimating alpha mattes of each instance at each frame\nof a video sequence. To tackle this challenging problem, we present MSG-VIM, a\nMask Sequence Guided Video Instance Matting neural network, as a novel baseline\nmodel for VIM. MSG-VIM leverages a mixture of mask augmentations to make\npredictions robust to inaccurate and inconsistent mask guidance. It\nincorporates temporal mask and temporal feature guidance to improve the\ntemporal consistency of alpha matte predictions. Furthermore, we build a new\nbenchmark for VIM, called VIM50, which comprises 50 video clips with multiple\nhuman instances as foreground objects. To evaluate performances on the VIM\ntask, we introduce a suitable metric called Video Instance-aware Matting\nQuality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50\nbenchmark and outperforms existing methods by a large margin. The project is\nopen-sourced at https://github.com/SHI-Labs/VIM.",
        "translated": ""
    },
    {
        "title": "Deep Hashing via Householder Quantization",
        "url": "http://arxiv.org/abs/2311.04207v1",
        "pub_date": "2023-11-07",
        "summary": "Hashing is at the heart of large-scale image similarity search, and recent\nmethods have been substantially improved through deep learning techniques. Such\nalgorithms typically learn continuous embeddings of the data. To avoid a\nsubsequent costly binarization step, a common solution is to employ loss\nfunctions that combine a similarity learning term (to ensure similar images are\ngrouped to nearby embeddings) and a quantization penalty term (to ensure that\nthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,\nthe interaction between these two terms can make learning harder and the\nembeddings worse. We propose an alternative quantization strategy that\ndecomposes the learning problem in two stages: first, perform similarity\nlearning over the embedding space with no quantization; second, find an optimal\northogonal transformation of the embeddings so each coordinate of the embedding\nis close to its sign, and then quantize the transformed embedding through the\nsign function. In the second step, we parametrize orthogonal transformations\nusing Householder matrices to efficiently leverage stochastic gradient descent.\nSince similarity measures are usually invariant under orthogonal\ntransformations, this quantization strategy comes at no cost in terms of\nperformance. The resulting algorithm is unsupervised, fast, hyperparameter-free\nand can be run on top of any existing deep hashing or metric learning\nalgorithm. We provide extensive experimental results showing that this approach\nleads to state-of-the-art performance on widely used image datasets, and,\nunlike other quantization strategies, brings consistent improvements in\nperformance to existing deep hashing algorithms.",
        "translated": ""
    },
    {
        "title": "Selective Visual Representations Improve Convergence and Generalization\n  for Embodied AI",
        "url": "http://arxiv.org/abs/2311.04193v1",
        "pub_date": "2023-11-07",
        "summary": "Embodied AI models often employ off the shelf vision backbones like CLIP to\nencode their visual observations. Although such general purpose representations\nencode rich syntactic and semantic information about the scene, much of this\ninformation is often irrelevant to the specific task at hand. This introduces\nnoise within the learning process and distracts the agent's focus from\ntask-relevant visual cues. Inspired by selective attention in humans-the\nprocess through which people filter their perception based on their\nexperiences, knowledge, and the task at hand-we introduce a parameter-efficient\napproach to filter visual stimuli for embodied AI. Our approach induces a\ntask-conditioned bottleneck using a small learnable codebook module. This\ncodebook is trained jointly to optimize task reward and acts as a\ntask-conditioned selective filter over the visual observation. Our experiments\nshowcase state-of-the-art performance for object goal navigation and object\ndisplacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR,\nand ManipulaTHOR. The filtered representations produced by the codebook are\nalso able generalize better and converge faster when adapted to other\nsimulation environments such as Habitat. Our qualitative analyses show that\nagents explore their environments more effectively and their representations\nretain task-relevant information like target object recognition while ignoring\nsuperfluous information about other objects. Code and pretrained models are\navailable at our project website: https://embodied-codebook.github.io.",
        "translated": ""
    },
    {
        "title": "GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and\n  reusing ModulEs",
        "url": "http://arxiv.org/abs/2311.04901v1",
        "pub_date": "2023-11-08",
        "summary": "Recent works have shown that Large Language Models (LLMs) could empower\ntraditional neuro-symbolic models via programming capabilities to translate\nlanguage into module descriptions, thus achieving strong visual reasoning\nresults while maintaining the model's transparency and efficiency. However,\nthese models usually exhaustively generate the entire code snippet given each\nnew instance of a task, which is extremely ineffective. We propose generative\nneuro-symbolic visual reasoning by growing and reusing modules. Specifically,\nour model consists of three unique stages, module initialization, module\ngeneration, and module execution. First, given a vision-language task, we adopt\nLLMs to examine whether we could reuse and grow over established modules to\nhandle this new task. If not, we initialize a new module needed by the task and\nspecify the inputs and outputs of this new module. After that, the new module\nis created by querying LLMs to generate corresponding code snippets that match\nthe requirements. In order to get a better sense of the new module's ability,\nwe treat few-shot training examples as test cases to see if our new module\ncould pass these cases. If yes, the new module is added to the module library\nfor future reuse. Finally, we evaluate the performance of our model on the\ntesting set by executing the parsed programs with the newly made visual modules\nto get the results. We find the proposed model possesses several advantages.\nFirst, it performs competitively on standard tasks like visual question\nanswering and referring expression comprehension; Second, the modules learned\nfrom one task can be seamlessly transferred to new tasks; Last but not least,\nit is able to adapt to new visual reasoning tasks by observing a few training\nexamples and reusing modules.",
        "translated": ""
    },
    {
        "title": "Two Complementary Perspectives to Continual Learning: Ask Not Only What\n  to Optimize, But Also How",
        "url": "http://arxiv.org/abs/2311.04898v1",
        "pub_date": "2023-11-08",
        "summary": "Recent years have seen considerable progress in the continual training of\ndeep neural networks, predominantly thanks to approaches that add replay or\nregularization terms to the loss function to approximate the joint loss over\nall tasks so far. However, we show that even with a perfect approximation to\nthe joint loss, these approaches still suffer from temporary but substantial\nforgetting when starting to train on a new task. Motivated by this 'stability\ngap', we propose that continual learning strategies should focus not only on\nthe optimization objective, but also on the way this objective is optimized.\nWhile there is some continual learning work that alters the optimization\ntrajectory (e.g., using gradient projection techniques), this line of research\nis positioned as alternative to improving the optimization objective, while we\nargue it should be complementary. To evaluate the merits of our proposition, we\nplan to combine replay-approximated joint objectives with gradient\nprojection-based optimization routines to test whether the addition of the\nlatter provides benefits in terms of (1) alleviating the stability gap, (2)\nincreasing the learning efficiency and (3) improving the final learning\noutcome.",
        "translated": ""
    },
    {
        "title": "DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of\n  mixture-of-datasets",
        "url": "http://arxiv.org/abs/2311.04894v1",
        "pub_date": "2023-11-08",
        "summary": "Construction of a universal detector poses a crucial question: How can we\nmost effectively train a model on a large mixture of datasets? The answer lies\nin learning dataset-specific features and ensembling their knowledge but do all\nthis in a single model. Previous methods achieve this by having separate\ndetection heads on a common backbone but that results in a significant increase\nin parameters. In this work, we present Mixture-of-Experts as a solution,\nhighlighting that MoEs are much more than a scalability tool. We propose\nDataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an\n`expert' of a dataset by learning to route each dataset tokens to its mapped\nexpert. Experiments on Universal Object-Detection Benchmark show that we\noutperform the existing state-of-the-art by average +10.2 AP score and improve\nover our non-MoE baseline by average +2.0 AP score. We also observe consistent\ngains while mixing datasets with (1) limited availability, (2) disparate\ndomains and (3) divergent label sets. Further, we qualitatively show that DAMEX\nis robust against expert representation collapse.",
        "translated": ""
    },
    {
        "title": "Towards Few-Annotation Learning in Computer Vision: Application to Image\n  Classification and Object Detection tasks",
        "url": "http://arxiv.org/abs/2311.04888v1",
        "pub_date": "2023-11-08",
        "summary": "In this thesis, we develop theoretical, algorithmic and experimental\ncontributions for Machine Learning with limited labels, and more specifically\nfor the tasks of Image Classification and Object Detection in Computer Vision.\nIn a first contribution, we are interested in bridging the gap between theory\nand practice for popular Meta-Learning algorithms used in Few-Shot\nClassification. We make connections to Multi-Task Representation Learning,\nwhich benefits from solid theoretical foundations, to verify the best\nconditions for a more efficient meta-learning. Then, to leverage unlabeled data\nwhen training object detectors based on the Transformer architecture, we\npropose both an unsupervised pretraining and a semi-supervised learning method\nin two other separate contributions. For pretraining, we improve Contrastive\nLearning for object detectors by introducing the localization information.\nFinally, our semi-supervised method is the first tailored to transformer-based\ndetectors.",
        "translated": ""
    },
    {
        "title": "Are foundation models efficient for medical image segmentation?",
        "url": "http://arxiv.org/abs/2311.04847v1",
        "pub_date": "2023-11-08",
        "summary": "Foundation models are experiencing a surge in popularity. The Segment\nAnything model (SAM) asserts an ability to segment a wide spectrum of objects\nbut required supervised training at unprecedented scale. We compared SAM's\nperformance (against clinical ground truth) and resources (labeling time,\ncompute) to a modality-specific, label-free self-supervised learning (SSL)\nmethod on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and\nrequired significantly more labeling and computing resources, demonstrating\nworse efficiency than SSL.",
        "translated": ""
    },
    {
        "title": "Window Attention is Bugged: How not to Interpolate Position Embeddings",
        "url": "http://arxiv.org/abs/2311.05613v1",
        "pub_date": "2023-11-09",
        "summary": "Window attention, position embeddings, and high resolution finetuning are\ncore concepts in the modern transformer era of computer vision. However, we\nfind that naively combining these near ubiquitous components can have a\ndetrimental effect on performance. The issue is simple: interpolating position\nembeddings while using window attention is wrong. We study two state-of-the-art\nmethods that have these three components, namely Hiera and ViTDet, and find\nthat both do indeed suffer from this bug. To fix it, we introduce a simple\nabsolute window position embedding strategy, which solves the bug outright in\nHiera and allows us to increase both speed and performance of the model in\nViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 box\nmAP on COCO, making it state-of-the-art for models that only use ImageNet-1k\npretraining. This all stems from what is essentially a 3 line bug fix, which we\nname \"absolute win\".",
        "translated": ""
    },
    {
        "title": "What Do I Hear? Generating Sounds for Visuals with ChatGPT",
        "url": "http://arxiv.org/abs/2311.05609v1",
        "pub_date": "2023-11-09",
        "summary": "This short paper introduces a workflow for generating realistic soundscapes\nfor visual media. In contrast to prior work, which primarily focus on matching\nsounds for on-screen visuals, our approach extends to suggesting sounds that\nmay not be immediately visible but are essential to crafting a convincing and\nimmersive auditory environment. Our key insight is leveraging the reasoning\ncapabilities of language models, such as ChatGPT. In this paper, we describe\nour workflow, which includes creating a scene context, brainstorming sounds,\nand generating the sounds.",
        "translated": ""
    },
    {
        "title": "Real-Time Neural Rasterization for Large Scenes",
        "url": "http://arxiv.org/abs/2311.05607v1",
        "pub_date": "2023-11-09",
        "summary": "We propose a new method for realistic real-time novel-view synthesis (NVS) of\nlarge scenes. Existing neural rendering methods generate realistic results, but\nprimarily work for small scale scenes (&lt;50 square meters) and have difficulty\nat large scale (&gt;10000 square meters). Traditional graphics-based rasterization\nrendering is fast for large scenes but lacks realism and requires expensive\nmanually created assets. Our approach combines the best of both worlds by\ntaking a moderate-quality scaffold mesh as input and learning a neural texture\nfield and shader to model view-dependant effects to enhance realism, while\nstill using the standard graphics pipeline for real-time rendering. Our method\noutperforms existing neural rendering methods, providing at least 30x faster\nrendering with comparable or better realism for large self-driving and drone\nscenes. Our work is the first to enable real-time rendering of large real-world\nscenes.",
        "translated": ""
    },
    {
        "title": "3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds",
        "url": "http://arxiv.org/abs/2311.05604v1",
        "pub_date": "2023-11-09",
        "summary": "Existing methods for learning 3D representations are deep neural networks\ntrained and tested on classical hardware. Quantum machine learning\narchitectures, despite their theoretically predicted advantages in terms of\nspeed and the representational capacity, have so far not been considered for\nthis problem nor for tasks involving 3D data in general. This paper thus\nintroduces the first quantum auto-encoder for 3D point clouds. Our 3D-QAE\napproach is fully quantum, i.e. all its data processing components are designed\nfor quantum hardware. It is trained on collections of 3D point clouds to\nproduce their compressed representations. Along with finding a suitable\narchitecture, the core challenges in designing such a fully quantum model\ninclude 3D data normalisation and parameter optimisation, and we propose\nsolutions for both these tasks. Experiments on simulated gate-based quantum\nhardware demonstrate that our method outperforms simple classical baselines,\npaving the way for a new research direction in 3D computer vision. The source\ncode is available at https://4dqv.mpi-inf.mpg.de/QAE3D/.",
        "translated": ""
    },
    {
        "title": "Reconstructing Objects in-the-wild for Realistic Sensor Simulation",
        "url": "http://arxiv.org/abs/2311.05602v1",
        "pub_date": "2023-11-09",
        "summary": "Reconstructing objects from real world data and rendering them at novel views\nis critical to bringing realism, diversity and scale to simulation for robotics\ntraining and testing. In this work, we present NeuSim, a novel approach that\nestimates accurate geometry and realistic appearance from sparse in-the-wild\ndata captured at distance and at limited viewpoints. Towards this goal, we\nrepresent the object surface as a neural signed distance function and leverage\nboth LiDAR and camera sensor data to reconstruct smooth and accurate geometry\nand normals. We model the object appearance with a robust physics-inspired\nreflectance representation effective for in-the-wild data. Our experiments show\nthat NeuSim has strong view synthesis performance on challenging scenarios with\nsparse training views. Furthermore, we showcase composing NeuSim assets into a\nvirtual world and generating realistic multi-sensor data for evaluating\nself-driving perception models.",
        "translated": ""
    }
]